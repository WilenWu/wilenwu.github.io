<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大数据手册(Spark)--Spark SQL and DataFrames | 雷小小</title><meta name="author" content="Tiny Lei"><meta name="copyright" content="Tiny Lei"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="DataFrame 抽象">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据手册(Spark)--Spark SQL and DataFrames">
<meta property="og:url" content="https://wilenwu.gitee.io/posts/bb755aa3/index.html">
<meta property="og:site_name" content="雷小小">
<meta property="og:description" content="DataFrame 抽象">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wilenwu.gitee.io/img/apache-spark-sql.png">
<meta property="article:published_time" content="2020-01-03T08:20:25.000Z">
<meta property="article:modified_time" content="2024-06-06T14:38:23.685Z">
<meta property="article:author" content="Tiny Lei">
<meta property="article:tag" content="大数据">
<meta property="article:tag" content="python">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wilenwu.gitee.io/img/apache-spark-sql.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://wilenwu.gitee.io/posts/bb755aa3/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="baidu-site-verification" content="code-7rymn5Bitx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?654e7415ab55bed7c9c2bc6d665f03c5";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大数据手册(Spark)--Spark SQL and DataFrames',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-06 22:38:23'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_2849223_xh1ftc8qym.css"><link rel="stylesheet" href="/css/link-card.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="雷小小" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">175</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">108</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">43</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/user-guide/"><i class="fa-fw fa fa-compass"></i><span> 用户指南</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa fa-book"></i><span> 文档</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-images"></i><span> 图库</span></a></div><div class="menus_item"><a class="site-page" href="/ebook/"><i class="fa-fw fa fa-book-reader"></i><span> 电子书</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-star"></i><span> 收藏夹</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa-solid fa-circle-chevron-down"></i><span> 更多</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fa fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/analytics/"><i class="fa-fw fa fa-line-chart"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/log/"><i class="fa-fw fa fa-history"></i><span> 更新日志</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/apache-spark-top-img.svg')"><nav id="nav"><span id="blog-info"><a href="/" title="雷小小"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/favicon.png"/><span class="site-name">雷小小</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/user-guide/"><i class="fa-fw fa fa-compass"></i><span> 用户指南</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa fa-book"></i><span> 文档</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-images"></i><span> 图库</span></a></div><div class="menus_item"><a class="site-page" href="/ebook/"><i class="fa-fw fa fa-book-reader"></i><span> 电子书</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-star"></i><span> 收藏夹</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa-solid fa-circle-chevron-down"></i><span> 更多</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fa fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/analytics/"><i class="fa-fw fa fa-line-chart"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/log/"><i class="fa-fw fa fa-history"></i><span> 更新日志</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大数据手册(Spark)--Spark SQL and DataFrames<a class="post-edit-link" href="https://gitee.com/WilenWu/myblog/edit/master/source/_posts/bigdata/Spark-SQL-and-DataFrames.md" rel="external nofollow noreferrer" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-01-03T08:20:25.000Z" title="发表于 2020-01-03 16:20:25">2020-01-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-06T14:38:23.685Z" title="更新于 2024-06-06 22:38:23">2024-06-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/big-data/">Big Data</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/big-data/spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>40分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="大数据手册(Spark)--Spark SQL and DataFrames"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="spark-sql"><a class="markdownIt-Anchor" href="#spark-sql"></a> Spark SQL</h1>
<p>Spark SQL用于对结构化数据进行处理，它提供了DataFrame的抽象，作为分布式平台数据查询引擎，可以在此组件上构建大数据仓库。</p>
<blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://sparkbyexamples.com/pyspark-tutorial/#google_vignette">PySpark 3.5 Tutorial For Beginners with Examples</a></p>
</blockquote>
<h2 id="sparksession"><a class="markdownIt-Anchor" href="#sparksession"></a> SparkSession</h2>
<p>在过去，你可能会使用 SparkContext、SQLContext和HiveContext来分别配置Spark环境、SQL环境和Hive环境。SparkSession本质上是这些环境的组合，包括读取数据、处理元数据、配置会话和管理集群资源。</p>
<p>要创建基本的SparkSession，只需使用SparkSession.builder</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">spark = SparkSession \</span><br><span class="line">   .builder \</span><br><span class="line">	 .enableHiveSupport() \</span><br><span class="line">   .appName(<span class="string">&#x27;myApp&#x27;</span>) \</span><br><span class="line">   .master(<span class="string">&#x27;local&#x27;</span>) \</span><br><span class="line">	 .config(<span class="string">&#x27;spark.driver.memory&#x27;</span>,<span class="string">&#x27;2g&#x27;</span>) \</span><br><span class="line">	 .config(<span class="string">&#x27;spark.driver.memoryOverhead&#x27;</span>,<span class="string">&#x27;1g&#x27;</span>) \</span><br><span class="line">	 .config(<span class="string">&#x27;spark.executor.memory&#x27;</span>,<span class="string">&#x27;2g&#x27;</span>) \</span><br><span class="line">	 .config(<span class="string">&#x27;spark.executor.memoryOverhead&#x27;</span>,<span class="string">&#x27;1g&#x27;</span>) \</span><br><span class="line">	 .config(<span class="string">&#x27;spark.driver.cores&#x27;</span>,<span class="string">&#x27;2&#x27;</span>) \</span><br><span class="line">   .config(<span class="string">&#x27;spark.executor.cores&#x27;</span>,<span class="string">&#x27;2&#x27;</span>) \</span><br><span class="line">   .getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">spark.stop() <span class="comment"># Stop the underlying SparkContext.</span></span><br></pre></td></tr></table></figure>
<p>在 Spark 交互式环境下，默认已经创建了名为 spark 的 SparkSession 对象，不需要自行创建。</p>
<table>
<thead>
<tr>
<th>SparkSession.builder</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>SparkSession.builder.appName(name)</code></td>
<td>设置Spark web UI上的应用名</td>
</tr>
<tr>
<td><code>SparkSession.builder.master(master)</code></td>
<td>设置Spark master URL</td>
</tr>
<tr>
<td><code>SparkSession.builder.config(key, value, …)</code></td>
<td>配置选项</td>
</tr>
<tr>
<td><code>SparkSession.builder.enableHiveSupport()</code></td>
<td>启用Hive支持</td>
</tr>
<tr>
<td><code>SparkSession.builder.getOrCreate</code></td>
<td>创建SparkSession</td>
</tr>
<tr>
<td><code>SparkSession.range()</code></td>
<td>创建一个只含id列的DataFrame</td>
</tr>
</tbody>
</table>
<p>在使用Spark与Hive集成时，需要使用enableHiveSupport方法来启用Hive支持。启用Hive支持后，就可以在Spark中使用Hive的元数据、表和数据源。</p>
<h2 id="catalog"><a class="markdownIt-Anchor" href="#catalog"></a> Catalog</h2>
<p>pyspark.sql.Catalog 是面向用户的目录 API。</p>
<table>
<thead>
<tr>
<th>pyspark.sql.Catalog</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Catalog.currentCatalog()</code></td>
<td>返回当前默认目录</td>
</tr>
<tr>
<td><code>Catalog.currentDatabase()</code></td>
<td>返回当前默认数据库</td>
</tr>
<tr>
<td><code>Catalog.databaseExists(dbName)</code></td>
<td>检查具有指定名称的数据库是否存在</td>
</tr>
<tr>
<td><code>Catalog.functionExists(functionName[, dbName])</code></td>
<td>检查具有指定名称的函数是否存在</td>
</tr>
<tr>
<td><code>Catalog.isCached(tableName)</code></td>
<td>检查指定表是否在内存中缓存</td>
</tr>
<tr>
<td><code>Catalog.tableExists(tableName[, dbName])</code></td>
<td>检查具有指定名称的表或视图是否存在</td>
</tr>
<tr>
<td><code>Catalog.listDatabases([pattern])</code></td>
<td>返回所有会话中可用的数据库列表</td>
</tr>
<tr>
<td><code>Catalog.listTables([dbName, pattern])</code></td>
<td>返回指定数据库中的表/视图列表</td>
</tr>
<tr>
<td><code>Catalog.listFunctions([dbName, pattern])</code></td>
<td>返回在指定数据库中注册的函数列表</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark.<span class="built_in">range</span>(<span class="number">1</span>).createTempView(<span class="string">&quot;test_view&quot;</span>)</span><br><span class="line">spark.catalog.listTables()</span><br><span class="line">[Table(name=<span class="string">&#x27;test_view&#x27;</span>, catalog=<span class="literal">None</span>, namespace=[], description=<span class="literal">None</span>, ...</span><br></pre></td></tr></table></figure>
<h1 id="dataframe"><a class="markdownIt-Anchor" href="#dataframe"></a> DataFrame</h1>
<p>DataFrame是一个分布式数据集，在概念上类似于传统数据库的表结构。</p>
<p>DataFrame的一个主要优点是：Spark引擎一开始就构建了一个逻辑执行计划，而且执行生成的代码是基于成本优化程序确定的物理计划。与Java或者Scala相比，Python中的RDD是非常慢的，而DataFrame的引入则使性能在各种语言中都保持稳定。</p>
<h2 id="创建-dataframe"><a class="markdownIt-Anchor" href="#创建-dataframe"></a> 创建 DataFrame</h2>
<p>PySpark DataFrame可以通过 SparkSession.createDataFrame创建。数据来源通常是结构化的python对象、RDD、Hive表或Spark数据源。并采用schema参数来指定DataFrame的模式，当schema被省略时，PySpark通过从数据中提取样本来推断相应的模式。</p>
<p><strong>从列表创建PySpark DataFrame</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># spark is an existing SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a DataFrame from a list of tuples.</span></span><br><span class="line">spark.createDataFrame([</span><br><span class="line">    (<span class="number">29</span>, <span class="string">&quot;Michael&quot;</span>),</span><br><span class="line">    (<span class="number">30</span>, <span class="string">&quot;Andy&quot;</span>),</span><br><span class="line">    (<span class="number">19</span>, <span class="string">&quot;Justin&quot;</span>)</span><br><span class="line">]).show()</span><br><span class="line">+----+-------+</span><br><span class="line">|  _1|     _2|</span><br><span class="line">+----+-------+</span><br><span class="line">|  <span class="number">29</span>|Michael|</span><br><span class="line">|  <span class="number">30</span>|   Andy|</span><br><span class="line">|  <span class="number">19</span>| Justin|</span><br><span class="line">+----+-------+</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a DataFrame from a list of dictionaries.</span></span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    &#123;<span class="string">&quot;age&quot;</span>: <span class="number">29</span>, <span class="string">&quot;name&quot;</span>: <span class="string">&quot;Michael&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;age&quot;</span>: <span class="number">30</span>, <span class="string">&quot;name&quot;</span>: <span class="string">&quot;Andy&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;age&quot;</span>: <span class="number">19</span>, <span class="string">&quot;name&quot;</span>: <span class="string">&quot;Justin&quot;</span>&#125;</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>通过采用schema参数来指定DataFrame的模式，schema 可以是字段名列表，DDL字符串，或者StructType</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create a DataFrame with column names specified.</span></span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">29</span>, <span class="string">&quot;Michael&quot;</span>),</span><br><span class="line">    (<span class="number">30</span>, <span class="string">&quot;Andy&quot;</span>),</span><br><span class="line">    (<span class="number">19</span>, <span class="string">&quot;Justin&quot;</span>)</span><br><span class="line">], schema=[<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a DataFrame with the schema in DDL formatted string.</span></span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">29</span>, <span class="string">&quot;Michael&quot;</span>),</span><br><span class="line">    (<span class="number">30</span>, <span class="string">&quot;Andy&quot;</span>),</span><br><span class="line">    (<span class="number">19</span>, <span class="string">&quot;Justin&quot;</span>)</span><br><span class="line">], schema=<span class="string">&quot;age: int, name: string&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a DataFrame with the explicit schema specified.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line">schema = StructType([</span><br><span class="line">    StructField(<span class="string">&quot;age&quot;</span>, IntegerType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">&quot;name&quot;</span>, StringType(), <span class="literal">True</span>)</span><br><span class="line">])</span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">29</span>, <span class="string">&quot;Michael&quot;</span>),</span><br><span class="line">    (<span class="number">30</span>, <span class="string">&quot;Andy&quot;</span>),</span><br><span class="line">    (<span class="number">19</span>, <span class="string">&quot;Justin&quot;</span>)</span><br><span class="line">], schema=schema)</span><br></pre></td></tr></table></figure>
<p>其中 StructField 包括 name（字段名）、dataType（数据类型）和nullable（此字段的值是否为空）。</p>
<p><strong>从 Rows 创建PySpark DataFrame</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"><span class="comment"># Create a DataFrame from a list of Rows.</span></span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    Row(age=<span class="number">29</span>, name=<span class="string">&quot;Michael&quot;</span>),</span><br><span class="line">    Row(age=<span class="number">30</span>, name=<span class="string">&quot;Andy&quot;</span>),</span><br><span class="line">    Row(age=<span class="number">19</span>, name=<span class="string">&quot;Justin&quot;</span>)</span><br><span class="line">], schema=<span class="string">&quot;age int, name string&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>从pandas DataFrame创建PySpark DataFrame</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># Create a DataFrame from a Pandas DataFrame.</span></span><br><span class="line">pandas_df = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>: [<span class="number">29</span>, <span class="number">30</span>, <span class="number">19</span>],</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>: [<span class="string">&#x27;Michael&#x27;</span>, <span class="string">&#x27;Andy&#x27;</span>, <span class="string">&#x27;Justin&#x27;</span>]</span><br><span class="line">&#125;)</span><br><span class="line">df = spark.createDataFrame(pandas_df)</span><br></pre></td></tr></table></figure>
<p><strong>从RDD创建PySpark DataFrame</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rdd = sc.parallelize([</span><br><span class="line">    (<span class="number">29</span>, <span class="string">&quot;Michael&quot;</span>),</span><br><span class="line">    (<span class="number">30</span>, <span class="string">&quot;Andy&quot;</span>),</span><br><span class="line">    (<span class="number">19</span>, <span class="string">&quot;Justin&quot;</span>)</span><br><span class="line">])</span><br><span class="line">df = rdd.toDF(schema=<span class="string">&quot;age int, name string&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>从 pandas-on-Spark DataFrame 创建 PySpark DataFrame</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.pandas <span class="keyword">as</span> ps</span><br><span class="line">psdf = ps.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>: [<span class="number">29</span>, <span class="number">30</span>, <span class="number">19</span>],</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>: [<span class="string">&#x27;Michael&#x27;</span>, <span class="string">&#x27;Andy&#x27;</span>, <span class="string">&#x27;Justin&#x27;</span>]</span><br><span class="line">&#125;)</span><br><span class="line">df = psdf.to_spark()</span><br></pre></td></tr></table></figure>
<p>上面创建的DataFrame具有相同的结果和模式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># All DataFrames above result same.</span></span><br><span class="line">df.show()</span><br><span class="line">+---+-------+                                                                   </span><br><span class="line">|age|   name|</span><br><span class="line">+---+-------+</span><br><span class="line">| <span class="number">29</span>|Michael|</span><br><span class="line">| <span class="number">30</span>|   Andy|</span><br><span class="line">| <span class="number">19</span>| Justin|</span><br><span class="line">+---+-------+</span><br><span class="line"></span><br><span class="line">df.printSchema()</span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment">#  |-- age: integer (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- name: string (nullable = true)</span></span><br></pre></td></tr></table></figure>
<p>PySpark DataFrame还提供了转化为其他结构化数据类型的方法</p>
<table>
<thead>
<tr>
<th>pyspark.sql.DataFrame</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>DataFrame.toPandas()</code></td>
<td>转化为Pandas DataFrame</td>
</tr>
<tr>
<td><code>DataFrame.rdd</code></td>
<td>转化为RDD</td>
</tr>
<tr>
<td><code>DataFrame.to_pandas_on_spark()</code></td>
<td>转化为pandas-on-Spark</td>
</tr>
<tr>
<td><code>DataFrame.pandas_api()</code></td>
<td>转化为pandas-on-Spark</td>
</tr>
<tr>
<td><code>DataFrame.toJSON()</code></td>
<td>转化为Json字符串RDD</td>
</tr>
<tr>
<td><code>DataFrame.to(schema)</code></td>
<td>改变列顺序和数据类型</td>
</tr>
</tbody>
</table>
<p>请注意，转换为Pandas DataFrame 时，会将所有数据收集到本地。当数据太大时，很容易导致 out-of-memory-error。</p>
<h2 id="dataframe信息"><a class="markdownIt-Anchor" href="#dataframe信息"></a> DataFrame信息</h2>
<table>
<thead>
<tr>
<th>pyspark.sql.DataFrame</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>DataFrame.show(n, truncate, vertical)</code></td>
<td>预览前n行数据</td>
</tr>
<tr>
<td><code>DataFrame.collect()</code></td>
<td>返回Row列表</td>
</tr>
<tr>
<td><code>DataFrame.take(num)</code></td>
<td>返回前num个Row列表</td>
</tr>
<tr>
<td><code>DataFrame.head(n)</code></td>
<td>返回前n个Row列表</td>
</tr>
<tr>
<td><code>DataFrame.tail(num)</code></td>
<td>返回后num个Row列表</td>
</tr>
<tr>
<td><code>DataFrame.columns</code></td>
<td>返回列名列表</td>
</tr>
<tr>
<td><code>DataFrame.dtypes</code></td>
<td>返回数据类型列表</td>
</tr>
<tr>
<td><code>DataFrame.printSchema(level)</code></td>
<td>打印模式信息</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># spark, df are from the previous example</span></span><br><span class="line"><span class="comment"># Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show()</span><br><span class="line">+---+-------+                                                                   </span><br><span class="line">|age|   name|</span><br><span class="line">+---+-------+</span><br><span class="line">| <span class="number">29</span>|Michael|</span><br><span class="line">| <span class="number">30</span>|   Andy|</span><br><span class="line">| <span class="number">19</span>| Justin|</span><br><span class="line">+---+-------+</span><br></pre></td></tr></table></figure>
<p>还可以通过启用<code>spark.sql.repl.eagerEval.enabled</code>，在Jupyter等notebook中美化 DataFrame 显示。显示的行数可以通过<code>spark.sql.repl.eagerEval.maxNumRows</code>配置。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark.conf.<span class="built_in">set</span>(<span class="string">&#x27;spark.sql.repl.eagerEval.enabled&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>
<p>当行太长而无法水平显示时，您也可以转置显示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.show(<span class="number">1</span>, vertical=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># -RECORD 0-------</span></span><br><span class="line"><span class="comment">#  age  | 29    </span></span><br><span class="line"><span class="comment">#  name | Michael </span></span><br><span class="line"><span class="comment"># only showing top 1 row</span></span><br></pre></td></tr></table></figure>
<p><code>DataFrame.collect()</code>将分布式数据收集到客户端，作为Python中的本地数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.collect()</span><br><span class="line"><span class="comment"># [Row(age=29, name=&#x27;Michael&#x27;), Row(age=30, name=&#x27;Andy&#x27;), Row(age=19, name=&#x27;Justin&#x27;)]</span></span><br></pre></td></tr></table></figure>
<p>请注意，当数据集太大时，可能会引发 out-of-memory-error。为了避免抛出内存异常，请使用<code>DataFrame.take()</code>或<code>DataFrame.tail()</code>。</p>
<p>还可以查看DataFrame的列名和模式</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.columns</span><br><span class="line"><span class="comment"># [&#x27;age&#x27;, &#x27;name&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the schema in a tree format</span></span><br><span class="line">df.printSchema()</span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment">#  |-- age: integer (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- name: string (nullable = true)</span></span><br></pre></td></tr></table></figure>
<h2 id="inputoutput"><a class="markdownIt-Anchor" href="#inputoutput"></a> Input/Output</h2>
<p>PySpark 支持许多外部数据源，如JDBC、text、binaryFile、Avro等。</p>
<h3 id="外部数据源"><a class="markdownIt-Anchor" href="#外部数据源"></a> 外部数据源</h3>
<p>Spark SQL提供pyspark.sql.DataFrameReader将文件读取到Spark DataFrame，并提供pyspark.sql.DataFrameWriter 写入文件。可用<code>option()</code>方法配置读写行为。</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://spark.apache.org/docs/3.5.1/sql-data-sources.html">Spark SQL Data Sources</a></p>
<p>本节先介绍使用Spark加载和保存数据的一般方法，然后介绍特定数据源。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># spark, df are from the previous example</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># &quot;people&quot; is a folder which contains multiple csv files and a _SUCCESS file.</span></span><br><span class="line">df.write.save(<span class="string">&quot;people.csv&quot;</span>, <span class="built_in">format</span>=<span class="string">&quot;csv&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read a csv with delimiter and a header</span></span><br><span class="line">df = spark.read.option(<span class="string">&quot;delimiter&quot;</span>, <span class="string">&quot;,&quot;</span>).option(<span class="string">&quot;header&quot;</span>, <span class="literal">True</span>).load(<span class="string">&quot;people.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also use options() to use multiple options</span></span><br><span class="line">df = spark.read.options(delimiter=<span class="string">&quot;,&quot;</span>, header=<span class="literal">True</span>).load(<span class="string">&quot;people.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run SQL on files directly</span></span><br><span class="line">df = spark.sql(<span class="string">&quot;SELECT * FROM parquet.`parquetFile.parquet`&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Parquet</span></span><br><span class="line">parquetDF.write.parquet(<span class="string">&#x27;parquetFile.parquet&#x27;</span>)</span><br><span class="line">parquetDF = spark.read.parquet(<span class="string">&#x27;parquetFile.parquet&#x27;</span>)</span><br><span class="line"><span class="comment"># ORC</span></span><br><span class="line">orcDF.write.orc(<span class="string">&#x27;orcFile.orc&#x27;</span>)</span><br><span class="line">orcDF = spark.read.orc(<span class="string">&#x27;orcFile.orc&#x27;</span>)</span><br><span class="line"><span class="comment"># Json</span></span><br><span class="line">jsonDF.write.json(<span class="string">&#x27;jsonFile.json&#x27;</span>)</span><br><span class="line">jsonDF = spark.read.json(<span class="string">&#x27;jsonFile.json&#x27;</span>)</span><br><span class="line"><span class="comment"># CSV </span></span><br><span class="line">csvDF.write.csv(<span class="string">&#x27;csvFile.csv&#x27;</span>, header=<span class="literal">True</span>)</span><br><span class="line">csvDF = spark.read.csv(<span class="string">&#x27;csvFile.csv&#x27;</span>, sep=<span class="string">&quot;,&quot;</span>, inferSchema=<span class="string">&quot;true&quot;</span>, header=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># Text</span></span><br><span class="line">textDF.write.txt(<span class="string">&#x27;textFile.orc&#x27;</span>)</span><br><span class="line">textDF = spark.read.txt(<span class="string">&#x27;textFile.orc&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>CSV简单明了，易于使用。Parquet和ORC是高效而紧凑的文件格式，可以更快地读取和写入。</p>
<h3 id="通用选项配置"><a class="markdownIt-Anchor" href="#通用选项配置"></a> 通用选项/配置</h3>
<p>仅适用于parquet、orc、avro、json、csv、text</p>
<table>
<thead>
<tr>
<th>Options</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.sql.files.ignoreCorruptFiles</td>
<td>忽略损坏的文件</td>
</tr>
<tr>
<td>spark.sql.files.ignoreMissingFiles</td>
<td>忽略缺失的文件</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># enable ignore corrupt files via the data source option</span></span><br><span class="line"><span class="comment"># dir1/file3.json is corrupt from parquet&#x27;s view</span></span><br><span class="line">spark.read.option(<span class="string">&quot;ignoreCorruptFiles&quot;</span>, <span class="string">&quot;true&quot;</span>)\</span><br><span class="line">    .parquet(<span class="string">&quot;examples/src/main/resources/dir1/&quot;</span>,</span><br><span class="line">             <span class="string">&quot;examples/src/main/resources/dir1/dir2/&quot;</span>) \</span><br><span class="line">    .show()</span><br><span class="line">+-------------+</span><br><span class="line">|         file|</span><br><span class="line">+-------------+</span><br><span class="line">|file1.parquet|</span><br><span class="line">|file2.parquet|</span><br><span class="line">+-------------+</span><br><span class="line"></span><br><span class="line"><span class="comment"># enable ignore corrupt files via the configuration</span></span><br><span class="line">spark.sql(<span class="string">&quot;set spark.sql.files.ignoreCorruptFiles=true&quot;</span>)</span><br><span class="line"><span class="comment"># dir1/file3.json is corrupt from parquet&#x27;s view</span></span><br><span class="line">spark.read.parquet(<span class="string">&quot;examples/src/main/resources/dir1/&quot;</span>,</span><br><span class="line">                   <span class="string">&quot;examples/src/main/resources/dir1/dir2/&quot;</span>)\</span><br><span class="line">    .show()</span><br><span class="line">+-------------+</span><br><span class="line">|         file|</span><br><span class="line">+-------------+</span><br><span class="line">|file1.parquet|</span><br><span class="line">|file2.parquet|</span><br><span class="line">+-------------+</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>Options</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>pathGlobFilter</td>
<td>路径Glob过滤器。加载与给定glob模式匹配的路径的文件</td>
</tr>
<tr>
<td>recursiveFileLookup</td>
<td>递归文件查找</td>
</tr>
<tr>
<td>modifiedBefore<br>modifiedAfter</td>
<td>加载给定修改时间范围内的文件</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pathGlobFilter is used to only include files with file names matching the pattern. </span></span><br><span class="line">spark.read.load(<span class="string">&quot;examples/src/main/resources/dir1&quot;</span>,</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&quot;parquet&quot;</span>, pathGlobFilter=<span class="string">&quot;*.parquet&quot;</span>).show()</span><br><span class="line">+-------------+</span><br><span class="line">|         file|</span><br><span class="line">+-------------+</span><br><span class="line">|file1.parquet|</span><br><span class="line">+-------------+</span><br><span class="line"></span><br><span class="line"><span class="comment"># recursiveFileLookup is used to recursively load files and it disables partition inferring. </span></span><br><span class="line">spark.read.<span class="built_in">format</span>(<span class="string">&quot;parquet&quot;</span>)\</span><br><span class="line">    .option(<span class="string">&quot;recursiveFileLookup&quot;</span>, <span class="string">&quot;true&quot;</span>)\</span><br><span class="line">    .load(<span class="string">&quot;examples/src/main/resources/dir1&quot;</span>).show()</span><br><span class="line">+-------------+</span><br><span class="line">|         file|</span><br><span class="line">+-------------+</span><br><span class="line">|file1.parquet|</span><br><span class="line">|file2.parquet|</span><br><span class="line">+-------------+</span><br><span class="line"></span><br><span class="line"><span class="comment"># Only load files modified before 07/1/2050 @ 08:30:00</span></span><br><span class="line">spark.read.load(<span class="string">&quot;examples/src/main/resources/dir1&quot;</span>,</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&quot;parquet&quot;</span>, modifiedBefore=<span class="string">&quot;2050-07-01T08:30:00&quot;</span>).show()</span><br><span class="line">+-------------+</span><br><span class="line">|         file|</span><br><span class="line">+-------------+</span><br><span class="line">|file1.parquet|</span><br><span class="line">+-------------+</span><br><span class="line"><span class="comment"># Only load files modified after 06/01/2050 @ 08:30:00</span></span><br><span class="line">spark.read.load(<span class="string">&quot;examples/src/main/resources/dir1&quot;</span>,</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&quot;parquet&quot;</span>, modifiedAfter=<span class="string">&quot;2050-06-01T08:30:00&quot;</span>).show()</span><br><span class="line">+-------------+</span><br><span class="line">|         file|</span><br><span class="line">+-------------+</span><br><span class="line">+-------------+</span><br></pre></td></tr></table></figure>
<h3 id="保存模式"><a class="markdownIt-Anchor" href="#保存模式"></a> 保存模式</h3>
<p>保存模式有以下几种</p>
<table>
<thead>
<tr>
<th>DataFrame.write.mode</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>“error” or “errorifexists” (default)</td>
<td>如果数据已存在，则会报错</td>
</tr>
<tr>
<td>“append”</td>
<td>如果数据/表已存在，则追加到现有数据中</td>
</tr>
<tr>
<td>“overwrite”</td>
<td>如果数据/表已经存在，则覆盖原数据</td>
</tr>
<tr>
<td>“ignore”</td>
<td>如果数据/表已经存在，则忽略，不改变原数据</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.write.mode(<span class="string">&quot;overwrite&quot;</span>).<span class="built_in">format</span>(<span class="string">&quot;parquet&quot;</span>).save(<span class="string">&quot;people.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="分区"><a class="markdownIt-Anchor" href="#分区"></a> 分区</h3>
<p>也可以对输出文件进行分区</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.write.partitionBy(<span class="string">&quot;name&quot;</span>).<span class="built_in">format</span>(<span class="string">&quot;parquet&quot;</span>).save(<span class="string">&quot;PartByName.parquet&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>请注意，分区列的数据类型是自动推断的。目前，支持数字、日期、时间戳和字符串类型。如果不想自动推断分区列的数据类型，可以配置<code>spark.sql.sources.partitionColumnTypeInference.enabled</code>为<code>false</code>，则分区为字符串类型。</p>
<h2 id="hive-表交互"><a class="markdownIt-Anchor" href="#hive-表交互"></a> Hive 表交互</h2>
<h3 id="hive配置"><a class="markdownIt-Anchor" href="#hive配置"></a> Hive配置</h3>
<p>由于Hive有大量的依赖项，这些依赖项不包含在默认的Spark发行版中。一般我们需要将Hive的配置文件<code>hive-site.xml</code>、<code>core-site.xml</code>（用于安全配置）和<code>hdfs-site.xml</code>（用于HDFS配置）文件放在Spark的配置目录 <code>conf/</code>下。</p>
<p>请注意，自Spark 2.0.0以来，<code>hive-site.xml</code>中的<code>hive.metastore.warehouse.dir</code>属性已弃用。请使用<code>spark.sql.warehouse.dir</code> 来指定数仓的默认位置。</p>
<p>当然，对于没有部署Hive的用户仍然可以启用Hive支持，此时 Spark会自动在当前目录中创建<code>metastore_db</code>，并创建 spark数仓目录 <code>spark.sql.warehouse.dir</code>，数仓目录位于Spark应用程序启动目录下的 <code>spark-warehouse</code>。</p>
<h3 id="sql-查询"><a class="markdownIt-Anchor" href="#sql-查询"></a> SQL 查询</h3>
<p>通过调用SparkSession.sql()方法可以使用任意SQL语句，并返回DataFrame。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">SparkSession.sql(sqlQuery, args=<span class="literal">None</span>, **kwargs)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM range(10) where id &gt; 7&quot;</span>).show()</span><br><span class="line">+---+</span><br><span class="line">| <span class="built_in">id</span>|</span><br><span class="line">+---+</span><br><span class="line">|  <span class="number">8</span>|</span><br><span class="line">|  <span class="number">9</span>|</span><br><span class="line">+---+</span><br><span class="line"></span><br><span class="line">spark.sql(</span><br><span class="line">    <span class="string">&quot;SELECT * FROM range(10) WHERE id &gt; &#123;bound1&#125; AND id &lt; &#123;bound2&#125;&quot;</span>, bound1=<span class="number">7</span>, bound2=<span class="number">9</span></span><br><span class="line">).show()</span><br><span class="line">+---+</span><br><span class="line">| <span class="built_in">id</span>|</span><br><span class="line">+---+</span><br><span class="line">|  <span class="number">8</span>|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure>
<h3 id="保存hive表"><a class="markdownIt-Anchor" href="#保存hive表"></a> 保存HIve表</h3>
<p>DataFrames可以使用 saveAsTable 方法和 insertInto 方法将表保存到Hive中。</p>
<ul>
<li>调用 <strong>saveAsTable</strong> 保存，如果hive中不存在该表，则spark会自动创建此表。如果表已存在，则匹配插入数据和原表的 schema。</li>
<li>调用 <strong>insertInto</strong> 保存，无关schema，只需按数据的顺序插入，如果原表不存在则会报错。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Write a DataFrame into a Parquet file in a partitioned manner.</span></span><br><span class="line">df.write.mode(<span class="string">&#x27;overwrite&#x27;</span>).saveAsTable(<span class="string">&#x27;people&#x27;</span>)</span><br><span class="line">df.write.insertInto(<span class="string">&#x27;people&#x27;</span>, overwrite=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>同样，Hive表的保存模式有以下几种</p>
<table>
<thead>
<tr>
<th>DataFrame.write.mode</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>“error” or “errorifexists” (default)</td>
<td>如果数据已存在，则会报错</td>
</tr>
<tr>
<td>“append”</td>
<td>如果数据/表已存在，则追加到现有数据中</td>
</tr>
<tr>
<td>“overwrite”</td>
<td>如果数据/表已经存在，则覆盖原数据</td>
</tr>
<tr>
<td>“ignore”</td>
<td>如果数据/表已经存在，则忽略，不改变原数据</td>
</tr>
</tbody>
</table>
<p>saveAsTable 支持分桶、排序和分区</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.write.partitionBy(<span class="string">&quot;name&quot;</span>).sortBy(<span class="string">&quot;age&quot;</span>).saveAsTable(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"><span class="comment"># Write a DataFrame into a Parquet file in a bucketed manner.</span></span><br><span class="line">df.write.bucketBy(<span class="number">2</span>, <span class="string">&quot;name&quot;</span>).sortBy(<span class="string">&quot;age&quot;</span>).saveAsTable(<span class="string">&quot;bucketed_table&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>对于分区表，可以先开启<strong>Hive动态分区</strong>，则不需要指定分区字段。如果有一个分区，那么默认为数据中最后一列为分区字段，有两个分区则为最后两列为分区字段，以此类推。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark.conf.<span class="built_in">set</span>(<span class="string">&quot;hive.exec.dynamic.partition&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">spark.conf.<span class="built_in">set</span>(<span class="string">&quot;hive.exec.dynamic.partition.mode&quot;</span>, <span class="string">&quot;nonstrict&quot;</span>)</span><br><span class="line"></span><br><span class="line">df.write.insertInto(<span class="string">&#x27;people&#x27;</span>, overwrite=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>若通过 <code>path</code>选项<strong>自定义表路径</strong>，当表被 DROP 时，自定义表路径不会被删除，表数据仍然存在。如果没有指定 <code>path</code>，Spark会将数据写入仓库目录下的默认表路径，当表被删除时，默认表路径也将被删除。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.write.option(<span class="string">&quot;path&quot;</span>, <span class="string">&quot;/some/path&quot;</span>).saveAsTable(<span class="string">&quot;people&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>请注意，在创建外部数据源表（具有<code>path</code>选项的表）时，默认不会收集分区信息。您可以调用<code>MSCK REPAIR TABLE</code>同步元存储中的分区信息。</p>
<h2 id="常用方法"><a class="markdownIt-Anchor" href="#常用方法"></a> 常用方法</h2>
<h3 id="select"><a class="markdownIt-Anchor" href="#select"></a> Select</h3>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql</th>
<th style="text-align:left">查询语句</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>DataFrame.select(*cols)</code></td>
<td style="text-align:left"><code>SELECT</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.selectExpr(*expr)</code></td>
<td style="text-align:left">解析SQL 语句</td>
</tr>
<tr>
<td style="text-align:left"><code>function.expr(*expr)</code></td>
<td style="text-align:left">解析SQL 语句(函数)</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.alias(*alias, **kwargs)</code></td>
<td style="text-align:left">重命名列</td>
</tr>
<tr>
<td style="text-align:left"><code>funtions.lit(col)</code></td>
<td style="text-align:left">常数列（函数）</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.limit(num)</code></td>
<td style="text-align:left"><code>LIMIT</code> in SQL</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">29</span>, <span class="string">&quot;Michael&quot;</span>),</span><br><span class="line">    (<span class="number">30</span>, <span class="string">&quot;Andy&quot;</span>),</span><br><span class="line">    (<span class="number">19</span>, <span class="string">&quot;Justin&quot;</span>)</span><br><span class="line">], schema=[<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select everybody.</span></span><br><span class="line">df.select(<span class="string">&#x27;*&#x27;</span>).show()</span><br><span class="line">+---+-------+                                                                   </span><br><span class="line">|age|   name|</span><br><span class="line">+---+-------+</span><br><span class="line">| <span class="number">29</span>|Michael|</span><br><span class="line">| <span class="number">30</span>|   Andy|</span><br><span class="line">| <span class="number">19</span>| Justin|</span><br><span class="line">+---+-------+</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select the &quot;name&quot; and &quot;age&quot; columns</span></span><br><span class="line">df.select(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>).show()</span><br><span class="line">df.select([<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>]).show()</span><br><span class="line">+---+-------+                                                                   </span><br><span class="line">|age|   name|</span><br><span class="line">+---+-------+</span><br><span class="line">| <span class="number">29</span>|Michael|</span><br><span class="line">| <span class="number">30</span>|   Andy|</span><br><span class="line">| <span class="number">19</span>| Justin|</span><br><span class="line">+---+-------+</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select everybody, but increment the age by 1</span></span><br><span class="line">df.select(df[<span class="string">&#x27;name&#x27;</span>], df[<span class="string">&#x27;age&#x27;</span>] + <span class="number">1</span>).show()</span><br><span class="line">+-------+---------+</span><br><span class="line">|   name|(age + <span class="number">1</span>)|</span><br><span class="line">+-------+---------+</span><br><span class="line">|Michael|       <span class="number">30</span>|</span><br><span class="line">|   Andy|       <span class="number">31</span>|</span><br><span class="line">| Justin|       <span class="number">20</span>|</span><br><span class="line">+-------+---------+</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> fn</span><br><span class="line">df.select(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>, fn.lit(<span class="string">&quot;2020&quot;</span>).alias(<span class="string">&quot;update&quot;</span>)).show()</span><br><span class="line">+----+-------+------+</span><br><span class="line">| age|   name|update|</span><br><span class="line">+----+-------+------+</span><br><span class="line">|  <span class="number">29</span>|Michael|  <span class="number">2020</span>|</span><br><span class="line">|  <span class="number">30</span>|   Andy|  <span class="number">2020</span>|</span><br><span class="line">|  <span class="number">19</span>| Justin|  <span class="number">2020</span>|</span><br><span class="line">+----+-------+------+</span><br><span class="line"></span><br><span class="line">df.selectExpr(<span class="string">&quot;age * 2&quot;</span>, <span class="string">&quot;abs(age)&quot;</span>).show()</span><br><span class="line">+---------+--------+</span><br><span class="line">|(age * <span class="number">2</span>)|<span class="built_in">abs</span>(age)|</span><br><span class="line">+---------+--------+</span><br><span class="line">|       <span class="number">58</span>|      <span class="number">29</span>|</span><br><span class="line">|       <span class="number">60</span>|      <span class="number">30</span>|</span><br><span class="line">|       <span class="number">38</span>|      <span class="number">19</span>|</span><br><span class="line">+---------+--------+</span><br></pre></td></tr></table></figure>
<h3 id="column"><a class="markdownIt-Anchor" href="#column"></a> Column</h3>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql</th>
<th style="text-align:left">修改列</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>DataFrame.withColumnRenamed(existing, new)</code></td>
<td style="text-align:left">重命名列</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.withColumnsRenamed(colsMap)</code></td>
<td style="text-align:left">重命名多列</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.toDF(*cols)</code></td>
<td style="text-align:left">重命名所有列</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.withColumn(colName, col)</code></td>
<td style="text-align:left">修改单列</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.withColumns(*colsMap)</code></td>
<td style="text-align:left">修改多列</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.drop(*cols)</code></td>
<td style="text-align:left">删除列</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.to(schema)</code></td>
<td style="text-align:left">模式变换</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.astype(dataType)</code></td>
<td style="text-align:left">数据类型转换</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.cast(dataType)</code></td>
<td style="text-align:left">数据类型转换</td>
</tr>
<tr>
<td style="text-align:left"><code>funtions.cast(dataType)</code></td>
<td style="text-align:left">数据类型转换（函数）</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.when(condition, value)</code></td>
<td style="text-align:left"><code>CASE WHEN...THEN</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.otherwise(value)</code></td>
<td style="text-align:left"><code>ELSE</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>funtions.when(condition, value)</code></td>
<td style="text-align:left"><code>CASE WHEN...THEN</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>funtions.col(col)</code></td>
<td style="text-align:left">返回Column实例</td>
</tr>
<tr>
<td style="text-align:left"><code>funtions.lit(col)</code></td>
<td style="text-align:left">常数列</td>
</tr>
</tbody>
</table>
<p>在Python中，大多数情况都是通过属性或索引操作DataFrame，它会返回一个<code>Column</code>实例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># spark, df are from the previous example</span></span><br><span class="line"></span><br><span class="line">df.age</span><br><span class="line"><span class="comment"># Column&lt;&#x27;age&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;age&#x27;</span>]</span><br><span class="line"><span class="comment"># Column&lt;&#x27;age&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> fn</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">2</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">3</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">5</span>, <span class="string">&quot;Bob&quot;</span>), </span><br><span class="line">    (<span class="number">10</span>, <span class="string">&quot;Bob&quot;</span>)</span><br><span class="line">], [<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line">df.withColumn(<span class="string">&quot;baby&quot;</span>, fn.when(df[<span class="string">&#x27;age&#x27;</span>] &lt;= <span class="number">3</span>, <span class="number">1</span>).otherwise(<span class="number">0</span>)).show()</span><br><span class="line">+---+-----+----+</span><br><span class="line">|age| name|baby|</span><br><span class="line">+---+-----+----+</span><br><span class="line">|  <span class="number">2</span>|Alice|   <span class="number">1</span>|</span><br><span class="line">|  <span class="number">3</span>|Alice|   <span class="number">1</span>|</span><br><span class="line">|  <span class="number">5</span>|  Bob|   <span class="number">0</span>|</span><br><span class="line">| <span class="number">10</span>|  Bob|   <span class="number">0</span>|</span><br><span class="line">+---+-----+----+</span><br></pre></td></tr></table></figure>
<h3 id="distinct"><a class="markdownIt-Anchor" href="#distinct"></a> Distinct</h3>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql</th>
<th style="text-align:left">去重</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>DataFrame.distinct()</code></td>
<td style="text-align:left"><code>DISTINCT</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.dropDuplicates(subset=None)</code></td>
<td style="text-align:left">删除重复项</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">2</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">3</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">5</span>, <span class="string">&quot;Bob&quot;</span>), </span><br><span class="line">    (<span class="number">10</span>, <span class="string">&quot;Bob&quot;</span>)</span><br><span class="line">], [<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line">df.dropDuplicates(subset=[<span class="string">&quot;name&quot;</span>]).show()</span><br><span class="line">+---+-----+</span><br><span class="line">|age| name|</span><br><span class="line">+---+-----+</span><br><span class="line">|  <span class="number">5</span>|  Bob|</span><br><span class="line">|  <span class="number">2</span>|Alice|</span><br><span class="line">+---+-----+</span><br></pre></td></tr></table></figure>
<h3 id="filter"><a class="markdownIt-Anchor" href="#filter"></a> Filter</h3>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql</th>
<th style="text-align:left">条件筛选</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>DataFrame.filter(condition)</code></td>
<td style="text-align:left">筛选，别名<code>where</code></td>
</tr>
<tr>
<td style="text-align:left"><code>Column.isin(*cols)</code></td>
<td style="text-align:left"><code>IN (...)</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.contains(other)</code></td>
<td style="text-align:left"><code>LIKE &quot;%other%&quot;</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.like(pattern)</code></td>
<td style="text-align:left"><code>LIKE</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.rlike(pattern)</code></td>
<td style="text-align:left">正则表达式匹配</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.startswith(pattern)</code></td>
<td style="text-align:left">匹配开始</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.endswith(pattern)</code></td>
<td style="text-align:left">匹配结尾</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.between(lowerBound, upperBound)</code></td>
<td style="text-align:left"><code>BETWEEN ... AND ...</code> in SQL</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="literal">None</span>, <span class="string">&quot;Michael&quot;</span>),</span><br><span class="line">    (<span class="number">30</span>, <span class="string">&quot;Andy&quot;</span>),</span><br><span class="line">    (<span class="number">19</span>, <span class="string">&quot;Justin&quot;</span>)</span><br><span class="line">], schema=[<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select people older than 21</span></span><br><span class="line">df.<span class="built_in">filter</span>(df[<span class="string">&#x27;age&#x27;</span>] &gt; <span class="number">21</span>).show()</span><br><span class="line">+---+----+</span><br><span class="line">|age|name|</span><br><span class="line">+---+----+</span><br><span class="line">| <span class="number">30</span>|Andy|</span><br><span class="line">+---+----+</span><br><span class="line"></span><br><span class="line"><span class="comment"># Filter by Column instances,</span></span><br><span class="line"><span class="comment"># Or SQL expression in a string.</span></span><br><span class="line">df.<span class="built_in">filter</span>(df.age == <span class="number">30</span>).show()</span><br><span class="line">df.<span class="built_in">filter</span>(<span class="string">&quot;age = 30&quot;</span>).show()</span><br><span class="line">+---+----+</span><br><span class="line">|age|name|</span><br><span class="line">+---+----+</span><br><span class="line">| <span class="number">30</span>|Andy|</span><br><span class="line">+---+----+</span><br><span class="line"></span><br><span class="line">df.<span class="built_in">filter</span>(df.name.rlike(<span class="string">&quot;^A&quot;</span>)).show()</span><br><span class="line">df.<span class="built_in">filter</span>(df.name.startswith(<span class="string">&quot;A&quot;</span>)).show()</span><br><span class="line">+---+----+</span><br><span class="line">|age|name|</span><br><span class="line">+---+----+</span><br><span class="line">| <span class="number">30</span>|Andy|</span><br><span class="line">+---+----+</span><br><span class="line"></span><br><span class="line">df.<span class="built_in">filter</span>(df.name.isin(<span class="string">&#x27;Andy&#x27;</span>,<span class="string">&#x27;Justin&#x27;</span>)).show()</span><br><span class="line">+---+-------+                                                                   </span><br><span class="line">|age|   name|</span><br><span class="line">+---+-------+</span><br><span class="line">| <span class="number">30</span>|   Andy|</span><br><span class="line">| <span class="number">19</span>| Justin|</span><br><span class="line">+---+-------+</span><br></pre></td></tr></table></figure>
<h3 id="group"><a class="markdownIt-Anchor" href="#group"></a> Group</h3>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql</th>
<th style="text-align:left">分组和聚合</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>DataFrame.groupBy(*cols)</code></td>
<td style="text-align:left">返回GroupedData</td>
</tr>
<tr>
<td style="text-align:left"><code>GroupedData.count()</code></td>
<td style="text-align:left">计数</td>
</tr>
<tr>
<td style="text-align:left"><code>GroupedData.sum(*cols)</code></td>
<td style="text-align:left">求和</td>
</tr>
<tr>
<td style="text-align:left"><code>GroupedData.avg(*cols)</code></td>
<td style="text-align:left">平均值</td>
</tr>
<tr>
<td style="text-align:left"><code>GroupedData.mean(*cols)</code></td>
<td style="text-align:left">平均值</td>
</tr>
<tr>
<td style="text-align:left"><code>GroupedData.max(*cols)</code></td>
<td style="text-align:left">最大值</td>
</tr>
<tr>
<td style="text-align:left"><code>GroupedData.min(*cols)</code></td>
<td style="text-align:left">最小值</td>
</tr>
<tr>
<td style="text-align:left"><code>GroupedData.agg(*exprs)</code></td>
<td style="text-align:left">应用表达式</td>
</tr>
<tr>
<td style="text-align:left"><code>GroupedData.pivot(pivot_col, values)</code></td>
<td style="text-align:left">透视</td>
</tr>
</tbody>
</table>
<p>PySpark DataFrame支持按特定条件对数据进行分组，将函数应用于每个组，然后将它们合并返回DataFrame。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">2</span>, <span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;pink&quot;</span>), </span><br><span class="line">    (<span class="number">3</span>, <span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;pink&quot;</span>), </span><br><span class="line">    (<span class="number">5</span>, <span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;pink&quot;</span>), </span><br><span class="line">    (<span class="number">10</span>, <span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;orange&quot;</span>)</span><br><span class="line">], [<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;color&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Group-by name, and count each group.</span></span><br><span class="line">df.groupBy(<span class="string">&#x27;name&#x27;</span>).count().show()</span><br><span class="line">df.groupBy(df.name).agg(&#123;<span class="string">&quot;*&quot;</span>: <span class="string">&quot;count&quot;</span>&#125;).show()</span><br><span class="line">+-----+--------+</span><br><span class="line">| name|count(<span class="number">1</span>)|</span><br><span class="line">+-----+--------+</span><br><span class="line">|Alice|       <span class="number">2</span>|</span><br><span class="line">|  Bob|       <span class="number">2</span>|</span><br><span class="line">+-----+--------+</span><br></pre></td></tr></table></figure>
<p>聚合函数还可以使用 pyspark.sql.functions 模块提供的其他函数，如countDistinct, kurtosis, skewness, stddev, sumDistinct, variance 等。或者使用 pyspark.pandas API对每个组应用Python原生函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> fn</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> pandas_udf, PandasUDFType</span><br><span class="line"></span><br><span class="line"><span class="comment"># Group-by name, and calculate the minimum age.</span></span><br><span class="line">df.groupBy(<span class="string">&#x27;name&#x27;</span>).agg(fn.<span class="built_in">min</span>(df.age)).show() </span><br><span class="line">+-----+--------+</span><br><span class="line">| name|<span class="built_in">min</span>(age)|</span><br><span class="line">+-----+--------+</span><br><span class="line">|  Bob|       <span class="number">5</span>|</span><br><span class="line">|Alice|       <span class="number">2</span>|</span><br><span class="line">+-----+--------+</span><br><span class="line"></span><br><span class="line"><span class="comment"># Same as above but uses pandas UDF.</span></span><br><span class="line"><span class="meta">@pandas_udf(<span class="params"><span class="string">&#x27;int&#x27;</span>, PandasUDFType.GROUPED_AGG</span>)  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min_udf</span>(<span class="params">v</span>):</span></span><br><span class="line">    <span class="keyword">return</span> v.<span class="built_in">min</span>()</span><br><span class="line"></span><br><span class="line">df.groupBy(df.name).agg(min_udf(df.age)).sort(<span class="string">&quot;name&quot;</span>).show()  </span><br><span class="line">+-----+------------+                                                            </span><br><span class="line">| name|min_udf(age)|</span><br><span class="line">+-----+------------+</span><br><span class="line">|Alice|           <span class="number">2</span>|</span><br><span class="line">|  Bob|           <span class="number">5</span>|</span><br><span class="line">+-----+------------+</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>pyspark.sql</th>
<th>增强分组</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>DataFrame.cube(*cols)</code></td>
<td><code>CUBE</code> in SQL</td>
</tr>
<tr>
<td><code>DataFrame.rollup(*cols)</code></td>
<td><code>ROLLUP</code> in SQL</td>
</tr>
<tr>
<td><code>functions.grouping(col)</code></td>
<td><code>GROUPING</code> in SQL</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> grouping, <span class="built_in">min</span></span><br><span class="line">df.cube(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;color&quot;</span>).agg(grouping(<span class="string">&quot;name&quot;</span>), <span class="built_in">min</span>(<span class="string">&quot;age&quot;</span>)).show()</span><br><span class="line">+-----+------+--------------+--------+</span><br><span class="line">| name| color|grouping(name)|<span class="built_in">min</span>(age)|</span><br><span class="line">+-----+------+--------------+--------+</span><br><span class="line">|  Bob|  NULL|             <span class="number">0</span>|       <span class="number">5</span>|</span><br><span class="line">|  Bob|orange|             <span class="number">0</span>|      <span class="number">10</span>|</span><br><span class="line">| NULL|orange|             <span class="number">1</span>|      <span class="number">10</span>|</span><br><span class="line">|Alice|  NULL|             <span class="number">0</span>|       <span class="number">2</span>|</span><br><span class="line">| NULL|  NULL|             <span class="number">1</span>|       <span class="number">2</span>|</span><br><span class="line">|Alice|  pink|             <span class="number">0</span>|       <span class="number">2</span>|</span><br><span class="line">|  Bob|  pink|             <span class="number">0</span>|       <span class="number">5</span>|</span><br><span class="line">| NULL|  pink|             <span class="number">1</span>|       <span class="number">2</span>|</span><br><span class="line">+-----+------+--------------+--------+</span><br></pre></td></tr></table></figure>
<h3 id="sort"><a class="markdownIt-Anchor" href="#sort"></a> Sort</h3>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql</th>
<th style="text-align:left">排序</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>DataFrame.sort(*col, **kwargs)</code></td>
<td style="text-align:left"><code>ORDER BY</code> in SQL，别名orderBy</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.asc()</code></td>
<td style="text-align:left"><code>ASC</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.asc_nulls_first()</code></td>
<td style="text-align:left"><code>ASC</code> ，NULL放第一位</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.asc_nulls_last()</code></td>
<td style="text-align:left"><code>ASC</code> ，NULL放最后一位</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.desc()</code></td>
<td style="text-align:left"><code>DESC</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.desc_nulls_first</code></td>
<td style="text-align:left"><code>DESC</code>，NULL放第一位</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.desc_nulls_last</code></td>
<td style="text-align:left"><code>DESC</code>，NULL放最后一位</td>
</tr>
<tr>
<td style="text-align:left"><code>funtions.asc(col)</code></td>
<td style="text-align:left"><code>ASC</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>funtions.desc(col)</code></td>
<td style="text-align:left"><code>DESC</code> in SQL</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> fn</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">2</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">3</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">5</span>, <span class="string">&quot;Bob&quot;</span>), </span><br><span class="line">    (<span class="number">10</span>, <span class="string">&quot;Bob&quot;</span>)</span><br><span class="line">], [<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sort the DataFrame in descending order.</span></span><br><span class="line">df.sort(df.age.desc()).show()</span><br><span class="line">df.sort(<span class="string">&quot;age&quot;</span>, ascending=<span class="literal">False</span>).show()</span><br><span class="line">df.sort(fn.desc(<span class="string">&quot;age&quot;</span>)).show()</span><br><span class="line">+---+-----+</span><br><span class="line">|age| name|</span><br><span class="line">+---+-----+</span><br><span class="line">| <span class="number">10</span>|  Bob|</span><br><span class="line">|  <span class="number">5</span>|  Bob|</span><br><span class="line">|  <span class="number">3</span>|Alice|</span><br><span class="line">|  <span class="number">2</span>|Alice|</span><br><span class="line">+---+-----+</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify multiple columns</span></span><br><span class="line">df.orderBy(fn.desc(<span class="string">&quot;age&quot;</span>), <span class="string">&quot;name&quot;</span>).show()</span><br><span class="line">+---+-----+</span><br><span class="line">|age| name|</span><br><span class="line">+---+-----+</span><br><span class="line">| <span class="number">10</span>|  Bob|</span><br><span class="line">|  <span class="number">5</span>|  Bob|</span><br><span class="line">|  <span class="number">3</span>|Alice|</span><br><span class="line">|  <span class="number">2</span>|Alice|</span><br><span class="line">+---+-----+</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify multiple columns for sorting order at ascending.</span></span><br><span class="line">df.orderBy([<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>], ascending=[<span class="literal">False</span>, <span class="literal">False</span>]).show()</span><br><span class="line">+---+-----+</span><br><span class="line">|age| name|</span><br><span class="line">+---+-----+</span><br><span class="line">| <span class="number">10</span>|  Bob|</span><br><span class="line">|  <span class="number">5</span>|  Bob|</span><br><span class="line">|  <span class="number">3</span>|Alice|</span><br><span class="line">|  <span class="number">2</span>|Alice|</span><br><span class="line">+---+-----+</span><br></pre></td></tr></table></figure>
<h3 id="joins"><a class="markdownIt-Anchor" href="#joins"></a> Joins</h3>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql</th>
<th style="text-align:left">连接</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>DataFrame.alias(alias)</code></td>
<td style="text-align:left">重命名表</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.join(other, on, how)</code></td>
<td style="text-align:left">表连接</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.crossJoin(other)</code></td>
<td style="text-align:left">笛卡尔集</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.intersect(other)</code></td>
<td style="text-align:left">交集，并移除重复行</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.intersectAll(other)</code></td>
<td style="text-align:left">交集</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.subtract(other)</code></td>
<td style="text-align:left">差集</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.exceptAll(other)</code></td>
<td style="text-align:left"><code>EXCEPT ALL</code> in SQL</td>
</tr>
</tbody>
</table>
<p>其中表连接 <code>DataFrame.join(other, on, how)</code>支持的参数有：</p>
<ul>
<li>on: str, list or Column.</li>
<li>how: str, default inner.<br />
Must be one of: inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer, semi, leftsemi, left_semi, anti, leftanti and left_anti.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.createDataFrame([(<span class="number">2</span>, <span class="string">&quot;Alice&quot;</span>), (<span class="number">5</span>, <span class="string">&quot;Bob&quot;</span>)]).toDF(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>)</span><br><span class="line">df2 = spark.createDataFrame([(<span class="number">80</span>, <span class="string">&quot;Tom&quot;</span>), (<span class="number">85</span>, <span class="string">&quot;Bob&quot;</span>)]).toDF(<span class="string">&quot;height&quot;</span>, <span class="string">&quot;name&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inner join on columns (default)</span></span><br><span class="line">df.join(df2, <span class="string">&#x27;name&#x27;</span>).select(df.name, df2.height).show()</span><br><span class="line">+----+------+</span><br><span class="line">|name|height|</span><br><span class="line">+----+------+</span><br><span class="line">| Bob|    <span class="number">85</span>|</span><br><span class="line">+----+------+</span><br><span class="line"></span><br><span class="line"><span class="comment"># Outer join for both DataFrames on the ‘name’ column.</span></span><br><span class="line">df.join(df2, df.name == df2.name, <span class="string">&#x27;outer&#x27;</span>).select(</span><br><span class="line">    df.name, df2.height).sort(fn.desc(<span class="string">&quot;name&quot;</span>)).show()</span><br><span class="line">+-----+------+</span><br><span class="line">| name|height|</span><br><span class="line">+-----+------+</span><br><span class="line">|  Bob|    <span class="number">85</span>|</span><br><span class="line">|Alice|  NULL|</span><br><span class="line">| NULL|    <span class="number">80</span>|</span><br><span class="line">+-----+------+</span><br></pre></td></tr></table></figure>
<h3 id="union"><a class="markdownIt-Anchor" href="#union"></a> Union</h3>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql</th>
<th style="text-align:left">合并</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>DataFrame.union(other)</code></td>
<td style="text-align:left"><code>UNION ALl</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.unionAll(other)</code></td>
<td style="text-align:left"><code>UNION ALL</code> in SQL</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1 = spark.createDataFrame([(<span class="number">2</span>, <span class="string">&quot;Alice&quot;</span>), (<span class="number">5</span>, <span class="string">&quot;Bob&quot;</span>)], [<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line">df2 = spark.createDataFrame([(<span class="number">3</span>, <span class="string">&quot;Charlie&quot;</span>), (<span class="number">4</span>, <span class="string">&quot;Dave&quot;</span>)], [<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line">df1.union(df2).show()</span><br><span class="line">+---+-------+</span><br><span class="line">|age|   name|</span><br><span class="line">+---+-------+</span><br><span class="line">|  <span class="number">2</span>|  Alice|</span><br><span class="line">|  <span class="number">5</span>|    Bob|</span><br><span class="line">|  <span class="number">3</span>|Charlie|</span><br><span class="line">|  <span class="number">4</span>|   Dave|</span><br><span class="line">+---+-------+</span><br></pre></td></tr></table></figure>
<h3 id="pivot"><a class="markdownIt-Anchor" href="#pivot"></a> Pivot</h3>
<table>
<thead>
<tr>
<th>pyspark.sql</th>
<th>透视</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>GroupedData.pivot(pivot_col, values)</code></td>
<td>透视</td>
</tr>
<tr>
<td><code>DataFrame.unpivot(ids, values, variableColumnName, …)</code></td>
<td>逆透视，别名melt</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">2</span>, <span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;pink&quot;</span>), </span><br><span class="line">    (<span class="number">3</span>, <span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;pink&quot;</span>), </span><br><span class="line">    (<span class="number">5</span>, <span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;pink&quot;</span>), </span><br><span class="line">    (<span class="number">10</span>, <span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;orange&quot;</span>)</span><br><span class="line">], [<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;color&quot;</span>])</span><br><span class="line"></span><br><span class="line">pivot_df = df.groupBy(<span class="string">&quot;name&quot;</span>).pivot(<span class="string">&quot;color&quot;</span>).avg(<span class="string">&quot;age&quot;</span>)</span><br><span class="line">pivot_df.show()</span><br><span class="line">+-----+------+----+</span><br><span class="line">| name|orange|pink|</span><br><span class="line">+-----+------+----+</span><br><span class="line">|  Bob|  <span class="number">10.0</span>| <span class="number">5.0</span>|</span><br><span class="line">|Alice|  NULL| <span class="number">2.5</span>|</span><br><span class="line">+-----+------+----+</span><br><span class="line"></span><br><span class="line">pivot_df.unpivot(<span class="string">&quot;name&quot;</span>, [<span class="string">&quot;pink&quot;</span>, <span class="string">&quot;orange&quot;</span>], <span class="string">&quot;color&quot;</span>, <span class="string">&quot;avg_age&quot;</span>).show()</span><br><span class="line">+-----+------+-------+</span><br><span class="line">| name| color|avg_age|</span><br><span class="line">+-----+------+-------+</span><br><span class="line">|  Bob|  pink|    <span class="number">5.0</span>|</span><br><span class="line">|  Bob|orange|   <span class="number">10.0</span>|</span><br><span class="line">|Alice|  pink|    <span class="number">2.5</span>|</span><br><span class="line">|Alice|orange|   NULL|</span><br><span class="line">+-----+------+-------+</span><br></pre></td></tr></table></figure>
<h3 id="window"><a class="markdownIt-Anchor" href="#window"></a> Window</h3>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql</th>
<th style="text-align:left">窗口定义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>Column.over(window)</code></td>
<td style="text-align:left"><code>OVER</code> 子句</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.Window.partitionBy(*cols)</code></td>
<td style="text-align:left">窗口内分区</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.Window.orderBy(*cols)</code></td>
<td style="text-align:left">窗口内排序</td>
</tr>
<tr>
<td style="text-align:left"><code>functionsWindow.rangeBetween(start, end)</code></td>
<td style="text-align:left">窗口区间</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.Window.rowsBetween(start, end)</code></td>
<td style="text-align:left">窗口区间</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.Window.unboundedFollowing</code></td>
<td style="text-align:left">最后一行</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.Window.unboundedPreceding</code></td>
<td style="text-align:left">第一行</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.Window.currentRow</code></td>
<td style="text-align:left">当前行</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Window</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> <span class="built_in">min</span>, desc</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">2</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">3</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">5</span>, <span class="string">&quot;Bob&quot;</span>), </span><br><span class="line">    (<span class="number">10</span>, <span class="string">&quot;Bob&quot;</span>)</span><br><span class="line">], [<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line">window = Window.partitionBy(<span class="string">&quot;name&quot;</span>)</span><br><span class="line"></span><br><span class="line">df.withColumn(</span><br><span class="line">     <span class="string">&quot;min&quot;</span>, <span class="built_in">min</span>(<span class="string">&#x27;age&#x27;</span>).over(window)</span><br><span class="line">).sort(desc(<span class="string">&quot;age&quot;</span>)).show()</span><br><span class="line">+---+-----+---+</span><br><span class="line">|age| name|<span class="built_in">min</span>|</span><br><span class="line">+---+-----+---+</span><br><span class="line">| <span class="number">10</span>|  Bob|  <span class="number">5</span>|</span><br><span class="line">|  <span class="number">5</span>|  Bob|  <span class="number">5</span>|</span><br><span class="line">|  <span class="number">3</span>|Alice|  <span class="number">2</span>|</span><br><span class="line">|  <span class="number">2</span>|Alice|  <span class="number">2</span>|</span><br><span class="line">+---+-----+---+</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql.functions</th>
<th style="text-align:left">窗口函数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>row_number()</code></td>
<td style="text-align:left">序号</td>
</tr>
<tr>
<td style="text-align:left"><code>dense_rank()</code></td>
<td style="text-align:left">排名，排名相等则下一个序号不间断</td>
</tr>
<tr>
<td style="text-align:left"><code>rank()</code></td>
<td style="text-align:left">排名，排名相等则下一个序号间断</td>
</tr>
<tr>
<td style="text-align:left"><code>percent_rank()</code></td>
<td style="text-align:left">百分比排名</td>
</tr>
<tr>
<td style="text-align:left"><code>cume_dist()</code></td>
<td style="text-align:left">累积分布</td>
</tr>
<tr>
<td style="text-align:left"><code>lag(col, offset, default)</code></td>
<td style="text-align:left">返回往上第 offset 行值</td>
</tr>
<tr>
<td style="text-align:left"><code>lead(col, offset, default)</code></td>
<td style="text-align:left">返回往下第 offset 行值</td>
</tr>
<tr>
<td style="text-align:left"><code>nth_value(col, offset, ignoreNulls)</code></td>
<td style="text-align:left">返回第 offset 行值</td>
</tr>
<tr>
<td style="text-align:left"><code>ntile(n)</code></td>
<td style="text-align:left">切成 n 个桶，返回当前切片号</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> cume_dist, row_number</span><br><span class="line"></span><br><span class="line">w = Window.orderBy(<span class="string">&quot;age&quot;</span>)</span><br><span class="line">df.withColumn(<span class="string">&quot;cume_dist&quot;</span>, cume_dist().over(w)) \</span><br><span class="line">  .withColumn(<span class="string">&quot;asc_order&quot;</span>, row_number().over(w)).show()</span><br><span class="line">+---+-----+---------+---------+</span><br><span class="line">|age| name|cume_dist|asc_order|</span><br><span class="line">+---+-----+---------+---------+</span><br><span class="line">|  <span class="number">2</span>|Alice|     <span class="number">0.25</span>|        <span class="number">1</span>|</span><br><span class="line">|  <span class="number">3</span>|Alice|      <span class="number">0.5</span>|        <span class="number">2</span>|</span><br><span class="line">|  <span class="number">5</span>|  Bob|     <span class="number">0.75</span>|        <span class="number">3</span>|</span><br><span class="line">| <span class="number">10</span>|  Bob|      <span class="number">1.0</span>|        <span class="number">4</span>|</span><br><span class="line">+---+-----+---------+---------+</span><br></pre></td></tr></table></figure>
<h2 id="函数"><a class="markdownIt-Anchor" href="#函数"></a> 函数</h2>
<h3 id="built-in-functions"><a class="markdownIt-Anchor" href="#built-in-functions"></a> Built-In Functions</h3>
<p>Spark SQL 还拥有丰富的函数库，包括字符串操作、日期算术、常见数学运算、集合函数，聚合函数等。常用的函数有下面几个</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html">PySpark SQL functions API</a></p>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql</th>
<th style="text-align:left">函数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>functions.coalesce(*cols)</code></td>
<td style="text-align:left"><code>COALESCE</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.nvl(col1, col2)</code></td>
<td style="text-align:left"><code>NVL</code> in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.greatest(*cols)</code></td>
<td style="text-align:left">最大列的值</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.least(*cols)</code></td>
<td style="text-align:left">最小列的值</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.monotonically_increasing_id()</code></td>
<td style="text-align:left">单调递增ID列</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.rand(seed)</code></td>
<td style="text-align:left">生成随机列，服从 0-1 均匀分布</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.randn(seed)</code></td>
<td style="text-align:left">生成随机列，服从标准正态分布</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.substr(startPos, length)</code></td>
<td style="text-align:left">截取字符串</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.substr(str, pos[, len])</code></td>
<td style="text-align:left">截取字符串(函数)</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.concat_ws(sep, *cols)</code></td>
<td style="text-align:left">连接字符串(函数)</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.randomSplit(weights, seed)</code></td>
<td style="text-align:left">拆分数据集</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.replace(to_replace, value, subset)</code></td>
<td style="text-align:left">替换值</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.sample(withReplacement, fraction, seed)</code></td>
<td style="text-align:left">抽样</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.sampleBy(col, fractions, seed)</code></td>
<td style="text-align:left">抽样</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> fn</span><br><span class="line">spark.<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">7</span>, <span class="number">2</span>).withColumn(<span class="string">&quot;rand&quot;</span>, fn.rand()).show()</span><br><span class="line">+---+--------------------+</span><br><span class="line">| <span class="built_in">id</span>|                rand|</span><br><span class="line">+---+--------------------+</span><br><span class="line">|  <span class="number">1</span>|  -<span class="number">1.424479548864398</span>|</span><br><span class="line">|  <span class="number">3</span>|-<span class="number">0.08662546937327156</span>|</span><br><span class="line">|  <span class="number">5</span>| -<span class="number">0.5638136550015606</span>|</span><br><span class="line">+---+--------------------+</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql</th>
<th style="text-align:left">集合函数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>functions.collect_set(col)</code></td>
<td style="text-align:left">行收集成数组，消除重复元素</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.collect_list(col)</code></td>
<td style="text-align:left">行收集成数组，具有重复项</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.explode(col)</code></td>
<td style="text-align:left">数组分解成单列</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">2</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">3</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">5</span>, <span class="string">&quot;Bob&quot;</span>), </span><br><span class="line">    (<span class="number">10</span>, <span class="string">&quot;Bob&quot;</span>)</span><br><span class="line">], [<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line">df2 = df.groupBy(<span class="string">&quot;name&quot;</span>).agg(fn.collect_list(<span class="string">&quot;age&quot;</span>).alias(<span class="string">&quot;age&quot;</span>))</span><br><span class="line">df2.show()</span><br><span class="line">+-----+-------+</span><br><span class="line">| name|    age|</span><br><span class="line">+-----+-------+</span><br><span class="line">|  Bob|[<span class="number">5</span>, <span class="number">10</span>]|</span><br><span class="line">|Alice| [<span class="number">2</span>, <span class="number">3</span>]|</span><br><span class="line">+-----+-------+</span><br><span class="line"></span><br><span class="line">df2.select(<span class="string">&quot;name&quot;</span>, fn.explode(<span class="string">&quot;age&quot;</span>).alias(<span class="string">&quot;age&quot;</span>)).show()</span><br><span class="line">+-----+---+</span><br><span class="line">| name|age|</span><br><span class="line">+-----+---+</span><br><span class="line">|  Bob|  <span class="number">5</span>|</span><br><span class="line">|  Bob| <span class="number">10</span>|</span><br><span class="line">|Alice|  <span class="number">2</span>|</span><br><span class="line">|Alice|  <span class="number">3</span>|</span><br><span class="line">+-----+---+</span><br></pre></td></tr></table></figure>
<h3 id="python-udfs"><a class="markdownIt-Anchor" href="#python-udfs"></a> Python UDFs</h3>
<p>定义python UDF需要使用<code>functions.udf()</code>作为装饰器或包装函数 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Declare the function and create the UDF</span></span><br><span class="line"><span class="meta">@udf(<span class="params">returnType=<span class="string">&#x27;int&#x27;</span></span>)  </span><span class="comment"># A default, pickled Python UDF</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plus_one</span>(<span class="params">s</span>):</span> <span class="comment"># type: ignore[no-untyped-<span class="keyword">def</span>]</span></span><br><span class="line">    <span class="keyword">return</span> s + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">29</span>, <span class="string">&quot;Michael&quot;</span>),</span><br><span class="line">    (<span class="number">30</span>, <span class="string">&quot;Andy&quot;</span>),</span><br><span class="line">    (<span class="number">19</span>, <span class="string">&quot;Justin&quot;</span>)</span><br><span class="line">], schema=[<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line">df.select(plus_one(<span class="string">&quot;age&quot;</span>)).show()</span><br><span class="line">+-------------+</span><br><span class="line">|plus_one(age)|</span><br><span class="line">+-------------+</span><br><span class="line">|           <span class="number">30</span>|</span><br><span class="line">|           <span class="number">31</span>|</span><br><span class="line">|           <span class="number">20</span>|</span><br><span class="line">+-------------+</span><br></pre></td></tr></table></figure>
<p>此外，本章定义的任意UDF都可以在SQL中注册并调用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Declare the function and create the UDF</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plus_one_func</span>(<span class="params">s</span>):</span> </span><br><span class="line">    <span class="keyword">return</span> s + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">plus_one = udf(plus_one_func, returnType=<span class="string">&#x27;int&#x27;</span>) <span class="comment"># type: ignore[no-untyped-<span class="keyword">def</span>]</span></span><br><span class="line">df.select(plus_one(<span class="string">&quot;age&quot;</span>)).show()</span><br><span class="line">+------------------+</span><br><span class="line">|plus_one_func(age)|</span><br><span class="line">+------------------+</span><br><span class="line">|                <span class="number">30</span>|</span><br><span class="line">|                <span class="number">31</span>|</span><br><span class="line">|                <span class="number">20</span>|</span><br><span class="line">+------------------+</span><br><span class="line"></span><br><span class="line"><span class="comment"># Register the DataFrame as a SQL temporary view</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;tableA&quot;</span>)</span><br><span class="line"></span><br><span class="line">spark.udf.register(<span class="string">&quot;plus_one&quot;</span>, plus_one)</span><br><span class="line">spark.sql(<span class="string">&quot;SELECT plus_one(age) FROM tableA&quot;</span>).show()</span><br><span class="line">+-------------+</span><br><span class="line">|plus_one(age)|</span><br><span class="line">+-------------+</span><br><span class="line">|           <span class="number">30</span>|</span><br><span class="line">|           <span class="number">31</span>|</span><br><span class="line">|           <span class="number">20</span>|</span><br><span class="line">+-------------+</span><br></pre></td></tr></table></figure>
<h3 id="arrow-python-udfs"><a class="markdownIt-Anchor" href="#arrow-python-udfs"></a> Arrow Python UDFs</h3>
<p><strong>Apache Arrow</strong> 是一种内存中的列式数据格式，用于在 Spark 中高效传输 JVM 和 Python 进程之间的数据。这目前对 Python 用户使用 Pandas/NumPy 数据最有利 。默认为禁用状态，需要配置<code>spark.sql.execution.arrow.pyspark.enabled</code> 启用。</p>
<p>例如，当Pandas DataFrame和Spark DataFrame相互转化时，可通过设置 Arrow 进行优化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable Arrow-based columnar data transfers</span></span><br><span class="line">spark.conf.<span class="built_in">set</span>(<span class="string">&quot;spark.sql.execution.arrow.pyspark.enabled&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate a Pandas DataFrame</span></span><br><span class="line">pdf = pd.DataFrame(np.random.rand(<span class="number">100</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a Spark DataFrame from a Pandas DataFrame using Arrow</span></span><br><span class="line">df = spark.createDataFrame(pdf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the Spark DataFrame back to a Pandas DataFrame using Arrow</span></span><br><span class="line">result_pdf = df.select(<span class="string">&quot;*&quot;</span>).toPandas()</span><br></pre></td></tr></table></figure>
<p><strong>Arrow Batch Size</strong>：Arrow 批处理可能会暂时导致 JVM 中的内存使用率较高。为避免出现内存不足异常，可通过设置<code>spark.sql.execution.arrow.maxRecordsPerBatch</code>来调整每个批次的最大行数。</p>
<p><strong>Arrow Python UDF</strong> 是利用 Arrow 高效批处理的自定义函数。通过设置udf()的参数<code>useArrow=True</code>来定义Arrow Python UDF 。</p>
<p>下面是一个示例，演示了Arrow Python UDF 的用法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</span><br><span class="line"></span><br><span class="line"><span class="meta">@udf(<span class="params">returnType=<span class="string">&#x27;int&#x27;</span>, useArrow=<span class="literal">True</span></span>)  </span><span class="comment"># An Arrow Python UDF</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">arrow_plus_one</span>(<span class="params">s</span>):</span> <span class="comment"># type: ignore[no-untyped-<span class="keyword">def</span>]</span></span><br><span class="line">    <span class="keyword">return</span> s + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">29</span>, <span class="string">&quot;Michael&quot;</span>),</span><br><span class="line">    (<span class="number">30</span>, <span class="string">&quot;Andy&quot;</span>),</span><br><span class="line">    (<span class="number">19</span>, <span class="string">&quot;Justin&quot;</span>)</span><br><span class="line">], schema=[<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line">df.select(arrow_plus_one(<span class="string">&quot;age&quot;</span>)).show()</span><br><span class="line">+-------------------+</span><br><span class="line">|arrow_plus_one(age)|</span><br><span class="line">+-------------------+</span><br><span class="line">|                 <span class="number">30</span>|</span><br><span class="line">|                 <span class="number">31</span>|</span><br><span class="line">|                 <span class="number">20</span>|</span><br><span class="line">+-------------------+</span><br></pre></td></tr></table></figure>
<p>此外，您还可以启用 <code>spark.sql.execution.pythonUDF.arrow.enabled</code>，在整个 SparkSession 中对 Python UDF 进行优化。与默认的序列化 Python UDF 相比，Arrow Python UDF 提供了更连贯的类型强制机制。</p>
<h3 id="pandas-udfs"><a class="markdownIt-Anchor" href="#pandas-udfs"></a> Pandas UDFs</h3>
<p>Pandas UDFs是用户定义的函数，由Spark执行，使用Arrow传输数据，支持矢量化操作。</p>
<p>定义一个 Pandas UDF需要使用pandas_udf() 作为装饰器或包装函数。注意，定义Pandas UDF时，需要定义数据类型提示。</p>
<p>以下示例演示如何创建Series to Series的 Pandas UDF。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> pandas_udf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Declare the function and create the UDF</span></span><br><span class="line"><span class="meta">@pandas_udf(<span class="params"><span class="string">&#x27;long&#x27;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pandas_plus_one</span>(<span class="params">series: pd.Series</span>) -&gt; pd.Series:</span></span><br><span class="line">    <span class="comment"># Simply plus one by using pandas Series.</span></span><br><span class="line">    <span class="keyword">return</span> series + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">29</span>, <span class="string">&quot;Michael&quot;</span>),</span><br><span class="line">    (<span class="number">30</span>, <span class="string">&quot;Andy&quot;</span>),</span><br><span class="line">    (<span class="number">19</span>, <span class="string">&quot;Justin&quot;</span>)</span><br><span class="line">], schema=[<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Execute function as a Spark vectorized UDF</span></span><br><span class="line">df.select(pandas_plus_one(<span class="string">&#x27;age&#x27;</span>)).show()</span><br><span class="line">+--------------------+</span><br><span class="line">|pandas_plus_one(age)|</span><br><span class="line">+--------------------+</span><br><span class="line">|                  <span class="number">30</span>|</span><br><span class="line">|                  <span class="number">31</span>|</span><br><span class="line">|                  <span class="number">20</span>|</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure>
<p>聚合 UDF 也可以与 GroupedData 或Window 一起使用。此外，计算时组内所有数据将被加载到内存中。</p>
<p>以下示例演示如何使用 Series to Scalar UDF 通过分组计算均值和窗口操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> pandas_udf</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Window</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">2</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">3</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">5</span>, <span class="string">&quot;Bob&quot;</span>), </span><br><span class="line">    (<span class="number">10</span>, <span class="string">&quot;Bob&quot;</span>)</span><br><span class="line">], [<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Declare the function and create the UDF</span></span><br><span class="line"><span class="meta">@pandas_udf(<span class="params"><span class="string">&quot;double&quot;</span></span>)  </span><span class="comment"># type: ignore[call-overload]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_udf</span>(<span class="params">s: pd.Series</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">    <span class="keyword">return</span> s.mean()</span><br><span class="line"></span><br><span class="line">df.select(mean_udf(<span class="string">&#x27;age&#x27;</span>)).show()</span><br><span class="line">+-------------+</span><br><span class="line">|mean_udf(age)|</span><br><span class="line">+-------------+</span><br><span class="line">|          <span class="number">5.0</span>|</span><br><span class="line">+-------------+</span><br><span class="line"></span><br><span class="line">df.groupby(<span class="string">&quot;name&quot;</span>).agg(mean_udf(<span class="string">&#x27;age&#x27;</span>)).show()</span><br><span class="line">+-----+-------------+</span><br><span class="line">| name|mean_udf(age)|</span><br><span class="line">+-----+-------------+</span><br><span class="line">|Alice|          <span class="number">2.5</span>|</span><br><span class="line">|  Bob|          <span class="number">7.5</span>|</span><br><span class="line">+-----+-------------+</span><br><span class="line"></span><br><span class="line">w = Window.partitionBy(<span class="string">&#x27;name&#x27;</span>) </span><br><span class="line">df.select(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, mean_udf(<span class="string">&#x27;age&#x27;</span>).over(w).alias(<span class="string">&quot;mean_age&quot;</span>)).show()</span><br><span class="line">+-----+---+--------+</span><br><span class="line">| name|age|mean_age|</span><br><span class="line">+-----+---+--------+</span><br><span class="line">|Alice|  <span class="number">2</span>|     <span class="number">2.5</span>|</span><br><span class="line">|Alice|  <span class="number">3</span>|     <span class="number">2.5</span>|</span><br><span class="line">|  Bob|  <span class="number">5</span>|     <span class="number">7.5</span>|</span><br><span class="line">|  Bob| <span class="number">10</span>|     <span class="number">7.5</span>|</span><br><span class="line">+-----+---+--------+</span><br></pre></td></tr></table></figure>
<h3 id="pandas-function-api"><a class="markdownIt-Anchor" href="#pandas-function-api"></a> Pandas Function API</h3>
<p>Pandas Function API 与Pandas UDF类似，使用Arrow传输数据，使用Pandas处理数据，允许矢量化操作。然而，Pandas Function API是直接作用于PySpark DataFrame而不是Column。</p>
<ul>
<li>GroupedData.applyInPandas()支持分组应用 UDF。</li>
<li>GroupedData.cogroup().applyInPandas()支持联合分组应用UDF，它允许两个PySpark DataFrame由一个公共密钥联合分组，然后将Python函数应用于每个联合组。</li>
</ul>
<p>映射函数需要定义以下内容：</p>
<ul>
<li>定义应用于每个组的原生函数，该函数的输入和输出都是<code>pandas.DataFrame</code></li>
<li>使用<code>StructType</code>对象或DDL模式的字符串定义输出数据类型。</li>
</ul>
<p>请注意，在应用函数之前，组的所有数据都将加载到内存中，这可能会导致内存不足。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">2</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">3</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">5</span>, <span class="string">&quot;Bob&quot;</span>), </span><br><span class="line">    (<span class="number">10</span>, <span class="string">&quot;Bob&quot;</span>)</span><br><span class="line">], [<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subtract_mean</span>(<span class="params">pdf: pd.DataFrame</span>) -&gt; pd.DataFrame:</span></span><br><span class="line">    <span class="comment"># pdf is a pandas.DataFrame</span></span><br><span class="line">    age = pdf.age</span><br><span class="line">    <span class="keyword">return</span> pdf.assign(age=age - age.mean())</span><br><span class="line"></span><br><span class="line">df.groupby(<span class="string">&quot;name&quot;</span>).applyInPandas(subtract_mean, schema=<span class="string">&quot;name string, age double&quot;</span>).show()</span><br><span class="line">+-----+----+</span><br><span class="line">| name| age|</span><br><span class="line">+-----+----+</span><br><span class="line">|Alice|-<span class="number">0.5</span>|</span><br><span class="line">|Alice| <span class="number">0.5</span>|</span><br><span class="line">|  Bob|-<span class="number">2.5</span>|</span><br><span class="line">|  Bob| <span class="number">2.5</span>|</span><br><span class="line">+-----+----+</span><br></pre></td></tr></table></figure>
<h3 id="udtfs"><a class="markdownIt-Anchor" href="#udtfs"></a> UDTFs</h3>
<p>Spark 3.5 引入了 Python 用户定义表函数 （UDTF），每次返回整个表作为输出。</p>
<p>实现 UDTF 类并创建 UDTF：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Define the UDTF class and implement the required `eval` method.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SquareNumbers</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval</span>(<span class="params">self, start: <span class="built_in">int</span>, end: <span class="built_in">int</span></span>):</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">range</span>(start, end + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">yield</span> (num, num * num)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lit, udtf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a UDTF using the class definition and the `udtf` function.</span></span><br><span class="line">square_num = udtf(SquareNumbers, returnType=<span class="string">&quot;num: int, squared: int&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Invoke the UDTF in PySpark.</span></span><br><span class="line">square_num(lit(<span class="number">1</span>), lit(<span class="number">3</span>)).show()</span><br><span class="line">+---+-------+</span><br><span class="line">|num|squared|</span><br><span class="line">+---+-------+</span><br><span class="line">|  <span class="number">1</span>|      <span class="number">1</span>|</span><br><span class="line">|  <span class="number">2</span>|      <span class="number">4</span>|</span><br><span class="line">|  <span class="number">3</span>|      <span class="number">9</span>|</span><br><span class="line">+---+-------+</span><br></pre></td></tr></table></figure>
<p>还可以使用装饰器语法创建 UDTF：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lit, udtf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a UDTF using the `udtf` decorator directly on the class.</span></span><br><span class="line"><span class="meta">@udtf(<span class="params">returnType=<span class="string">&quot;num: int, squared: int&quot;</span></span>)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SquareNumbers</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval</span>(<span class="params">self, start: <span class="built_in">int</span>, end: <span class="built_in">int</span></span>):</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">range</span>(start, end + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">yield</span> (num, num * num)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Invoke the UDTF in PySpark using the SquareNumbers class directly.</span></span><br><span class="line">SquareNumbers(lit(<span class="number">1</span>), lit(<span class="number">3</span>)).show()</span><br><span class="line">+---+-------+</span><br><span class="line">|num|squared|</span><br><span class="line">+---+-------+</span><br><span class="line">|  <span class="number">1</span>|      <span class="number">1</span>|</span><br><span class="line">|  <span class="number">2</span>|      <span class="number">4</span>|</span><br><span class="line">|  <span class="number">3</span>|      <span class="number">9</span>|</span><br><span class="line">+---+-------+</span><br></pre></td></tr></table></figure>
<p>创建 UDTF 时，也可以设置参数<code>useArrow=True</code> 启用 Arrow 优化。</p>
<p>Python UDTF 也可以注册并在 SQL 查询中使用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Register the UDTF for use in Spark SQL.</span></span><br><span class="line">spark.udtf.register(<span class="string">&quot;square_numbers&quot;</span>, SquareNumbers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using the UDTF in SQL.</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM square_numbers(1, 3)&quot;</span>).show()</span><br><span class="line">+---+-------+</span><br><span class="line">|num|squared|</span><br><span class="line">+---+-------+</span><br><span class="line">|  <span class="number">1</span>|      <span class="number">1</span>|</span><br><span class="line">|  <span class="number">2</span>|      <span class="number">4</span>|</span><br><span class="line">|  <span class="number">3</span>|      <span class="number">9</span>|</span><br><span class="line">+---+-------+</span><br></pre></td></tr></table></figure>
<h2 id="统计信息"><a class="markdownIt-Anchor" href="#统计信息"></a> 统计信息</h2>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql</th>
<th style="text-align:left">统计信息</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>DataFrame.describe(*cols)</code></td>
<td style="text-align:left">描述性统计</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.summary(*statistics)</code></td>
<td style="text-align:left">描述性统计</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.count()</code></td>
<td style="text-align:left">行数</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.agg(*exprs)</code></td>
<td style="text-align:left">聚合</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.approxQuantile(col, prob, relativeError)</code></td>
<td style="text-align:left">百分位数</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.corr(col1, col2, method=None)</code></td>
<td style="text-align:left">相关系数</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.cov(col1, col2)</code></td>
<td style="text-align:left">方差</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.freqItems(cols, support)</code></td>
<td style="text-align:left">收集频繁条目</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.observe(observation, *exprs)</code></td>
<td style="text-align:left">提取统计信息</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">2</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">3</span>, <span class="string">&quot;Alice&quot;</span>), </span><br><span class="line">    (<span class="number">5</span>, <span class="string">&quot;Bob&quot;</span>), </span><br><span class="line">    (<span class="number">10</span>, <span class="string">&quot;Bob&quot;</span>)</span><br><span class="line">], [<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line">df.count()</span><br><span class="line"><span class="comment"># 4</span></span><br><span class="line"></span><br><span class="line">df.describe().show()</span><br><span class="line">+-------+------------------+-----+</span><br><span class="line">|summary|               age| name|</span><br><span class="line">+-------+------------------+-----+</span><br><span class="line">|  count|                 <span class="number">4</span>|    <span class="number">4</span>|</span><br><span class="line">|   mean|               <span class="number">5.0</span>| NULL|</span><br><span class="line">| stddev|<span class="number">3.5590260840104366</span>| NULL|</span><br><span class="line">|    <span class="built_in">min</span>|                 <span class="number">2</span>|Alice|</span><br><span class="line">|    <span class="built_in">max</span>|                <span class="number">10</span>|  Bob|</span><br><span class="line">+-------+------------------+-----+</span><br><span class="line"></span><br><span class="line">df.summary().show()</span><br><span class="line">+-------+------------------+-----+</span><br><span class="line">|summary|               age| name|</span><br><span class="line">+-------+------------------+-----+</span><br><span class="line">|  count|                 <span class="number">4</span>|    <span class="number">4</span>|</span><br><span class="line">|   mean|               <span class="number">5.0</span>| NULL|</span><br><span class="line">| stddev|<span class="number">3.5590260840104366</span>| NULL|</span><br><span class="line">|    <span class="built_in">min</span>|                 <span class="number">2</span>|Alice|</span><br><span class="line">|    <span class="number">25</span>%|                 <span class="number">2</span>| NULL|</span><br><span class="line">|    <span class="number">50</span>%|                 <span class="number">3</span>| NULL|</span><br><span class="line">|    <span class="number">75</span>%|                 <span class="number">5</span>| NULL|</span><br><span class="line">|    <span class="built_in">max</span>|                <span class="number">10</span>|  Bob|</span><br><span class="line">+-------+------------------+-----+</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col, count, lit, <span class="built_in">max</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Observation</span><br><span class="line"></span><br><span class="line">observation = Observation(<span class="string">&quot;my metrics&quot;</span>)</span><br><span class="line">observed_df = df.observe(observation, count(lit(<span class="number">1</span>)).alias(<span class="string">&quot;count&quot;</span>), <span class="built_in">max</span>(col(<span class="string">&quot;age&quot;</span>)))</span><br><span class="line">observed_df.show()</span><br><span class="line">+---+-----+</span><br><span class="line">|age| name|</span><br><span class="line">+---+-----+</span><br><span class="line">|  <span class="number">2</span>|Alice|</span><br><span class="line">|  <span class="number">3</span>|Alice|</span><br><span class="line">|  <span class="number">5</span>|  Bob|</span><br><span class="line">| <span class="number">10</span>|  Bob|</span><br><span class="line">+---+-----+</span><br><span class="line">observation.get</span><br><span class="line"><span class="comment"># &#123;&#x27;count&#x27;: 4, &#x27;max(age)&#x27;: 10&#125;</span></span><br></pre></td></tr></table></figure>
<h2 id="缺失值处理"><a class="markdownIt-Anchor" href="#缺失值处理"></a> 缺失值处理</h2>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql</th>
<th style="text-align:left">缺失值处理</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>DataFrame.na.fill(value, subset=None)</code></td>
<td style="text-align:left">缺失值填充</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.na.drop(how='any', thresh=None, subset=None)</code></td>
<td style="text-align:left">缺失值删除</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.na.replace(to_teplace, value, subset=None)</code></td>
<td style="text-align:left">替换</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.eqNullSafe(other)</code></td>
<td style="text-align:left">安全处理NULL</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.isNotNull()</code></td>
<td style="text-align:left"><code>IS NOT NULL</code>  in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>Column.isNull()</code></td>
<td style="text-align:left"><code>IS NULL</code>  in SQL</td>
</tr>
<tr>
<td style="text-align:left"><code>functions.isnan(col)</code></td>
<td style="text-align:left"><code>IS NaN</code></td>
</tr>
<tr>
<td style="text-align:left"><code>functions.isnull(col)</code></td>
<td style="text-align:left"><code>IS NULL</code></td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="literal">None</span>, <span class="string">&quot;Michael&quot;</span>),</span><br><span class="line">    (<span class="number">30</span>, <span class="string">&quot;Andy&quot;</span>),</span><br><span class="line">    (<span class="number">19</span>, <span class="string">&quot;Justin&quot;</span>)</span><br><span class="line">], schema=[<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line">df.na.fill(<span class="number">25</span>).show()</span><br><span class="line">df.na.fill(&#123;<span class="string">&#x27;age&#x27;</span>:<span class="number">25</span>, <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;unknow&#x27;</span>&#125;).show()</span><br><span class="line">+---+-------+</span><br><span class="line">|age|   name|</span><br><span class="line">+---+-------+</span><br><span class="line">| <span class="number">25</span>|Michael|</span><br><span class="line">| <span class="number">30</span>|   Andy|</span><br><span class="line">| <span class="number">19</span>| Justin|</span><br><span class="line">+---+-------+</span><br><span class="line"></span><br><span class="line">df.na.drop().show()</span><br><span class="line">+---+------+</span><br><span class="line">|age|  name|</span><br><span class="line">+---+------+</span><br><span class="line">| <span class="number">30</span>|  Andy|</span><br><span class="line">| <span class="number">19</span>|Justin|</span><br><span class="line">+---+------+df.<span class="built_in">filter</span>(df.age.isNotNull()).show()</span><br><span class="line">+---+-------+                                                                   </span><br><span class="line">|age|   name|</span><br><span class="line">+---+-------+</span><br><span class="line">| <span class="number">30</span>|   Andy|</span><br><span class="line">| <span class="number">19</span>| Justin|</span><br><span class="line">+---+-------+</span><br><span class="line"></span><br><span class="line">df.select(</span><br><span class="line">    df[<span class="string">&quot;age&quot;</span>].eqNullSafe(<span class="number">30</span>),</span><br><span class="line">    df[<span class="string">&quot;age&quot;</span>].eqNullSafe(<span class="literal">None</span>),</span><br><span class="line">).show()</span><br><span class="line">+------------+--------------+</span><br><span class="line">|(age &lt;=&gt; <span class="number">30</span>)|(age &lt;=&gt; NULL)|</span><br><span class="line">+------------+--------------+</span><br><span class="line">|       false|          true|</span><br><span class="line">|        true|         false|</span><br><span class="line">|       false|         false|</span><br><span class="line">+------------+--------------+</span><br><span class="line"></span><br><span class="line">df.<span class="built_in">filter</span>(df.age.isNotNull()).show()</span><br><span class="line">+---+-------+                                                                   </span><br><span class="line">|age|   name|</span><br><span class="line">+---+-------+</span><br><span class="line">| <span class="number">30</span>|   Andy|</span><br><span class="line">| <span class="number">19</span>| Justin|</span><br><span class="line">+---+-------+</span><br><span class="line"></span><br><span class="line">df.select(</span><br><span class="line">    df[<span class="string">&quot;age&quot;</span>].eqNullSafe(<span class="number">30</span>),</span><br><span class="line">    df[<span class="string">&quot;age&quot;</span>].eqNullSafe(<span class="literal">None</span>),</span><br><span class="line">).show()</span><br><span class="line">+------------+--------------+</span><br><span class="line">|(age &lt;=&gt; <span class="number">30</span>)|(age &lt;=&gt; NULL)|</span><br><span class="line">+------------+--------------+</span><br><span class="line">|       false|          true|</span><br><span class="line">|        true|         false|</span><br><span class="line">|       false|         false|</span><br><span class="line">+------------+--------------+</span><br></pre></td></tr></table></figure>
<h2 id="临时视图"><a class="markdownIt-Anchor" href="#临时视图"></a> 临时视图</h2>
<h3 id="临时视图-2"><a class="markdownIt-Anchor" href="#临时视图-2"></a> 临时视图</h3>
<p>DataFrame和Spark SQL共享相同的执行引擎，因此，您可以将DataFrame注册为临时视图，并轻松运行SQL查询：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">29</span>, <span class="string">&quot;Michael&quot;</span>),</span><br><span class="line">    (<span class="number">30</span>, <span class="string">&quot;Andy&quot;</span>),</span><br><span class="line">    (<span class="number">19</span>, <span class="string">&quot;Justin&quot;</span>)</span><br><span class="line">], schema=[<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Register the DataFrame as a SQL temporary view</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;tableA&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SQL can be run over DataFrames that have been registered as a table.</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT count(*) from tableA&quot;</span>).show()</span><br><span class="line">+--------+</span><br><span class="line">|count(<span class="number">1</span>)|</span><br><span class="line">+--------+</span><br><span class="line">|       <span class="number">3</span>|</span><br><span class="line">+--------+</span><br></pre></td></tr></table></figure>
<p>这些SQL表达式可以直接用作DataFrame列：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> expr</span><br><span class="line"></span><br><span class="line">df.select(expr(<span class="string">&#x27;count(*)&#x27;</span>) &gt; <span class="number">0</span>).show()</span><br><span class="line">+--------------+</span><br><span class="line">|(count(<span class="number">1</span>) &gt; <span class="number">0</span>)|</span><br><span class="line">+--------------+</span><br><span class="line">|          true|</span><br><span class="line">+--------------+</span><br></pre></td></tr></table></figure>
<h3 id="全局临时视图"><a class="markdownIt-Anchor" href="#全局临时视图"></a> 全局临时视图</h3>
<p>Spark SQL中的临时视图是会话范围的，如果创建它的会话终止，它将消失。如果您想拥有一个在所有会话之间共享的临时视图，并保持活动状态，直到Spark应用程序终止，您可以创建一个全局临时视图。全局临时视图与系统保留的数据库<code>global_temp</code>绑定，我们必须使用严格的字段名来引用它。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Register the DataFrame as a global temporary view</span></span><br><span class="line">df.createOrReplaceGlobalTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Global temporary view is tied to a system preserved database `global_temp`</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT * FROM global_temp.people&quot;</span>).show()</span><br><span class="line">+---+-------+                                                                   </span><br><span class="line">|age|   name|</span><br><span class="line">+---+-------+</span><br><span class="line">| <span class="number">29</span>|Michael|</span><br><span class="line">| <span class="number">30</span>|   Andy|</span><br><span class="line">| <span class="number">19</span>| Justin|</span><br><span class="line">+---+-------+</span><br><span class="line"></span><br><span class="line"><span class="comment"># Global temporary view is cross-session</span></span><br><span class="line">spark.newSession().sql(<span class="string">&quot;SELECT * FROM global_temp.people&quot;</span>).show()</span><br><span class="line">+---+-------+                                                                   </span><br><span class="line">|age|   name|</span><br><span class="line">+---+-------+</span><br><span class="line">| <span class="number">29</span>|Michael|</span><br><span class="line">| <span class="number">30</span>|   Andy|</span><br><span class="line">| <span class="number">19</span>| Justin|</span><br><span class="line">+---+-------+</span><br></pre></td></tr></table></figure>
<h2 id="缓存数据"><a class="markdownIt-Anchor" href="#缓存数据"></a> 缓存数据</h2>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.sql</th>
<th style="text-align:left">缓存</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>DataFrame.cache()</code></td>
<td style="text-align:left">使用默认等级MEMORY_AND_DISK_DESER缓存</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.persist(storageLevel)</code></td>
<td style="text-align:left">缓存</td>
</tr>
<tr>
<td style="text-align:left"><code>DataFrame.unpersist()</code></td>
<td style="text-align:left">删除缓存</td>
</tr>
</tbody>
</table>
<p>DataFrame 同样有变换和行动两种操作，变换操作也是惰性的，不立即计算其结果。</p>
<p>当DataFrame在未来的行动中重复利用时，我们可以使用<code>persist()</code>或<code>cache()</code>方法将标记为持久。缓存是迭代算法和快速交互使用的关键工具。</p>
<p>第一次计算后，它将保存在节点的内存中。Spark的缓存是容错的如果，任何分区丢失，它将使用最初创建它的转换自动重新计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Persists the data in the disk by specifying the storage level.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel</span><br><span class="line">df.persist(StorageLevel.DISK_ONLY)</span><br></pre></td></tr></table></figure>
<p>**注意：**在Python中，存储的对象将始终使用Pickle库进行序列化，因此您是否选择序列化级别并不重要。Python中的可用存储级别包括<code>MEMORY_ONLY</code>、<code>MEMORY_ONLY_2</code>、<code>MEMORY_AND_DISK</code>、<code>MEMORY_AND_DISK_2</code>、<code>DISK_ONLY</code>、<code>DISK_ONLY_2</code>和<code>DISK_ONLY_3</code>。</p>
<h1 id="pandas-api-on-spark"><a class="markdownIt-Anchor" href="#pandas-api-on-spark"></a> Pandas API on Spark</h1>
<p>pandas-on-Spark DataFrame和pandas DataFrame相似，提供了pandas Dataframe的几乎所有属性和方法。然而，前者是分布式计算，后者是单机计算。</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html">pandas_on_spark user guide</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Customarily, we import pandas API on Spark as follows:</span></span><br><span class="line"><span class="keyword">import</span> pyspark.pandas <span class="keyword">as</span> ps</span><br></pre></td></tr></table></figure>
<h2 id="创建-dataframe-2"><a class="markdownIt-Anchor" href="#创建-dataframe-2"></a> 创建 DataFrame</h2>
<p>类似于创建 Pandas DataFrame一样创建 pandas-on-Spark DataFrame</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = ps.Series([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="literal">None</span>, <span class="number">6</span>, <span class="number">8</span>])</span><br><span class="line"><span class="built_in">print</span>(s)</span><br><span class="line"><span class="comment"># 0    1.0</span></span><br><span class="line"><span class="comment"># 1    3.0</span></span><br><span class="line"><span class="comment"># 2    5.0</span></span><br><span class="line"><span class="comment"># 3    NaN</span></span><br><span class="line"><span class="comment"># 4    6.0</span></span><br><span class="line"><span class="comment"># 5    8.0</span></span><br><span class="line"></span><br><span class="line">psdf = ps.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;id&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>: [<span class="number">29</span>, <span class="number">30</span>, <span class="number">19</span>],</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>: [<span class="string">&#x27;Michael&#x27;</span>, <span class="string">&#x27;Andy&#x27;</span>, <span class="string">&#x27;Justin&#x27;</span>]</span><br><span class="line">    &#125;, index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(psdf)</span><br><span class="line"><span class="comment">#    id  age     name</span></span><br><span class="line"><span class="comment"># a   1   29  Michael</span></span><br><span class="line"><span class="comment"># b   2   30     Andy</span></span><br><span class="line"><span class="comment"># c   3   19   Justin</span></span><br></pre></td></tr></table></figure>
<p>从 Pandas DataFrame转化为 pandas-on-Spark DataFrame</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">pdf = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;id&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>: [<span class="number">29</span>, <span class="number">30</span>, <span class="number">19</span>],</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>: [<span class="string">&#x27;Michael&#x27;</span>, <span class="string">&#x27;Andy&#x27;</span>, <span class="string">&#x27;Justin&#x27;</span>]</span><br><span class="line">    &#125;, index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, this pandas DataFrame can be converted to a pandas-on-Spark DataFrame</span></span><br><span class="line">psdf = ps.from_pandas(pdf) </span><br></pre></td></tr></table></figure>
<p>从 Spark DataFrame转化为 pandas-on-Spark DataFrame</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sdf = spark.createDataFrame(pdf) <span class="comment"># Spark DataFrame</span></span><br><span class="line">psdf = sdf.pandas_api(index_col=<span class="string">&quot;id&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(psdf)</span><br><span class="line"><span class="comment">#     age     name</span></span><br><span class="line"><span class="comment"># id              </span></span><br><span class="line"><span class="comment"># 1    29  Michael</span></span><br><span class="line"><span class="comment"># 2    30     Andy</span></span><br><span class="line"><span class="comment"># 3    19   Justin</span></span><br></pre></td></tr></table></figure>
<p>请注意，当从Spark DataFrame创建pandas-on-Spark DataFrame时，会创建一个新的默认索引。为了避免这种开销，请尽可能指定要用作索引的列。</p>
<table>
<thead>
<tr>
<th>pyspark.pandas.DataFrame</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>DataFrame.to_pandas()</code></td>
<td>转化为Pandas DataFrame</td>
</tr>
<tr>
<td><code>DataFrame.to_spark()</code></td>
<td>转化为Spark DataFrame</td>
</tr>
<tr>
<td><code>DataFrame.spark.frame([index_col])</code></td>
<td>获取Spark DataFrame</td>
</tr>
</tbody>
</table>
<p>请注意，将pandas-on-Spark DataFrame转换为pandas DataFrame时，需要将所有数据收集到客户端机器中，数据太大时，容易引发内存错误。</p>
<h2 id="inputoutput-2"><a class="markdownIt-Anchor" href="#inputoutput-2"></a> Input/Output</h2>
<p>pandas-on-Spark 不仅支持 Pandas IO，还完全支持Spark IO：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># CSV</span></span><br><span class="line">psdf.to_csv(<span class="string">&#x27;csvFile.csv&#x27;</span>)</span><br><span class="line">ps.read_csv(<span class="string">&#x27;csvFile.csv&#x27;</span>)</span><br><span class="line"><span class="comment"># Parquet</span></span><br><span class="line">psdf.to_parquet(<span class="string">&#x27;parquetFile.parquet&#x27;</span>)</span><br><span class="line">ps.read_parquet(<span class="string">&#x27;parquetFile.parquet&#x27;</span>)</span><br><span class="line"><span class="comment"># Spark IO</span></span><br><span class="line">psdf.to_spark_io(<span class="string">&#x27;orcFile.orc&#x27;</span>, <span class="built_in">format</span>=<span class="string">&quot;orc&quot;</span>)</span><br><span class="line">ps.read_spark_io(<span class="string">&#x27;orcFile.orc&#x27;</span>, <span class="built_in">format</span>=<span class="string">&quot;orc&quot;</span>)</span><br></pre></td></tr></table></figure>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/spark/">spark</a></div><div class="post_share"><div class="social-share" data-image="/img/apache-spark-sql.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/morty3.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/morty3.jpg" alt="Give me money!"/></a><div class="post-qr-code-desc">Give me money!</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/32722c50/" title="大数据手册(Spark)--Spark 简介"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-overview.png" onerror="onerror=null;src='/img/404_moon.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">大数据手册(Spark)--Spark 简介</div></div></a></div><div class="next-post pull-right"><a href="/posts/264c088/" title="大数据手册(Spark)--Spark Core and RDDs"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-core.png" onerror="onerror=null;src='/img/404_moon.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">大数据手册(Spark)--Spark Core and RDDs</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/264c088/" title="大数据手册(Spark)--Spark Core and RDDs"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-core.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-03</div><div class="title">大数据手册(Spark)--Spark Core and RDDs</div></div></a></div><div><a href="/posts/75974533/" title="大数据手册(Spark)--PySpark MLlib"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-mllib.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-01</div><div class="title">大数据手册(Spark)--PySpark MLlib</div></div></a></div><div><a href="/posts/34eba6aa/" title="大数据手册(Spark)--PySpark Streaming"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-streaming.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-13</div><div class="title">大数据手册(Spark)--PySpark Streaming</div></div></a></div><div><a href="/posts/d02a6da3/" title="大数据手册(Spark)--Spark安装配置"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/spark-install.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-15</div><div class="title">大数据手册(Spark)--Spark安装配置</div></div></a></div><div><a href="/posts/32722c50/" title="大数据手册(Spark)--Spark 简介"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-overview.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-03</div><div class="title">大数据手册(Spark)--Spark 简介</div></div></a></div><div><a href="/posts/a1358f89/" title="PySpark 特征工程(II)--特征构造"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/spark-install.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-03</div><div class="title">PySpark 特征工程(II)--特征构造</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Gitalk</span><span class="switch-btn"></span><span class="second-comment">Waline</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div><div><div id="waline-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Tiny Lei</div><div class="author-info__description">每天进步一点点...</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">175</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">108</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">43</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://gitee.com/wilenwu" rel="external nofollow noreferrer" target="_blank" title="Gitee"><i class="iconfont icon-gitee"></i></a><a class="social-icon" href="https://github.com/wilenwu" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="iconfont icon-github"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_41518277" rel="external nofollow noreferrer" target="_blank" title="CSDN"><i class="iconfont icon-csdn"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="iconfont icon-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">感谢访问本站，若喜欢请收藏^_^</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#spark-sql"><span class="toc-number">1.</span> <span class="toc-text"> Spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#sparksession"><span class="toc-number">1.1.</span> <span class="toc-text"> SparkSession</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#catalog"><span class="toc-number">1.2.</span> <span class="toc-text"> Catalog</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#dataframe"><span class="toc-number">2.</span> <span class="toc-text"> DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-dataframe"><span class="toc-number">2.1.</span> <span class="toc-text"> 创建 DataFrame</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dataframe%E4%BF%A1%E6%81%AF"><span class="toc-number">2.2.</span> <span class="toc-text"> DataFrame信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#inputoutput"><span class="toc-number">2.3.</span> <span class="toc-text"> Input&#x2F;Output</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="toc-number">2.3.1.</span> <span class="toc-text"> 外部数据源</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E7%94%A8%E9%80%89%E9%A1%B9%E9%85%8D%E7%BD%AE"><span class="toc-number">2.3.2.</span> <span class="toc-text"> 通用选项&#x2F;配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%BC%8F"><span class="toc-number">2.3.3.</span> <span class="toc-text"> 保存模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA"><span class="toc-number">2.3.4.</span> <span class="toc-text"> 分区</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hive-%E8%A1%A8%E4%BA%A4%E4%BA%92"><span class="toc-number">2.4.</span> <span class="toc-text"> Hive 表交互</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#hive%E9%85%8D%E7%BD%AE"><span class="toc-number">2.4.1.</span> <span class="toc-text"> Hive配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sql-%E6%9F%A5%E8%AF%A2"><span class="toc-number">2.4.2.</span> <span class="toc-text"> SQL 查询</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98hive%E8%A1%A8"><span class="toc-number">2.4.3.</span> <span class="toc-text"> 保存HIve表</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">2.5.</span> <span class="toc-text"> 常用方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#select"><span class="toc-number">2.5.1.</span> <span class="toc-text"> Select</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#column"><span class="toc-number">2.5.2.</span> <span class="toc-text"> Column</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#distinct"><span class="toc-number">2.5.3.</span> <span class="toc-text"> Distinct</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#filter"><span class="toc-number">2.5.4.</span> <span class="toc-text"> Filter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#group"><span class="toc-number">2.5.5.</span> <span class="toc-text"> Group</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sort"><span class="toc-number">2.5.6.</span> <span class="toc-text"> Sort</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#joins"><span class="toc-number">2.5.7.</span> <span class="toc-text"> Joins</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#union"><span class="toc-number">2.5.8.</span> <span class="toc-text"> Union</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pivot"><span class="toc-number">2.5.9.</span> <span class="toc-text"> Pivot</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#window"><span class="toc-number">2.5.10.</span> <span class="toc-text"> Window</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%BD%E6%95%B0"><span class="toc-number">2.6.</span> <span class="toc-text"> 函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#built-in-functions"><span class="toc-number">2.6.1.</span> <span class="toc-text"> Built-In Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#python-udfs"><span class="toc-number">2.6.2.</span> <span class="toc-text"> Python UDFs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#arrow-python-udfs"><span class="toc-number">2.6.3.</span> <span class="toc-text"> Arrow Python UDFs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pandas-udfs"><span class="toc-number">2.6.4.</span> <span class="toc-text"> Pandas UDFs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pandas-function-api"><span class="toc-number">2.6.5.</span> <span class="toc-text"> Pandas Function API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#udtfs"><span class="toc-number">2.6.6.</span> <span class="toc-text"> UDTFs</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E4%BF%A1%E6%81%AF"><span class="toc-number">2.7.</span> <span class="toc-text"> 统计信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86"><span class="toc-number">2.8.</span> <span class="toc-text"> 缺失值处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%B4%E6%97%B6%E8%A7%86%E5%9B%BE"><span class="toc-number">2.9.</span> <span class="toc-text"> 临时视图</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%B4%E6%97%B6%E8%A7%86%E5%9B%BE-2"><span class="toc-number">2.9.1.</span> <span class="toc-text"> 临时视图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E5%B1%80%E4%B8%B4%E6%97%B6%E8%A7%86%E5%9B%BE"><span class="toc-number">2.9.2.</span> <span class="toc-text"> 全局临时视图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%93%E5%AD%98%E6%95%B0%E6%8D%AE"><span class="toc-number">2.10.</span> <span class="toc-text"> 缓存数据</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pandas-api-on-spark"><span class="toc-number">3.</span> <span class="toc-text"> Pandas API on Spark</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-dataframe-2"><span class="toc-number">3.1.</span> <span class="toc-text"> 创建 DataFrame</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#inputoutput-2"><span class="toc-number">3.2.</span> <span class="toc-text"> Input&#x2F;Output</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/8f3475e8/" title="30分钟速成Java"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/openbook.jpg" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="30分钟速成Java"/></a><div class="content"><a class="title" href="/posts/8f3475e8/" title="30分钟速成Java">30分钟速成Java</a><time datetime="2024-08-29T11:37:15.174Z" title="发表于 2024-08-29 19:37:15">2024-08-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/a14a3278/" title="机器学习(VII)--强化学习(七)Actor-Critic"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/reinforcement_learning_cover.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="机器学习(VII)--强化学习(七)Actor-Critic"/></a><div class="content"><a class="title" href="/posts/a14a3278/" title="机器学习(VII)--强化学习(七)Actor-Critic">机器学习(VII)--强化学习(七)Actor-Critic</a><time datetime="2024-08-29T09:24:00.000Z" title="发表于 2024-08-29 17:24:00">2024-08-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3644c10c/" title="机器学习(VII)--强化学习(六)策略梯度方法"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/reinforcement_learning_cover.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="机器学习(VII)--强化学习(六)策略梯度方法"/></a><div class="content"><a class="title" href="/posts/3644c10c/" title="机器学习(VII)--强化学习(六)策略梯度方法">机器学习(VII)--强化学习(六)策略梯度方法</a><time datetime="2024-08-29T09:23:00.000Z" title="发表于 2024-08-29 17:23:00">2024-08-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/2bd55622/" title="机器学习(VII)--强化学习(五)值函数近似"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/reinforcement_learning_cover.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="机器学习(VII)--强化学习(五)值函数近似"/></a><div class="content"><a class="title" href="/posts/2bd55622/" title="机器学习(VII)--强化学习(五)值函数近似">机器学习(VII)--强化学习(五)值函数近似</a><time datetime="2024-08-29T09:22:00.000Z" title="发表于 2024-08-29 17:22:00">2024-08-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/363ac257/" title="机器学习(VII)--强化学习(四)时序差分方法"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/reinforcement_learning_cover.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="机器学习(VII)--强化学习(四)时序差分方法"/></a><div class="content"><a class="title" href="/posts/363ac257/" title="机器学习(VII)--强化学习(四)时序差分方法">机器学习(VII)--强化学习(四)时序差分方法</a><time datetime="2024-08-29T09:21:00.000Z" title="发表于 2024-08-29 17:21:00">2024-08-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Tiny Lei</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'forest'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addModeChange('mermaid', runMermaid)

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '7c65134b48b13f306114',
      clientSecret: 'f049f68368a11925fdb69e57c64839eac94e13c1'',
      repo: 'gitalk-comments',
      owner: 'WilenWu',
      admin: ['WilenWu'],
      id: 'c1a77fcaffe50327aa26e02fd6f71130',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.textContent= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script><script>function loadWaline () {
  function initWaline () {
    const waline = Waline.init(Object.assign({
      el: '#waline-wrap',
      serverURL: 'https://waline-comments-9etq63pcv-wilenwu.vercel.app',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      path: window.location.pathname,
      comment: false,
    }, {"requiredMeta":["monsterid"]}))
  }

  if (typeof Waline === 'object') initWaline()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css').then(() => {
      getScript('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.js').then(initWaline)
    })
  }
}

if ('Gitalk' === 'Waline' || !true) {
  if (true) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
  else setTimeout(loadWaline, 0)
} else {
  function loadOtherComment () {
    loadWaline()
  }
}</script></div><script src="/js/custom.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>