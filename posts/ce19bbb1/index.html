<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>特征工程(II)--数据预处理 | 雷小小</title><meta name="author" content="Tiny Lei"><meta name="copyright" content="Tiny Lei"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据预处理 数据预处理是特征工程的最重要的起始步骤，需要把特征预处理成机器学习模型所能接受的形式，我们可以使用sklearn.preproccessing模块来解决大部分数据预处理问题。 本章使用两条线并行处理数据：  基于pandas的函数封装实现 基于sklearn的pipeline实现  先定义一个计时器，方便后续评估性能。 def timer(func):	import time	imp">
<meta property="og:type" content="article">
<meta property="og:title" content="特征工程(II)--数据预处理">
<meta property="og:url" content="https://www.tinylei.tech/posts/ce19bbb1/index.html">
<meta property="og:site_name" content="雷小小">
<meta property="og:description" content="数据预处理 数据预处理是特征工程的最重要的起始步骤，需要把特征预处理成机器学习模型所能接受的形式，我们可以使用sklearn.preproccessing模块来解决大部分数据预处理问题。 本章使用两条线并行处理数据：  基于pandas的函数封装实现 基于sklearn的pipeline实现  先定义一个计时器，方便后续评估性能。 def timer(func):	import time	imp">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.tinylei.tech/img/FeatureEngine.png">
<meta property="article:published_time" content="2024-03-16T15:40:52.000Z">
<meta property="article:modified_time" content="2024-03-16T15:43:48.596Z">
<meta property="article:author" content="Tiny Lei">
<meta property="article:tag" content="python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.tinylei.tech/img/FeatureEngine.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.tinylei.tech/posts/ce19bbb1/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="baidu-site-verification" content="code-7rymn5Bitx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?654e7415ab55bed7c9c2bc6d665f03c5";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '特征工程(II)--数据预处理',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-16 23:43:48'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_2849223_xh1ftc8qym.css"><link rel="stylesheet" href="/css/link-card.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="雷小小" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">161</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">105</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">43</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/user-guide/"><i class="fa-fw fa fa-compass"></i><span> 用户指南</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa fa-book"></i><span> 文档</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-images"></i><span> 图库</span></a></div><div class="menus_item"><a class="site-page" href="/ebook/"><i class="fa-fw fa fa-book-reader"></i><span> 电子书</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-star"></i><span> 收藏夹</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa-solid fa-circle-chevron-down"></i><span> 更多</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fa fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/analytics/"><i class="fa-fw fa fa-line-chart"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/log/"><i class="fa-fw fa fa-history"></i><span> 更新日志</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/sklearn-top-img.svg')"><nav id="nav"><span id="blog-info"><a href="/" title="雷小小"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/favicon.png"/><span class="site-name">雷小小</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/user-guide/"><i class="fa-fw fa fa-compass"></i><span> 用户指南</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa fa-book"></i><span> 文档</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-images"></i><span> 图库</span></a></div><div class="menus_item"><a class="site-page" href="/ebook/"><i class="fa-fw fa fa-book-reader"></i><span> 电子书</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-star"></i><span> 收藏夹</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa-solid fa-circle-chevron-down"></i><span> 更多</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fa fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/analytics/"><i class="fa-fw fa fa-line-chart"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/log/"><i class="fa-fw fa fa-history"></i><span> 更新日志</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">特征工程(II)--数据预处理<a class="post-edit-link" href="https://gitee.com/WilenWu/myblog/edit/master/source/_posts/python/Feature-Engineering-with-Python(II)--Data-PreProcessing.md" rel="external nofollow noreferrer" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-03-16T15:40:52.000Z" title="发表于 2024-03-16 23:40:52">2024-03-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-16T15:43:48.596Z" title="更新于 2024-03-16 23:43:48">2024-03-16</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/python/">Python</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/python/machine-learning/">Machine Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>39分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="特征工程(II)--数据预处理"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="数据预处理"><a class="markdownIt-Anchor" href="#数据预处理"></a> 数据预处理</h1>
<p>数据预处理是特征工程的最重要的起始步骤，需要把特征预处理成机器学习模型所能接受的形式，我们可以使用sklearn.preproccessing模块来解决大部分数据预处理问题。</p>
<p>本章使用两条线并行处理数据：</p>
<ul>
<li>基于pandas的函数封装实现</li>
<li>基于sklearn的pipeline实现</li>
</ul>
<p>先定义一个计时器，方便后续评估性能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">timer</span>(<span class="params">func</span>):</span></span><br><span class="line">	<span class="keyword">import</span> time</span><br><span class="line">	<span class="keyword">import</span> functools</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">strfdelta</span>(<span class="params">tdelta, fmt</span>):</span></span><br><span class="line">	    hours, remainder = <span class="built_in">divmod</span>(tdelta, <span class="number">3600</span>)</span><br><span class="line">	    minutes, seconds = <span class="built_in">divmod</span>(remainder, <span class="number">60</span>)</span><br><span class="line">	    <span class="keyword">return</span> fmt.<span class="built_in">format</span>(hours, minutes, seconds)</span><br><span class="line"><span class="meta">	@functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">	    click = time.time()</span><br><span class="line">	    result = func(*args, **kwargs)</span><br><span class="line">	    delta = strfdelta(time.time() - click, <span class="string">&quot;&#123;:.0f&#125; hours &#123;:.0f&#125; minutes &#123;:.0f&#125; seconds&quot;</span>)</span><br><span class="line">	    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> cost time <span class="subst">&#123;delta&#125;</span>&quot;</span>)</span><br><span class="line">	    <span class="keyword">return</span> result</span><br><span class="line">	<span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure>
<h2 id="数据清洗"><a class="markdownIt-Anchor" href="#数据清洗"></a> 数据清洗</h2>
<p>数据清洗(Data cleaning)：对数据进行重新审查和校验的过程，目的在于删除重复信息、纠正存在的错误，并提供数据一致性。</p>
<p>首先，根据某个/多个特征值构成的样本ID去重</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.drop_duplicates(subset=[ID_col], keep=<span class="string">&#x27;last&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>字符型数字自动转成数字</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.to_numeric(df, errors=<span class="string">&#x27;ignore&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>有时，有些数值型特征标识的只是不同类别，其数值的大小并没有实际意义，因此我们将其转化为类别特征。本项目并无此类特征，以 hours_appr_process_start 为示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;hours_appr_process_start &#x27;</span>] = df[<span class="string">&#x27;hours_appr_process_start &#x27;</span>].astype(<span class="string">&quot;category&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>接下来，我们根据业务常识，或者使用但不限于箱型图（Box-plot）发现数据中不合理的特征值进行清洗。</p>
<p>注意到，DAYS_BIRTH列（年龄）中的数字是负数，由于它们是相对于当前贷款申请计算的，所以我们将其转化成正数后查看分布</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(df[<span class="string">&#x27;DAYS_BIRTH&#x27;</span>] / -<span class="number">365</span>).describe()</span><br></pre></td></tr></table></figure>
<p>那些年龄看起来合理，没有异常值。</p>
<p>接下来，我们对其他的 DAYS 特征作同样的分析</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">days_registration</span><br><span class="line">days_id_publish</span><br><span class="line">own_car_age</span><br><span class="line">app_train[<span class="string">&#x27;DAYS_EMPLOYED&#x27;</span>].describe() </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.cut(df[<span class="string">&#x27;DAYS_EMPLOYED&#x27;</span>] / -<span class="number">365</span>, bins=<span class="number">10</span>).value_counts()</span><br></pre></td></tr></table></figure>
<p>有超过50000个用户的DAYS_EMPLOYED在1000年上，可以猜测这只是缺失值标记。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Replace the anomalous values with nan</span></span><br><span class="line">df[<span class="string">&#x27;DAYS_EMPLOYED&#x27;</span>].where(df[<span class="string">&#x27;DAYS_EMPLOYED&#x27;</span>]&lt;<span class="number">365243</span>, np.nan, inplace = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">app_train[<span class="string">&#x27;DAYS_EMPLOYED&#x27;</span>].plot.hist(title = <span class="string">&#x27;Days Employment Histogram&#x27;</span>);</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Days Employment&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>可以看到，数据分布基本正常了。</p>
<p>同样，将其他特征的缺失值标记转换成缺失，方便后续统一处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.replace(<span class="string">&#x27;XNA&#x27;</span>, np.nan)</span><br></pre></td></tr></table></figure>
<p>最后，使用函数封装以上步骤</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = df.drop_duplicates(subset=[ID_col], keep=<span class="string">&#x27;last&#x27;</span>)</span><br><span class="line">X = df.drop(ID_col).copy()</span><br><span class="line">y = X.pop(target)</span><br><span class="line"></span><br><span class="line"><span class="meta">@timer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean</span>(<span class="params">X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">    X = pd.to_numeric(X), errors=<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">    X[<span class="string">&#x27;hours_appr_process_start &#x27;</span>].astype(<span class="string">&quot;category&quot;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    X[<span class="string">&#x27;DAYS_EMPLOYED&#x27;</span>].where(X[<span class="string">&#x27;DAYS_EMPLOYED&#x27;</span>]&lt;<span class="number">365243</span>, np.nan, inplace=<span class="literal">True</span>)</span><br><span class="line">    X.replace(<span class="string">&#x27;XNA&#x27;</span>, np.nan, inplace=<span class="literal">True</span>)</span><br><span class="line">    X.select_dtypes(<span class="string">&quot;object&quot;</span>).astype(<span class="string">&quot;category&quot;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<h2 id="离散特征编码"><a class="markdownIt-Anchor" href="#离散特征编码"></a> 离散特征编码</h2>
<p>有很多机器学习算法只能接受数值型特征的输入，不能处理离散值特征，比如线性回归，逻辑回归等线性模型，那么我们需要将离散特征重编码成数值变量。</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>函数</th>
<th>python包</th>
</tr>
</thead>
<tbody>
<tr>
<td>顺序编码</td>
<td>OrdinalEncoder</td>
<td>sklearn.preprocessing</td>
</tr>
<tr>
<td>顺序编码</td>
<td>CategoricalDtype</td>
<td>pandas.api.types</td>
</tr>
<tr>
<td>One-hot 编码</td>
<td>OneHotEncoder</td>
<td>sklearn.preprocessing</td>
</tr>
<tr>
<td>哑变量变换</td>
<td>pd.get_dummies</td>
<td>pandas</td>
</tr>
<tr>
<td>平均数编码</td>
<td>self-define</td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p>实际上，大多数情况下不区分one-hot和dummy。它们的区别是：如果我们的特征有N个取值，one-hot会用N个新的0/1特征代替，dummy只需要N-1个新特征来代替。</p>
</blockquote>
<p>不同类型的离散特征有不同的编码方式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">features = df.columns.drop([ID_col, target]).to_list() </span><br><span class="line">categorical_cols = df.select_dtypes([<span class="string">&quot;object&quot;</span>, <span class="string">&quot;category&quot;</span>]).columns.tolist()</span><br><span class="line">numeric_cols = df.select_dtypes(<span class="string">&quot;number&quot;</span>).columns.tolist()</span><br></pre></td></tr></table></figure>
<p><strong>有序分类特征</strong>实际上表征着潜在的排序关系，我们将这些特征的类别映射成有大小的数字，因此可以用顺序编码。</p>
<p>多数情况下，整型变量都存储了有限的离散值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.select_dtypes(<span class="string">&quot;int32&quot;</span>).nunique()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The ordinal (ordered) categorical features</span></span><br><span class="line"><span class="comment"># Pandas calls the categories &quot;levels&quot;</span></span><br><span class="line"></span><br><span class="line">ordered_levels = &#123;</span><br><span class="line">    <span class="string">&quot;NAME_YIELD_GROUP&quot;</span>: [<span class="string">&quot;low_action&quot;</span>, <span class="string">&quot;low_normal&quot;</span>, <span class="string">&quot;middle&quot;</span>, <span class="string">&quot;high&quot;</span>],</span><br><span class="line">  	<span class="string">&quot;NAME_EDUCATION_TYPE&quot;</span>: [<span class="string">&quot;Lower secondary&quot;</span>, <span class="string">&quot;Secondary / secondary special&quot;</span>, <span class="string">&quot;Incomplete higher&quot;</span>, <span class="string">&quot;Higher education&quot;</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>无序分类特征</strong>对于树集成模型（tree-ensemble like XGBoost）是可用的，但对于线性模型（like Lasso or Ridge）则必须使用one-hot重编码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The nominative (unordered) categorical features</span></span><br><span class="line">nominal_categories= [col <span class="keyword">for</span> col <span class="keyword">in</span> categorical_cols <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> ordered_levels.keys()]</span><br></pre></td></tr></table></figure>
<p>现在我们来看看每个离散变量的类别数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[nominal_categories].nunique()</span><br></pre></td></tr></table></figure>
<p><strong>使用pandas实现编码</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Using pandas to encode categorical features</span></span><br><span class="line"><span class="keyword">from</span> pandas.api.types <span class="keyword">import</span> CategoricalDtype</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">onehot_encode</span>(<span class="params">X, variables=<span class="literal">None</span>, dummy_na=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Replace the categorical variables by the binary variables.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    X: pd.DataFrame of shape = [n_samples, n_features]</span></span><br><span class="line"><span class="string">        The data to encode.</span></span><br><span class="line"><span class="string">        Can be the entire dataframe, not just seleted variables.</span></span><br><span class="line"><span class="string">    variables: list, default=None</span></span><br><span class="line"><span class="string">        The list of categorical variables that will be encoded. </span></span><br><span class="line"><span class="string">        If None, the encoder will find and encode all variables of type object or categorical by default.</span></span><br><span class="line"><span class="string">    dummy_na: boolean, default=True</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    X_new: pd.DataFrame.</span></span><br><span class="line"><span class="string">        The encoded dataframe. The shape of the dataframe will be different from</span></span><br><span class="line"><span class="string">        the original as it includes the dummy variables in place of the of the</span></span><br><span class="line"><span class="string">        original categorical ones.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">	<span class="comment"># pd.get_dummies automatically convert the categorical column into dummy variables</span></span><br><span class="line">    <span class="keyword">if</span> variables <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        X = pd.get_dummies(X, dummy_na=<span class="literal">True</span>)</span><br><span class="line">        variables = X.select_dtypes([<span class="string">&#x27;category&#x27;</span>, <span class="string">&#x27;object&#x27;</span>]).columns.to_list()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        X_dummy = pd.get_dummies(X[variables].astype(<span class="string">&#x27;category&#x27;</span>), dummy_na=<span class="literal">True</span>)</span><br><span class="line">        X = pd.concat([X, X_dummy], axis=<span class="number">1</span>, errors=<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">        <span class="comment"># drop the original non-encoded variables.</span></span><br><span class="line">        X = X.drop(variables)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">len</span>(variables):d&#125;</span> columns were one-hot encoded&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Training Features shape: <span class="subst">&#123;X.shape&#125;</span>&#x27;</span>)</span><br><span class="line">	<span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ordinal_encode</span>(<span class="params">X, levels: <span class="built_in">dict</span> = <span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> levels <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        variables = X.select_dtypes([<span class="string">&#x27;category&#x27;</span>, <span class="string">&#x27;object&#x27;</span>]).columns.to_list()</span><br><span class="line">        X[variables].astype(<span class="string">&quot;category&quot;</span>, copy=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dtypes = &#123;name: CategoricalDtype(levels[name], ordered=<span class="literal">True</span>) <span class="keyword">for</span> name <span class="keyword">in</span> levels&#125;</span><br><span class="line">        X.astype(dtypes, copy=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Add a None category for missing values</span></span><br><span class="line">    <span class="comment"># def add_na(x):</span></span><br><span class="line">    <span class="comment"># if &quot;NA&quot; not in x.cat.categories:</span></span><br><span class="line">    <span class="comment">#    return x.cat.add_categories(&quot;NA&quot;)</span></span><br><span class="line">    <span class="comment">#  else:</span></span><br><span class="line">    <span class="comment">#    return x</span></span><br><span class="line">    <span class="comment"># X = X.apply(add_na)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The `cat.codes` attribute holds the category levels.</span></span><br><span class="line">    <span class="comment"># For missing values, -1 is the default code.</span></span><br><span class="line">    X = X.apply(<span class="keyword">lambda</span> x: x.cat.codes)</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">len</span>(variables):d&#125;</span> columns were ordinal encoded&#x27;</span>)</span><br><span class="line">  <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X_train.pipe(onehot_encode, variables=nominal_categories)</span><br><span class="line">             .pipe(ordinal_encode, levels=ordered_levels)</span><br><span class="line">             .head(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p><strong>使用sklearn实现编码</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Using sklearn</span></span><br><span class="line">ordinal_encoder = OrdinalEncoder(</span><br><span class="line">	categories=[np.array(levels) <span class="keyword">for</span> levels <span class="keyword">in</span> ordered_levels.values()]</span><br><span class="line">	handle_unknown=<span class="string">&#x27;use_encoded_value&#x27;</span>, </span><br><span class="line">	unknown_value=-<span class="number">1</span>,</span><br><span class="line">	encoded_missing_value=-<span class="number">1</span>,</span><br><span class="line">	max_categories=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">onehot_encoder = OneHotEncoder(</span><br><span class="line">	drop=<span class="string">&#x27;if_binary&#x27;</span>, </span><br><span class="line">	min_frequency=<span class="number">0.02</span>, </span><br><span class="line">	max_categories=<span class="number">20</span>, </span><br><span class="line">	handle_unknown=<span class="string">&#x27;ignore&#x27;</span>, </span><br><span class="line">	feature_name_combiner=<span class="string">&#x27;concat&#x27;</span>,</span><br><span class="line">	sparse_output=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">categorical_encoder = ColumnTransformer(</span><br><span class="line">	[(<span class="string">&#x27;onehot_encoder&#x27;</span>, onehot_encoder, nominal_categories),</span><br><span class="line">	(<span class="string">&#x27;ordinal_encoder&#x27;</span>, ordinal_encoder, ordered_levels.keys().to_list())],</span><br><span class="line">	remainder=<span class="string">&#x27;passthrough&#x27;</span>, verbose_feature_names_out=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">categorical_encoder.fit_transform(X_train)</span><br><span class="line">categorical_encoder.transform(X_valid)</span><br><span class="line">categorical_encoder.get_feature_names_out()</span><br></pre></td></tr></table></figure>
<p><strong>平均数编码</strong>：一般情况下，针对分类特征，我们只需要使用sklearn的OneHotEncoder或OrdinalEncoder进行编码，这类简单的预处理能够满足大多数数据挖掘算法的需求。如果某一个分类特征的可能值非常多（高基数 high cardinality），那么再使用one-hot编码往往会出现维度爆炸。平均数编码（mean encoding）是一种高效的编码方式，在实际应用中，能极大提升模型的性能。</p>
<p>我们可以使用 feature-engine开源包实现平均数编码。<a target="_blank" rel="noopener external nofollow noreferrer" href="https://feature-engine.trainindata.com/en/latest/">feature-engine</a>将特征工程中常用的方法进行了封装。</p>
<p>其中变量 OCCUPATION_TYPE （职业类型）和 ORGANIZATION_TYPE类别数较多，准备使用平均数编码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> feature_engine.encoding <span class="keyword">import</span> MeanEncoder</span><br><span class="line">mean_encoder = MeanEncoder([<span class="string">&#x27;OCCUPATION_TYPE&#x27;</span>, <span class="string">&#x27;ORGANIZATION_TYPE&#x27;</span>], missing_values=<span class="string">&#x27;ignore&#x27;</span>, ignore_format=<span class="literal">True</span>, unseen=<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">mean_encoder.fit_transform(X_train, y_train)</span><br><span class="line">mean_encoder.transform(X_valid)</span><br><span class="line">mean_encoder.get_feature_names_out()</span><br></pre></td></tr></table></figure>
<h2 id="连续特征分箱"><a class="markdownIt-Anchor" href="#连续特征分箱"></a> 连续特征分箱</h2>
<p>Binning Continuous Features</p>
<p>在实际的模型训练过程中，我们也经常对连续特征进行离散化处理，这样能消除特征量纲的影响，同时还能极大减少异常值的影响，增加特征的稳定性。</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>函数</th>
<th>python包</th>
</tr>
</thead>
<tbody>
<tr>
<td>二值化</td>
<td>Binarizer</td>
<td>sklearn.preprocessing</td>
</tr>
<tr>
<td>分箱</td>
<td>KBinsDiscretizer</td>
<td>sklearn.preprocessing</td>
</tr>
<tr>
<td>等频分箱</td>
<td>pd.qcut</td>
<td>pandas</td>
</tr>
<tr>
<td>等宽分箱</td>
<td>pd.cut</td>
<td>pandas</td>
</tr>
</tbody>
</table>
<p>分箱主要分为等频分箱、等宽分箱和聚类分箱三种。等频分箱会一定程度受到异常值的影响，而等宽分箱又容易完全忽略异常值信息，从而一定程度上导致信息损失，若要更好的兼顾变量的原始分布，则可以考虑聚类分箱。所谓聚类分箱，指的是先对某连续变量进行聚类（往往是 k-Means 聚类），然后使用样本所属类别。</p>
<p>以年龄对还款的影响为例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Find the correlation of the positive days since birth and target</span></span><br><span class="line">app_train[<span class="string">&#x27;DAYS_BIRTH&#x27;</span>] = <span class="built_in">abs</span>(app_train[<span class="string">&#x27;DAYS_BIRTH&#x27;</span>])</span><br><span class="line">app_train[<span class="string">&#x27;DAYS_BIRTH&#x27;</span>].corr(app_train[<span class="string">&#x27;TARGET&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>客户年龄与目标意义呈负相关关系，即随着客户年龄的增长，他们往往会更经常地按时偿还贷款。我们接下来将制作一个核心密度估计图（KDE），直观地观察年龄对目标的影响。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize = (<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">sns.kdeplot(app_train.loc[app_train[<span class="string">&#x27;DAYS_BIRTH&#x27;</span>] / <span class="number">365</span>, color=app_train[<span class="string">&#x27;TARGET&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Age (years)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Density&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Distribution of Ages&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p>如果我们把年龄分箱：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Age information into a separate dataframe</span></span><br><span class="line">age_data = app_train[[<span class="string">&#x27;TARGET&#x27;</span>, <span class="string">&#x27;DAYS_BIRTH&#x27;</span>]]</span><br><span class="line">age_data[<span class="string">&#x27;YEARS_BIRTH&#x27;</span>] = age_data[<span class="string">&#x27;DAYS_BIRTH&#x27;</span>] / <span class="number">365</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bin the age data</span></span><br><span class="line">age_data[<span class="string">&#x27;YEARS_BINNED&#x27;</span>] = pd.cut(age_data[<span class="string">&#x27;YEARS_BIRTH&#x27;</span>], bins = np.linspace(<span class="number">20</span>, <span class="number">70</span>, num = <span class="number">11</span>))</span><br><span class="line">age_groups  = age_data.groupby(<span class="string">&#x27;YEARS_BINNED&#x27;</span>).mean()</span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"><span class="comment"># Graph the age bins and the average of the target as a bar plot</span></span><br><span class="line">plt.bar(age_groups.index.astype(<span class="built_in">str</span>), <span class="number">100</span> * age_groups[<span class="string">&#x27;TARGET&#x27;</span>])</span><br><span class="line"><span class="comment"># Plot labeling</span></span><br><span class="line">plt.xticks(rotation = <span class="number">75</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Age Group (years)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Failure to Repay (%)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Failure to Repay by Age Group&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p>有一个明显的趋势：年轻的申请人更有可能不偿还贷款！ 年龄最小的三个年龄组的失败率在10％以上，最老的年龄组为5％。</p>
<p>使用pandas实现分箱</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discretize</span>(<span class="params">X, variables=<span class="literal">None</span>, bins=<span class="number">10</span>, strategy=<span class="string">&quot;uniform&quot;</span>, bucket_labels=<span class="literal">None</span>, encoding=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    bucket_labels: dict, default=None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> strategy <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;uniform&quot;</span>, <span class="string">&quot;quantile&quot;</span>]:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;strategy takes only values &#x27;uniform&#x27; or &#x27;quantile&#x27;&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> encoding <span class="keyword">not</span> <span class="keyword">in</span> [<span class="literal">None</span>, <span class="string">&quot;onehot&quot;</span>, <span class="string">&quot;ordinal&quot;</span>]:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;encoding takes only values None, &#x27;onehot&#x27; or &#x27;ordinal&#x27;&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> strategy == <span class="string">&quot;uniform&quot;</span>:</span><br><span class="line">        cut = pd.cut</span><br><span class="line">    <span class="keyword">elif</span> strategy == <span class="string">&quot;quantile&quot;</span>:</span><br><span class="line">        cut = pd.qcut</span><br><span class="line">    </span><br><span class="line">    bucket_labels = &#123;&#125; <span class="keyword">if</span> bucket_labels <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> bucket_labels</span><br><span class="line">    <span class="keyword">if</span> variables <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        variables = X.select_dtypes(<span class="string">&quot;number&quot;</span>).columns.to_list()</span><br><span class="line">    X_binned = X[variables].apply(<span class="keyword">lambda</span> x: cut(labels=bucket_labels.get(x.name)), </span><br><span class="line">               bins=bins, labels=labels, duplicates=<span class="string">&#x27;drop&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> encoding == <span class="string">&quot;onehot&quot;</span>:</span><br><span class="line">        X_binned = pd.get_dummies(X_binned, dummy_na=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">elif</span> encoding == <span class="string">&quot;ordinal&quot;</span>:</span><br><span class="line">        X_binned = X_binned.apply(<span class="keyword">lambda</span> x: x.cat.codes)</span><br><span class="line"></span><br><span class="line">    X = pd.concat(X.drop(variables), X_binned, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">discretize(X).head()</span><br></pre></td></tr></table></figure>
<p>sklearn.preprocessing 模块中的 KBinsDiscretizer 可以实现等频分箱、等宽分箱或聚类分箱，同时还可以对分箱后的离散特征进一步进行one-hot编码或顺序编码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> KBinsDiscretizer</span><br><span class="line"></span><br><span class="line">equal_frequency_discretiser = KBinsDiscretizer(n_bins=<span class="number">10</span>, encode=<span class="string">&#x27;ordinal&#x27;</span>, strategy=<span class="string">&#x27;uniform&#x27;</span>)</span><br><span class="line">equal_width_discretiser = KBinsDiscretizer(n_bins=<span class="number">10</span>, encode=<span class="string">&#x27;ordinal&#x27;</span>, strategy=<span class="string">&#x27;quantile&#x27;</span>)</span><br><span class="line">kmeans_cluster_discretiser = KBinsDiscretizer(n_bins=<span class="number">10</span>, encode=<span class="string">&#x27;ordinal&#x27;</span>, strategy=<span class="string">&#x27;kmeans&#x27;</span>)</span><br><span class="line"></span><br><span class="line">X_binned = equal_width_discretiser.fit_transform(X_train)</span><br><span class="line">X_binned.bin_edges_</span><br></pre></td></tr></table></figure>
<h2 id="异常值检测"><a class="markdownIt-Anchor" href="#异常值检测"></a> 异常值检测</h2>
<p>我们在实际项目中拿到的数据往往有不少异常数据，这些异常数据很可能让我们模型有很大的偏差。异常检测的方法有很多，例如3倍标准差、箱线法的单变量标记，或者聚类、iForest和LocalOutlierFactor等无监督学习方法。</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>python模块</th>
</tr>
</thead>
<tbody>
<tr>
<td>分位数检测</td>
<td>self-define</td>
</tr>
<tr>
<td>3倍标准差原则</td>
<td>self-define</td>
</tr>
<tr>
<td>聚类检测</td>
<td>self-define</td>
</tr>
<tr>
<td>One Class SVM</td>
<td>sklearn.svm.OneClassSVM</td>
</tr>
<tr>
<td>Elliptic Envelope</td>
<td>sklearn.linear_model.SGDOneClassSVM</td>
</tr>
<tr>
<td>Elliptic Envelope</td>
<td>sklearn.covariance.EllipticEnvelope</td>
</tr>
<tr>
<td>Isolation Forest</td>
<td>sklearn.ensemble.IsolationForest</td>
</tr>
<tr>
<td>LOF</td>
<td>sklearn.neighbors.LocalOutlierFactor</td>
</tr>
</tbody>
</table>
<p><strong>箱线图检测</strong>根据四分位点判断是否异常。四分位数具有鲁棒性，不受异常值的干扰。通常认为小于 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>Q</mi><mn>1</mn></msub><mo>−</mo><mn>1.5</mn><mo>∗</mo><mi>I</mi><mi>Q</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">Q_1-1.5*IQR</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mord mathdefault">Q</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span></span></span></span> 或大于 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>Q</mi><mn>3</mn></msub><mo>+</mo><mn>1.5</mn><mo>∗</mo><mi>I</mi><mi>Q</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">Q_3+1.5*IQR</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mord mathdefault">Q</span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span></span></span></span> 的点为离群点。</p>
<p><strong>3倍标准差原则</strong>：假设数据满足正态分布，通常定义偏离均值的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>3</mn><mi>σ</mi></mrow><annotation encoding="application/x-tex">3\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span> 之外内的点为离群点，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="double-struck">P</mi><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi>X</mi><mo>−</mo><mi>μ</mi><mi mathvariant="normal">∣</mi><mo>&lt;</mo><mn>3</mn><mi>σ</mi><mo stretchy="false">)</mo><mo>=</mo><mn>99.73</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\mathbb P(|X-\mu|&lt;3\sigma)=99.73\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathbb">P</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">μ</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3</span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.80556em;vertical-align:-0.05556em;"></span><span class="mord">9</span><span class="mord">9</span><span class="mord">.</span><span class="mord">7</span><span class="mord">3</span><span class="mord">%</span></span></span></span>。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。</p>
<p>使用pandas实现，并封装在transformer中</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OutlierCapper</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Caps maximum and/or minimum values of a variable at automatically</span></span><br><span class="line"><span class="string">    determined values.</span></span><br><span class="line"><span class="string">    Works only with numerical variables. A list of variables can be indicated. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    method: str, &#x27;gaussian&#x27; or &#x27;iqr&#x27;, default=&#x27;iqr&#x27;</span></span><br><span class="line"><span class="string">        If method=&#x27;gaussian&#x27;: </span></span><br><span class="line"><span class="string">            - upper limit: mean + 3* std</span></span><br><span class="line"><span class="string">            - lower limit: mean - 3* std</span></span><br><span class="line"><span class="string">        If method=&#x27;iqr&#x27;: </span></span><br><span class="line"><span class="string">            - upper limit: 75th quantile + 3* IQR</span></span><br><span class="line"><span class="string">            - lower limit: 25th quantile - 3* IQR</span></span><br><span class="line"><span class="string">            where IQR is the inter-quartile range: 75th quantile - 25th quantile.</span></span><br><span class="line"><span class="string">    fold: int, default=3   </span></span><br><span class="line"><span class="string">        You can select how far out to cap the maximum or minimum values.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">method=<span class="string">&#x27;iqr&#x27;</span>, fold=<span class="number">3</span>, variables=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.method = method</span><br><span class="line">        self.fold = fold</span><br><span class="line">        self.variables = variables</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Learn the values that should be used to replace outliers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X : pandas dataframe of shape = [n_samples, n_features]</span></span><br><span class="line"><span class="string">            The training input samples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        y : pandas Series, default=None</span></span><br><span class="line"><span class="string">            y is not needed in this transformer. You can pass y or None.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># check input dataframe</span></span><br><span class="line">        X = check_X(X)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the names and number of features in the train set.</span></span><br><span class="line">        self.feature_names_in_ = X.columns.to_list()</span><br><span class="line">        self.n_features_in_ = X.shape[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># find or check for numerical variables</span></span><br><span class="line">        numeric_vars = X.select_dtypes(<span class="string">&quot;number&quot;</span>).columns.to_list()</span><br><span class="line">        <span class="keyword">if</span> self.variables <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.variables = numeric_vars</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.variables = <span class="built_in">list</span>(<span class="built_in">set</span>(numeric_vars) &amp; <span class="built_in">set</span>(self.variables))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.method == <span class="string">&quot;gaussian&quot;</span>:</span><br><span class="line">            mean = X[self.variables].mean()</span><br><span class="line">            bias= [mean, mean]</span><br><span class="line">            scale = X[self.variables].std(ddof=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">elif</span> self.method == <span class="string">&quot;iqr&quot;</span>:</span><br><span class="line">            Q1, Q3 = X[self.variables].quantile(q=(<span class="number">0.25</span>, <span class="number">0.75</span>))</span><br><span class="line">            bias = [Q1, Q3]</span><br><span class="line">            scale = Q3 - Q1         </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># estimate the end values</span></span><br><span class="line">        <span class="keyword">if</span> (scale == <span class="number">0</span>).<span class="built_in">any</span>():</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;Input columns <span class="subst">&#123;scale[scale == <span class="number">0</span>].index.tolist()!r&#125;</span>&quot;</span></span><br><span class="line">                <span class="string">f&quot; have low variation for method <span class="subst">&#123;self.capping_method!r&#125;</span>.&quot;</span></span><br><span class="line">                <span class="string">f&quot; Try other capping methods or drop these columns.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.upper_limit = bias[<span class="number">1</span>] + self.fold * scale</span><br><span class="line">            self.lower_limit = bias[<span class="number">0</span>] - self.fold * scale   </span><br><span class="line"></span><br><span class="line">        self.feature_names_in_ = X.columns.to_list()</span><br><span class="line">        self.n_features_in_ = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self <span class="comment"># always return self!</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Cap the variable values.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X: pandas dataframe of shape = [n_samples, n_features]</span></span><br><span class="line"><span class="string">            The data to be transformed.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        X_new: pandas dataframe of shape = [n_samples, n_features]</span></span><br><span class="line"><span class="string">            The dataframe with the capped variables.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># check if class was fitted</span></span><br><span class="line">        check_is_fitted(self)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># replace outliers</span></span><br><span class="line">        X[self.variables] = X[self.variables].clip(</span><br><span class="line">            upper=self.upper_limit,</span><br><span class="line">            lower=self.lower_limit</span><br><span class="line">        )</span><br><span class="line">        outiers = (X[self.variables].gt(self.upper_limit) | </span><br><span class="line">                   X[self.variables].gt(self.lower_limit))</span><br><span class="line">        n = outiers.<span class="built_in">sum</span>().gt(<span class="number">0</span>).<span class="built_in">sum</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Your selected dataframe has <span class="subst">&#123;n&#125;</span> out of <span class="subst">&#123;outiers.shape[<span class="number">1</span>]&#125;</span> columns that have outliers.&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_feature_names_out</span>(<span class="params">input_features=<span class="literal">None</span></span>):</span></span><br><span class="line">        check_is_fitted(self)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> input_features <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> self.feature_names_in_</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">len</span>(input_features) == <span class="built_in">len</span>(self.n_features_in_):</span><br><span class="line">            <span class="comment"># If the input was an array, we let the user enter the variable names.</span></span><br><span class="line">            <span class="keyword">return</span> = <span class="built_in">list</span>(input_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;The number of input_features does not match the number of &quot;</span></span><br><span class="line">                <span class="string">&quot;features seen in the dataframe used in fit.&quot;</span></span><br><span class="line">                ) </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">outlier_capper = OutlierCapper()</span><br><span class="line">_ = outlier_capper.fit_transform(X)</span><br></pre></td></tr></table></figure>
<p>sklearn 包目前支持的异常检测算法：</p>
<ul>
<li><strong>One Class SVM</strong>：基于 SVM (使用高斯内核) 的思想在特征空间中训练一个超球面，边界外的点即为异常值。</li>
<li><strong>Elliptic Envelope</strong>：假设数据满足正态分布，训练一个椭圆包络线，边界外的点则为离群点 。</li>
<li><strong>Isolation Forest</strong>：是一种高效的异常检测算法，它和随机森林类似，但每次分裂特征和划分点（值）时都是随机的，而不是根据信息增益或基尼指数来选择。</li>
<li><strong>LOF</strong>：基于密度的异常检测算法。离群点的局部密度显著低于大部分近邻点，适用于非均匀的数据集。</li>
<li><strong>聚类检测</strong>：常用KMeans聚类将训练样本分成若干个簇，如果某一个簇里的样本数很少，而且簇质心和其他所有的簇都很远，那么这个簇里面的样本极有可能是异常特征样本了。</li>
</ul>
<p>筛选出来的异常样本需要根据实际含义处理：</p>
<ul>
<li>根据异常点的数量和影响，考虑是否将该条记录删除。</li>
<li>对数据做 log-scale 变换后消除异常值。</li>
<li>通过数据分箱来平滑异常值。</li>
<li>使用均值/中位数/众数来修正替代异常点，简单高效。</li>
<li>标记异常值或新增异常值得分列。</li>
<li>树模型对离群点的鲁棒性较高，可以选择忽略异常值。</li>
</ul>
<p>我们接下来考虑对数值型变量添加箱线图异常标记，计算iForest得分并标记异常样本。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> IsolationForest</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomIsolationForest</span>(<span class="params">IsolationForest, TransformerMixin</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Isolation Forest Algorithm.</span></span><br><span class="line"><span class="string">    Compute the anomaly score of each sample using the IsolationForest algorithm.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs, drop=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.drop = drop</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span>  </span><br><span class="line">        anomaly_scores = <span class="built_in">super</span>().decision_function(X)</span><br><span class="line">        pred = <span class="built_in">super</span>().predict(X)</span><br><span class="line">        n_outiers = pred[pred == -<span class="number">1</span>].size</span><br><span class="line">        <span class="keyword">if</span> self.drop:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Remove <span class="subst">&#123;n_outiers&#125;</span> outliers from the dataset&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> X.loc[pred == <span class="number">1</span>,:]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Return average anomaly score of X.</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;The number of outiers: <span class="subst">&#123;n_outiers&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> anomaly_scores.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_feature_names_out</span>(<span class="params">self, input_features=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.drop:</span><br><span class="line">            <span class="keyword">return</span> self.feature_names_in_</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="string">&quot;anomaly_score&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># fit the model for anomaly detection</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="缺失值处理"><a class="markdownIt-Anchor" href="#缺失值处理"></a> 缺失值处理</h2>
<p>特征有缺失值是非常常见的，大部分机器学习模型在拟合前需要处理缺失值（Handle Missing Values）。</p>
<p>缺失值统计</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Function to calculate missing values by column</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_missing</span>(<span class="params">df, threshold=<span class="literal">None</span>, verbose=<span class="literal">True</span></span>):</span></span><br><span class="line">		missing_df = pd.DataFrame(&#123;</span><br><span class="line">		<span class="string">&quot;missing_number&quot;</span>: df.isna().<span class="built_in">sum</span>(),  <span class="comment"># Total missing values</span></span><br><span class="line">		<span class="string">&quot;missing_rate&quot;</span>: df.isna().mean()   <span class="comment"># Proportion of missing values</span></span><br><span class="line">		&#125;, index=df.columns)</span><br><span class="line">    missing_df = missing_df.query(<span class="string">&quot;missing_rate&gt;0&quot;</span>).sort_values(<span class="string">&quot;missing_rate&quot;</span>, ascending=<span class="literal">True</span>)</span><br><span class="line">    threshold = <span class="number">0.25</span> <span class="keyword">if</span> threshold <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> threshold</span><br><span class="line">    high_missing = missing_df.query(<span class="string">f&quot;missing_rate&gt;<span class="subst">&#123;threshold&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># Print some summary information</span></span><br><span class="line">    <span class="keyword">if</span> verbose:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">f&quot;Your selected dataframe has <span class="subst">&#123;missing_df.shape[<span class="number">0</span>]&#125;</span> out of <span class="subst">&#123;df.shape[<span class="number">0</span>]&#125;</span> columns that have missing values.&quot;</span>)</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">f&quot;There are <span class="subst">&#123;high_missing.shape[<span class="number">0</span>]&#125;</span> columns with more than <span class="subst">&#123;threshold:<span class="number">.1</span>%&#125;</span> missing values.&quot;</span>)</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;Columns with high missing rate:&quot;</span>, high_missing.index.tolist())</span><br><span class="line">    <span class="comment"># Return the dataframe with missing information</span></span><br><span class="line">    <span class="keyword">if</span> threshold <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      <span class="keyword">return</span> missing_df</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> high_missing</span><br><span class="line"></span><br><span class="line"><span class="comment"># Missing values statistics</span></span><br><span class="line"><span class="built_in">print</span>(display_missing(df).head(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>可视化缺失率</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.xlabel(<span class="string">&#x27;Percent of missing values&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Features&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Percent missing data by feature&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">sns.barplot(y=<span class="string">&quot;missing_rate&quot;</span>, x=<span class="string">&#x27;index&#x27;</span>, data=display_missing(df).head(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>缺失值处理通常有两种策略：</p>
<ul>
<li>缺失值可以用常量值估算，也可以使用缺失值所在列的统计信息（平均值、中位数或众数）估算。</li>
<li>如果某个特征的缺失值超过阈值（例如20%），那么该特征对模型的贡献就会降低，通常就可以考虑删除该特征。</li>
</ul>
<table>
<thead>
<tr>
<th>缺失值处理方法</th>
<th>函数</th>
<th>python包</th>
</tr>
</thead>
<tbody>
<tr>
<td>统计量插补</td>
<td>SimpleImputer</td>
<td>sklearn.impute</td>
</tr>
<tr>
<td>统计量/随机插补</td>
<td>df.fillna()</td>
<td>pandas</td>
</tr>
<tr>
<td>多重插补</td>
<td>IterativeImputer</td>
<td>sklearn.impute</td>
</tr>
<tr>
<td>最近邻插补</td>
<td>KNNImputer</td>
<td>sklearn.impute</td>
</tr>
<tr>
<td>缺失值删除</td>
<td>df.dropna()</td>
<td>pandas</td>
</tr>
<tr>
<td>缺失值标记</td>
<td>MissingIndicator</td>
<td>sklearn.impute</td>
</tr>
<tr>
<td>缺失值标记</td>
<td>df.isna(), df.isnull()</td>
<td>pandas</td>
</tr>
</tbody>
</table>
<p>首先，删除缺失值超过20%的特征</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">threshold = <span class="built_in">int</span>(df.shape[<span class="number">0</span>]*<span class="number">0.8</span>)</span><br><span class="line">df.dropna(axis=<span class="number">1</span>, thresh=threshold)</span><br></pre></td></tr></table></figure>
<p>有时，对于每个含有缺失值的列，我们额外添加一列来表示该列中缺失值的位置，在某些应用中，能取得不错的效果。<br />
继续分析之前清洗过的 DAYS_EMPLOYED 异常，我们对缺失数据进行标记，看看他们是否影响客户违约。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">app_train.groupby(app_train[<span class="string">&#x27;DAYS_EMPLOYED&#x27;</span>].isna())[target].mean()</span><br></pre></td></tr></table></figure>
<p>发现缺失值的逾期率（）低于正常值的逾期率（），与Target的相关性很强，因此新增一列DAYS_EMPLOYED_MISSING 标记。这种处理应该是对线性方法比较有效，而基于树的方法应该可以自动识别。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create a flag column</span></span><br><span class="line">app_train[<span class="string">&#x27;DAYS_EMPLOYED_MISSING&#x27;</span>] = app_train[<span class="string">&quot;DAYS_EMPLOYED&quot;</span>].isna()</span><br></pre></td></tr></table></figure>
<p>然后，根据业务知识来进行人工填充。</p>
<p>若变量是离散型，且不同值较少，可在编码时转换成哑变量。例如，性别变量 code_gender</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.get_dummies(df[<span class="string">&quot;code_gender&quot;</span>], dummy_na=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>若变量是布尔型，视情况可统一填充为零</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.<span class="built_in">filter</span>(regex=<span class="string">&quot;^FLAG_&quot;</span>).fillna(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>如果我们仔细观察一下字段描述，会发现很多缺失值都有迹可循，比如name_type_suite缺失，办理贷款的时候无人陪同，因此可以用 None 来填补。客户的社会关系中有多少30天/60天逾期及申请贷款前1小时/天/周/月/季度/年查询了多少次征信都可填充为数字0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">features_fill_none = [<span class="string">&quot;name_type_suite&quot;</span>, <span class="string">&quot;occupation_type&quot;</span>, <span class="string">&quot;orgnization_type&quot;</span>]</span><br><span class="line">df[features_fill_none].fillna(<span class="string">&#x27;None&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">features_fill_zero = [<span class="string">&quot;obs_30_cnt_social_circle&quot;</span>, <span class="string">&quot;AMT_REQ_CREDIT_BUREAU_HOUR&quot;</span>] </span><br><span class="line">df[features_fill_zero].fillna(<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>对于缺失率较低（小于5%）的数值特征可以用中位数或均值插补，缺失率较低（小于5%）的离散型特征，则可以用众数插补。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">features_fill_mean = df.select_dtypes(<span class="string">&quot;number&quot;</span>).isna().mean().lt(<span class="number">0.05</span>).columns.tolist()</span><br><span class="line">df[features_fill_mean].fillna(df[features_fill_mean].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">features_fill_mode = df[df.select_dtypes(<span class="string">&quot;category&quot;</span>).isna().mean().lt(<span class="number">0.05</span>)]</span><br><span class="line">df[features_fill_mode].fillna(df[features_fill_mode].mode(), inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>通过之前的相关分析，我们知道LotFrontage这个特征与LotAreaCut和Neighborhood有比较大的关系，所以这里用这两个特征分组后的中位数进行插补，称为条件平均值填充法（Conditional Mean Completer）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood</span></span><br><span class="line">df[<span class="string">&quot;LotFrontage&quot;</span>] = df.groupby(<span class="string">&quot;Neighborhood&quot;</span>)[<span class="string">&quot;LotFrontage&quot;</span>].transform(</span><br><span class="line">    <span class="keyword">lambda</span> x: x.fillna(x.median()))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Filling the missing values in Age with the medians of Sex and Pclass groups</span></span><br><span class="line">grouped = df.groupby([<span class="string">&#x27;Sex&#x27;</span>, <span class="string">&#x27;Pclass&#x27;</span>])[<span class="string">&#x27;Age&#x27;</span>].transform(<span class="string">&#x27;median&#x27;</span>)</span><br><span class="line">df[<span class="string">&#x27;Age&#x27;</span>].fillna(grouped, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>最后，总结下我们的缺失处理策略：</p>
<ul>
<li>为每个特征添加缺失标记，特征选择阶段可以用卡方检验选择性删除</li>
<li>有业务含义的进行人工插补</li>
<li>缺失率高于20%：删除特征</li>
<li>缺失率5-20%：多重插补或条件平均插补</li>
<li>缺失率低于5%：简单统计插补</li>
</ul>
<p>使用pandas实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. Adds a binary variable to flag missing observations.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_missing_indicator</span>(<span class="params">X, y=<span class="literal">None</span>, alpha=<span class="number">0.05</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Adds a binary variable to flag missing observations(one indicator per variable). </span></span><br><span class="line"><span class="string">    The added variables (missing indicators) are named with the original variable name plus &#x27;_missing&#x27;.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    alpha: float, default=0.05</span></span><br><span class="line"><span class="string">        Features with p-values more than alpha are selected.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Compute chi-squared stats between each missing indicator and y.</span></span><br><span class="line">	chi2_stats, p_values = chi2(X.isna(), y)</span><br><span class="line">	<span class="comment"># find variables for which indicator should be added.</span></span><br><span class="line">	missing_indicator = X.iloc[:, p_values &gt; alpha]</span><br><span class="line">	indicator_names = missing_indicator.columns.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x + <span class="string">&quot;_missing&quot;</span>)</span><br><span class="line">	X[indicator_names] = missing_indicator</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&quot;Added <span class="subst">&#123;<span class="built_in">len</span>(missing_indicator)&#125;</span> missing indicators&quot;</span>)</span><br><span class="line">	<span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. manual imputer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">impute_manually</span>(<span class="params">X</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Replaces missing values by an arbitrary value</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># boolean</span></span><br><span class="line">    X.<span class="built_in">filter</span>(regex=<span class="string">&quot;^FLAG_&quot;</span>).fillna(<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># fill none</span></span><br><span class="line">    features_fill_none = [<span class="string">&quot;name_type_suite&quot;</span>, <span class="string">&quot;occupation_type&quot;</span>, <span class="string">&quot;orgnization_type&quot;</span>]</span><br><span class="line">    X[features_fill_none].fillna(<span class="string">&#x27;None&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># fill 0</span></span><br><span class="line">    features_fill_zero = [<span class="string">&quot;obs_30_cnt_social_circle&quot;</span>, <span class="string">&quot;AMT_REQ_CREDIT_BUREAU_HOUR&quot;</span>]</span><br><span class="line">    X[features_fill_zero].fillna(<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Remove variables with high missing rate</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop_missing_data</span>(<span class="params">X, threshold=<span class="number">0.8</span></span>):</span></span><br><span class="line">    <span class="comment"># Remove variables with missing more than threshold(default 20%)</span></span><br><span class="line">    threshold = <span class="built_in">int</span>(X.size * threshold)</span><br><span class="line">    X_new = X.dropna(axis=<span class="number">1</span>, thresh=threshold)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Removed <span class="subst">&#123;<span class="built_in">len</span>(X)-<span class="built_in">len</span>(X_new)&#125;</span> variables with missing more than <span class="subst">&#123;threshold:%&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> X_new</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. conditional statistic completer</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> mutual_info_regression, mutual_info_classif</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_classif, chi2</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> r_regression, f_regression</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fillna_by_groups</span>(<span class="params">X, threshold=<span class="number">0.8</span>, groupby=<span class="literal">None</span>, k=<span class="number">2</span>, min_categories=<span class="number">2</span>, bins=<span class="number">10</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Replaces missing values by groups.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    k: int, default=2</span></span><br><span class="line"><span class="string">        Number of top features to group by.</span></span><br><span class="line"><span class="string">    min_categories: int, default=2</span></span><br><span class="line"><span class="string">        Specifies an lower limit to the number of categories for each feature to group by.</span></span><br><span class="line"><span class="string">        If None, there is no limit.</span></span><br><span class="line"><span class="string">    bins: int, default=10</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    Y = X.copy()</span><br><span class="line">    variables = Y.isna().mean().lt(<span class="number">1</span> - threshold).index.to_list()</span><br><span class="line">    </span><br><span class="line">    features_num = X.select_dtypes(<span class="string">&#x27;number&#x27;</span>).columns.to_list()</span><br><span class="line">    features_cat = [colname <span class="keyword">for</span> colname <span class="keyword">in</span> X.columns <span class="keyword">if</span> colname <span class="keyword">not</span> <span class="keyword">in</span> features_num]</span><br><span class="line">    X[features_num] = X[features_num].apply(pd.qcut, q=bins, duplicates=<span class="string">&quot;drop&quot;</span>)</span><br><span class="line">    X[features_cat] = X[features_cat].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">    X = X.transform(<span class="keyword">lambda</span> x: x.cat.codes)</span><br><span class="line">    X = X.transform(<span class="keyword">lambda</span> x: x - x.<span class="built_in">min</span>()) <span class="comment"># for chi-squared to stats each non-negative feature</span></span><br><span class="line">    <span class="keyword">if</span> groupby <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        features_groupby = X.columns.tolist()</span><br><span class="line">    useful_columns = X.nunique().geq(min_categories).index.tolist()</span><br><span class="line">    features_groupby = <span class="built_in">list</span>(<span class="built_in">set</span>(features_groupby) &amp; <span class="built_in">set</span>(useful_columns))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Estimate mutual information for a target variable.</span></span><br><span class="line">    <span class="keyword">for</span> colname <span class="keyword">in</span> variables:</span><br><span class="line">        other_features = <span class="built_in">list</span>(<span class="built_in">set</span>(features_groupby) - &#123;colname&#125;)</span><br><span class="line">        <span class="keyword">if</span> colname <span class="keyword">in</span> features_num:</span><br><span class="line">            score_func = mutual_info_regression</span><br><span class="line">        <span class="keyword">elif</span> colname <span class="keyword">in</span> features_cat:</span><br><span class="line">            score_func = mutual_info_classif</span><br><span class="line">        scores = score_func(X[other_features], Y[colname], discrete_features=<span class="literal">True</span>)</span><br><span class="line">        scores = pd.Series(scores, index=other_features).sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line">        vars_top_k = scores[:K].index.tolist()</span><br><span class="line">        <span class="keyword">if</span> colname <span class="keyword">in</span> features_num:</span><br><span class="line">            <span class="comment"># Replaces missing values by the mean or median</span></span><br><span class="line">            Y[colname] = Y.groupby(vars_top_k)[colname].transform(<span class="keyword">lambda</span> x:x.fillna(x.median()))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Filling the missing values in <span class="subst">&#123;colname&#125;</span> with the medians of <span class="subst">&#123;vars_top_k&#125;</span> groups.&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> colname <span class="keyword">in</span> features_cat:</span><br><span class="line">            <span class="comment"># Replaces missing values by the most frequent category</span></span><br><span class="line">            Y[colname] = Y.groupby(vars_top_k)[colname].transform(<span class="keyword">lambda</span> x:x.fillna(x.mode()[<span class="number">0</span>]))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Filling the missing values in <span class="subst">&#123;colname&#125;</span> with the modes of <span class="subst">&#123;vars_top_k&#125;</span> groups.&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Transformed <span class="subst">&#123;<span class="built_in">len</span>(variables)&#125;</span> variables with missing (threshold=<span class="subst">&#123;threshold:<span class="number">.1</span>%&#125;</span>).&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Simple imputer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">impute_simply</span>(<span class="params">X, threshold=<span class="number">0.8</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Univariate imputer for completing missing values with simple strategies.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    variables = X.isna().mean().lt(<span class="number">1</span> - threshold).index.to_list()</span><br><span class="line">    features_num = X[variables].select_dtypes(<span class="string">&#x27;number&#x27;</span>).columns.to_list()</span><br><span class="line">    features_cat = [colname <span class="keyword">for</span> colname <span class="keyword">in</span> variables <span class="keyword">if</span> colname <span class="keyword">not</span> <span class="keyword">in</span> features_num]</span><br><span class="line">    <span class="comment"># Replaces missing values by the median or mode</span></span><br><span class="line">    medians = X[features_num].median().to_dict()</span><br><span class="line">    modes = X[features_cat].apply(<span class="keyword">lambda</span> x: x.mode()[<span class="number">0</span>]).to_dict()</span><br><span class="line">    impute_dict = &#123;**medians, **modes&#125;</span><br><span class="line">    X[variables] = X[variables].fillna(impute_dict)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Transformed <span class="subst">&#123;<span class="built_in">len</span>(variables)&#125;</span> variables with missing (threshold=<span class="subst">&#123;threshold:<span class="number">.1</span>%&#125;</span>).&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<p>使用sklearn实现</p>
<p>先自定义几个转换器</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"><span class="keyword">from</span> sklearn.utils.validation <span class="keyword">import</span> check_X, check_is_fitted</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DropMissingData</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Remove features from data.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    threshold: float, default=None</span></span><br><span class="line"><span class="string">        Require that percentage of non-NA values in a column to keep it.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, threshold=<span class="number">0.8</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="number">0</span> &lt; threshold &lt;= <span class="number">1</span>:</span><br><span class="line">            self.threshold = threshold</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;threshold must be a value between 0 &lt; x &lt;= 1. &quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Find the rows for which missing data should be evaluated to decide if a</span></span><br><span class="line"><span class="string">        variable should be dropped.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X: pandas dataframe of shape = [n_samples, n_features]</span></span><br><span class="line"><span class="string">            The training data set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        y: pandas Series, default=None</span></span><br><span class="line"><span class="string">            y is not needed. You can pass None or y.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># check input dataframe</span></span><br><span class="line">        X = check_X(X)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the names and number of features in the train set (the dataframe used during fit).</span></span><br><span class="line">        self.feature_names_in_ = X.columns.to_list()</span><br><span class="line">        self.n_features_in_ = X.shape[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Find the features to keep</span></span><br><span class="line">        self.variables = X.isna().mean().gt(<span class="number">1</span> - self.threshold).columns.to_list()</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span>	 </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Remove variables with missing more than threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X: pandas dataframe of shape = [n_samples, n_features]</span></span><br><span class="line"><span class="string">            The dataframe to be transformed.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        X_new: pandas dataframe</span></span><br><span class="line"><span class="string">            The complete case dataframe for the selected variables.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># Remove variables with missing more than threshold.</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Removed <span class="subst">&#123;<span class="built_in">len</span>(self.variables)&#125;</span> variables with missing more than <span class="subst">&#123;self.threshold:%&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> X.drop(self.variables)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_feature_names_out</span>(<span class="params">input_features=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Get output feature names for transformation. In other words, returns the</span></span><br><span class="line"><span class="string">        variable names of transformed dataframe.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        input_features : array or list, default=None</span></span><br><span class="line"><span class="string">            This parameter exits only for compatibility with the Scikit-learn pipeline.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            - If `None`, then `feature_names_in_` is used as feature names in.</span></span><br><span class="line"><span class="string">            - If an array or list, then `input_features` must match `feature_names_in_`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        feature_names_out: list</span></span><br><span class="line"><span class="string">            Transformed feature names.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        check_is_fitted(self)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> input_features <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            feature_names_in = self.feature_names_in_</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">len</span>(input_features) == <span class="built_in">len</span>(self.n_features_in_):</span><br><span class="line">            <span class="comment"># If the input was an array, we let the user enter the variable names.</span></span><br><span class="line">            feature_names_in = <span class="built_in">list</span>(input_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;The number of input_features does not match the number of &quot;</span></span><br><span class="line">                <span class="string">&quot;features seen in the dataframe used in fit.&quot;</span></span><br><span class="line">                )      </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Remove features.</span></span><br><span class="line">        feature_names_out = [var <span class="keyword">for</span> var <span class="keyword">in</span> self.feature_names_in_ <span class="keyword">if</span> var <span class="keyword">not</span> <span class="keyword">in</span> self.variables]</span><br><span class="line">        <span class="keyword">return</span> feature_names_out</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> mutual_info_regression, mutual_info_classif</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_classif, chi2</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> r_regression, f_regression</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConditionalStatisticImputer</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Replaces missing values by groups.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    threshold: float, default=None</span></span><br><span class="line"><span class="string">        Require that percentage of non-NA values in a column to impute.</span></span><br><span class="line"><span class="string">    k: int, default=2</span></span><br><span class="line"><span class="string">        Number of top features to group by.</span></span><br><span class="line"><span class="string">    min_categories: int, default=2</span></span><br><span class="line"><span class="string">        Specifies an lower limit to the number of categories for each feature to group by.</span></span><br><span class="line"><span class="string">        If None, there is no limit.</span></span><br><span class="line"><span class="string">    bins: int, default=10</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, threshold=<span class="number">0.8</span>, groupby=<span class="literal">None</span>, k=<span class="number">2</span>, min_categories=<span class="number">2</span>, bins=<span class="number">10</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="number">0</span> &lt; threshold &lt;= <span class="number">1</span>:</span><br><span class="line">            self.threshold = threshold</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;threshold must be a value between 0 &lt; x &lt;= 1. &quot;</span>)</span><br><span class="line">        self.groupby = groupby </span><br><span class="line">        self.k = k</span><br><span class="line">        self.min_categories = min_categories</span><br><span class="line">        self.bins = bins</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Compute correlation coefficient matrix.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X: pandas dataframe of shape = [n_samples, n_features]</span></span><br><span class="line"><span class="string">            The training data set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        y: pandas Series, default=None</span></span><br><span class="line"><span class="string">            y is not needed. You can pass None or y.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># check input dataframe</span></span><br><span class="line">        X = check_X(X)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the names and number of features in the train set (the dataframe used during fit).</span></span><br><span class="line">        self.feature_names_in_ = X.columns.to_list()</span><br><span class="line">        self.n_features_in_ = X.shape[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        Y = X.copy()</span><br><span class="line">        self.variables = Y.isna().mean().lt(<span class="number">1</span> - threshold).index.to_list()</span><br><span class="line">        X = self.discretize_encode(X, y=<span class="literal">None</span>, bins=self.bins)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.groupby <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            features_groupby = X.columns.to_list()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            features_groupby = <span class="built_in">list</span>(groupby)</span><br><span class="line">        useful_columns = X.nunique().geq(self.min_categories).index.to_list()</span><br><span class="line">        features_groupby = <span class="built_in">list</span>(<span class="built_in">set</span>(features_groupby) &amp; <span class="built_in">set</span>(useful_columns))</span><br><span class="line">        </span><br><span class="line">        score_matrix = pd.DataFrame(index=self.variables) <span class="comment"># init a matrix to hold scores</span></span><br><span class="line">        <span class="comment"># Estimate mutual information for a target variable.</span></span><br><span class="line">        <span class="keyword">for</span> colname <span class="keyword">in</span> self.variables:</span><br><span class="line">            <span class="keyword">if</span> colname <span class="keyword">in</span> features_num:</span><br><span class="line">                score_func = mutual_info_regression</span><br><span class="line">            <span class="keyword">elif</span> colname <span class="keyword">in</span> features_cat:</span><br><span class="line">                score_func = mutual_info_classif</span><br><span class="line">            scores = score_func(X[features_groupby], Y[colname], discrete_features=<span class="literal">True</span>)</span><br><span class="line">            score_matrix[colname] = pd.Series(scores, index=features_groupby).sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line">        self.score_matrix  = score_matrix </span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span>	 </span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Remove variables with missing more than threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        X: pandas dataframe of shape = [n_samples, n_features]</span></span><br><span class="line"><span class="string">            The dataframe to be transformed.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        X_new: pandas dataframe</span></span><br><span class="line"><span class="string">            The complete case dataframe for the selected variables.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        Y = X.copy()</span><br><span class="line">        X = self.discretize_encode(X, y=<span class="literal">None</span>, bins=self.bins)</span><br><span class="line">        features_num = X.select_dtypes(<span class="string">&#x27;number&#x27;</span>).columns.to_list()</span><br><span class="line">        features_cat = [colname <span class="keyword">for</span> colname <span class="keyword">in</span> X.columns <span class="keyword">if</span> colname <span class="keyword">not</span> <span class="keyword">in</span> features_num]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> colname <span class="keyword">in</span> self.variables:  </span><br><span class="line">            vars_top_k = self.score_matrix[colname].drop(colname)[:self.K].index.to_list()</span><br><span class="line">            <span class="keyword">if</span> colname <span class="keyword">in</span> features_num:</span><br><span class="line">                <span class="comment"># Replaces missing values by the mean or median</span></span><br><span class="line">                Y[colname] = Y.groupby(vars_top_k)[colname].transform(<span class="keyword">lambda</span> x:x.fillna(x.median()))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Filling the missing values in <span class="subst">&#123;colname&#125;</span> with the medians of <span class="subst">&#123;vars_top_k&#125;</span> groups.&quot;</span>)</span><br><span class="line">            <span class="keyword">elif</span> colname <span class="keyword">in</span> features_cat:</span><br><span class="line">                <span class="comment"># Replaces missing values by the most frequent category</span></span><br><span class="line">                Y[colname] = Y.groupby(vars_top_k)[colname].transform(<span class="keyword">lambda</span> x:x.fillna(x.mode()))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Filling the missing values in <span class="subst">&#123;colname&#125;</span> with the modes of <span class="subst">&#123;vars_top_k&#125;</span> groups.&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Transformed <span class="subst">&#123;<span class="built_in">len</span>(variables)&#125;</span> variables with missing (threshold=<span class="subst">&#123;threshold:<span class="number">.1</span>%&#125;</span>).&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">discretize_encode</span>(<span class="params">self, X, y=<span class="literal">None</span>, bins=<span class="number">10</span></span>):</span></span><br><span class="line">        features_num = X.select_dtypes(<span class="string">&#x27;number&#x27;</span>).columns.to_list()</span><br><span class="line">        features_cat = [colname <span class="keyword">for</span> colname <span class="keyword">in</span> X.columns <span class="keyword">if</span> colname <span class="keyword">not</span> <span class="keyword">in</span> features_num]</span><br><span class="line">        </span><br><span class="line">        X[features_num] = X[features_num].apply(pd.qcut, q=bins, duplicates=<span class="string">&quot;drop&quot;</span>)</span><br><span class="line">        X[features_cat] = X[features_cat].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">        X = X.transform(<span class="keyword">lambda</span> x: x.cat.codes)</span><br><span class="line">        X = X.transform(<span class="keyword">lambda</span> x: x - x.<span class="built_in">min</span>()) <span class="comment"># for chi-squared to stats each non-negative feature</span></span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_feature_names_out</span>(<span class="params">input_features=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Get output feature names for transformation. In other words, returns the</span></span><br><span class="line"><span class="string">        variable names of transformed dataframe.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        input_features : array or list, default=None</span></span><br><span class="line"><span class="string">            This parameter exits only for compatibility with the Scikit-learn pipeline.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            - If `None`, then `feature_names_in_` is used as feature names in.</span></span><br><span class="line"><span class="string">            - If an array or list, then `input_features` must match `feature_names_in_`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        feature_names_out: list</span></span><br><span class="line"><span class="string">            Transformed feature names.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        check_is_fitted(self)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> input_features <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            feature_names_in = self.feature_names_in_</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">len</span>(input_features) == <span class="built_in">len</span>(self.n_features_in_):</span><br><span class="line">            <span class="comment"># If the input was an array, we let the user enter the variable names.</span></span><br><span class="line">            feature_names_in = <span class="built_in">list</span>(input_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;The number of input_features does not match the number of &quot;</span></span><br><span class="line">                <span class="string">&quot;features seen in the dataframe used in fit.&quot;</span></span><br><span class="line">                )      </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> feature_names_in</span><br></pre></td></tr></table></figure>
<p>整合到pipeline中</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> MissingIndicator</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer, IterativeImputer, KNNImputer</span><br><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer, make_column_transformer, make_column_selector</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> ColumnUnion, make_union, make_pipeline, make_union</span><br><span class="line"><span class="keyword">from</span> feature_selection <span class="keyword">import</span> SelectKBest, SelectFpr</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_column</span>(<span class="params">X</span>)</span></span><br><span class="line"><span class="function">    <span class="title">fill_dict</span> = <span class="title">dict</span>()</span></span><br><span class="line"><span class="function">    <span class="title">fill_dict</span>[&#x27;<span class="title">zero</span>&#x27;] = <span class="title">X</span>.<span class="title">columns</span>.<span class="title">str</span>.<span class="title">startswith</span>(<span class="params"><span class="string">&quot;FLAG_&quot;</span></span>).<span class="title">to_list</span>() + [&quot;<span class="title">obs_30_cnt_social_circle</span>&quot;, &quot;<span class="title">AMT_REQ_CREDIT_BUREAU_HOUR</span>&quot;]</span></span><br><span class="line"><span class="function">    <span class="title">fill_dict</span>[&#x27;<span class="title">None</span>&#x27;] = [&quot;<span class="title">name_type_suite</span>&quot;, &quot;<span class="title">occupation_type</span>&quot;, &quot;<span class="title">orgnization_type</span>&quot;]</span></span><br><span class="line"><span class="function">    <span class="title">fill_dict</span>[&#x27;<span class="title">median</span>&#x27;] = <span class="title">X</span>.<span class="title">select_dtypes</span>(<span class="params"><span class="string">&quot;number&quot;</span></span>).<span class="title">isna</span>().<span class="title">mean</span>().<span class="title">lt</span>(<span class="params"><span class="number">0.05</span></span>).<span class="title">index</span>.<span class="title">to_list</span>()</span></span><br><span class="line"><span class="function">    <span class="title">fill_dict</span>[&#x27;<span class="title">mode</span>&#x27;] = <span class="title">X</span>.<span class="title">select_dtypes</span>(<span class="params"><span class="string">&quot;category&quot;</span></span>).<span class="title">isna</span>().<span class="title">mean</span>().<span class="title">lt</span>(<span class="params"><span class="number">0.05</span></span>).<span class="title">index</span>.<span class="title">to_list</span>()</span></span><br><span class="line"><span class="function">    <span class="title">return</span> <span class="title">fill_dict</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">fill_dict</span> = <span class="title">select_column</span>(<span class="params">X_train</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">imputer</span> = <span class="title">make_column_transformer</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    (<span class="params">SimpleImputer(<span class="params">strategy=<span class="string">&quot;constant&quot;</span>, fill_value=<span class="string">&quot;None&quot;</span></span>), features_fill_none</span>)</span>),</span></span><br><span class="line"><span class="function">    (<span class="params">SimpleImputer(<span class="params">strategy=<span class="string">&quot;constant&quot;</span>, fill_value=<span class="number">0</span></span>), features_fill_zero</span>),</span></span><br><span class="line"><span class="function">    (<span class="params">SimpleImputer(<span class="params">strategy=<span class="string">&quot;median&quot;</span></span>), features_fill_median</span>),</span></span><br><span class="line"><span class="function">    (<span class="params">SimpleImputer(<span class="params">strategy=<span class="string">&quot;most_frequent&quot;</span></span>), features_fill_mode</span>),</span></span><br><span class="line"><span class="function">    <span class="title">remainder</span>=<span class="title">KNNImputer</span>(),</span></span><br><span class="line"><span class="function">    <span class="title">verbose</span>=<span class="title">True</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"># <span class="title">find</span> <span class="title">variables</span> <span class="title">for</span> <span class="title">which</span> <span class="title">indicator</span> <span class="title">should</span> <span class="title">be</span> <span class="title">added</span>.</span></span><br><span class="line"><span class="function"><span class="title">indicate_missing</span> = <span class="title">make_pipeline</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    MissingIndicator(<span class="params">features=<span class="string">&#x27;all&#x27;</span>, sparse=<span class="literal">False</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">    SelectFpr(<span class="params">alpha=<span class="number">0.05</span></span>)</span></span></span><br><span class="line"><span class="params"><span class="function">    verbose=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">handle_missing_pipeline</span> = <span class="title">make_union</span>(<span class="params">imputer, indicate_missing, verbose=<span class="literal">True</span></span>)</span></span><br></pre></td></tr></table></figure>
<p>最后确认缺失值是否已全部处理完毕：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.isna().<span class="built_in">sum</span>().<span class="built_in">max</span>()</span><br></pre></td></tr></table></figure>
<h2 id="标准化归一化"><a class="markdownIt-Anchor" href="#标准化归一化"></a> 标准化/归一化</h2>
<p>数据标准化和归一化可以提高一些算法的准确度，也能加速梯度下降收敛速度。也有不少模型不需要做标准化和归一化，主要是基于概率分布的模型，比如决策树大家族的CART，随机森林等。</p>
<ul>
<li><strong>z-score标准化</strong>是最常见的特征预处理方式，基本所有的线性模型在拟合的时候都会做标准化。前提是假设特征服从正态分布，标准化后，其转换成均值为0标准差为1的标准正态分布。</li>
<li><strong>max-min标准化</strong>也称为离差标准化，预处理后使特征值映射到[0,1]之间。这种方法的问题就是如果测试集或者预测数据里的特征有小于min，或者大于max的数据，会导致max和min发生变化，需要重新计算。所以实际算法中， 除非你对特征的取值区间有需求，否则max-min标准化没有 z-score标准化好用。</li>
<li><strong>L1/L2范数标准化</strong>：如果我们只是为了统一量纲，那么通过L2范数整体标准化。</li>
</ul>
<table>
<thead>
<tr>
<th>sklearn.preprocessing</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>StandardScaler()</td>
<td>z-score标准化</td>
</tr>
<tr>
<td>Normalizer(norm=‘l2’)</td>
<td>使用<code>l1</code>、<code>l2</code>或<code>max</code>范数归一化</td>
</tr>
<tr>
<td>MinMaxScaler()</td>
<td>min-max归一化</td>
</tr>
<tr>
<td>MaxAbsScaler()</td>
<td>Max-abs归一化，缩放稀疏数据的推荐方法</td>
</tr>
<tr>
<td>RobustScaler()</td>
<td>分位数归一化，推荐缩放有离群值的数据</td>
</tr>
</tbody>
</table>
<p>pandas实现z-score标准化和分位数归一化在之前检测离群值函数里已有，其他标准化方法不太常用。</p>
<p>由于数据集中依然存在一定的离群点，我们可以用RobustScaler对数据进行标准化处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scaler = RobustScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br></pre></td></tr></table></figure>
<h2 id="正态变换"><a class="markdownIt-Anchor" href="#正态变换"></a> 正态变换</h2>
<p>我们先来计算一下各个数值特征的偏度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Check the skew of all numerical features</span></span><br><span class="line">skewness = X_train.select_dtypes(<span class="string">&#x27;number&#x27;</span>).skew()</span><br><span class="line">skewness = skewness[<span class="built_in">abs</span>(skewness) &gt; <span class="number">0.75</span>].sort_values()</span><br></pre></td></tr></table></figure>
<p>可以看到这些特征的偏度较高，在许多回归算法中，尤其是线性模型，常常假设数值型特征服从正态分布。因此我们尝试变换，让数据接近正态分布。</p>
<p>以age特征为例，我们画出分布图和QQ图（使用之前定义的函数）。</p>
<blockquote>
<p>Quantile-Quantile图是一种常用的统计图形，用来比较两个数据集之间的分布。它是由标准正态分布的分位数为横坐标，样本值为纵坐标的散点图。如果QQ图上的点在一条直线附近，则说明数据近似于正态分布，且该直线的斜率为标准差，截距为均值。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">norm_comparison_plot(np.log1p(df[<span class="string">&#x27;SalePrice&#x27;</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>sklearn.preprocessing模块目前支持的非线性变换：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>QuantileTransformer</td>
<td>分位数变换，映射到[0,1]之间的均匀分布，或正态分布</td>
</tr>
<tr>
<td>PowerTransformer</td>
<td>幂变换，将数据从任何分布映射到尽可能接近高斯分布，以稳定方差并最小化倾斜度</td>
</tr>
</tbody>
</table>
<p>此外，最常用的是log变换。对于含有负数的特征，可以先min-max缩放到[0,1]之间后再做变换。</p>
<p>这里我们对age特征做Box-Cox变换</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Box Cox Transformation of skewed features (instead of log-transformation)</span></span><br><span class="line">norm_trans = PowerTransformer(<span class="string">&quot;box-cox&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">norm_comparison_plot(np.log1p(df[<span class="string">&#x27;SalePrice&#x27;</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>可以看到经过Box-Cox变换后，基本符合正态分布了。</p>
<h2 id="baseline"><a class="markdownIt-Anchor" href="#baseline"></a> Baseline</h2>
<p>至此，数据预处理已经基本完毕，我们可以选择模型开始训练了。为了检测模型的表现，我们先定义一个评估函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">score_dataset</span>(<span class="params">X, y, model</span>):</span></span><br><span class="line">    <span class="comment"># Metric for Housing competition is RMSLE (Root Mean Squared Log Error)</span></span><br><span class="line">    log_y = np.log(y)</span><br><span class="line">    score = cross_val_score(</span><br><span class="line">        model, X, log_y, cv=<span class="number">5</span>, scoring=<span class="string">&quot;neg_root_mean_squared_error&quot;</span>,</span><br><span class="line">    ).mean()</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span> * score</span><br><span class="line"></span><br><span class="line">X = df.drop(ID_col).copy()</span><br><span class="line">y = X.pop(target)</span><br><span class="line">X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=<span class="number">0.3</span>, randm_state=SEED)</span><br><span class="line">baseline_score = score_dataset(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Baseline score: <span class="subst">&#123;baseline_score:<span class="number">.5</span>f&#125;</span> RMSLE&quot;</span>)</span><br><span class="line"></span><br><span class="line">categorical_cols = df.select_dtypes([<span class="string">&quot;object&quot;</span>, <span class="string">&quot;category&quot;</span>]).columns.tolist()</span><br><span class="line">numeric_cols = df.select_dtypes(<span class="string">&quot;number&quot;</span>).columns.tolist()</span><br></pre></td></tr></table></figure>
<p>使用sklearn实现预处理的代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># split</span></span><br><span class="line">X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">cleaner = FunctionTransformer(clean, features_name_out=<span class="string">&#x27;one-to-one&#x27;</span>)</span><br><span class="line"></span><br><span class="line">prepare_data = Pipeline([</span><br><span class="line">    (<span class="string">&quot;clean&quot;</span>, cleaner),</span><br><span class="line">    (<span class="string">&quot;encode&quot;</span>, categorical_encoder),</span><br><span class="line">    mean_encoder</span><br><span class="line">    handle_missing_pipeline</span><br><span class="line">    (<span class="string">&quot;scale&quot;</span>, RobustScaler()),</span><br><span class="line">    (<span class="string">&quot;classifier&quot;</span>, LogitRegression())</span><br><span class="line">])</span><br><span class="line">categorical_encoder </span><br><span class="line">estimators = [(<span class="string">&#x27;linear_pca&#x27;</span>, PCA()), (<span class="string">&#x27;kernel_pca&#x27;</span>, KernelPCA())]</span><br><span class="line">combined = FeatureUnion(estimators)</span><br></pre></td></tr></table></figure>
<p>选择逻辑回归、SVM和LightGBM模型训练结果作为baseline</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>auc</th>
<th></th>
<th>SVM</th>
</tr>
</thead>
<tbody>
<tr>
<td>baseline</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a></div><div class="post_share"><div class="social-share" data-image="/img/FeatureEngine.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/morty3.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/morty3.jpg" alt="Give me money!"/></a><div class="post-qr-code-desc">Give me money!</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/ca507391/" title="特征工程(III)--特征构造"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/FeatureEngine.png" onerror="onerror=null;src='/img/404_moon.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">特征工程(III)--特征构造</div></div></a></div><div class="next-post pull-right"><a href="/posts/c8fe03db/" title="特征工程(V)--时序特征工程"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/FeatureEngine.png" onerror="onerror=null;src='/img/404_moon.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">特征工程(V)--时序特征工程</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/264c088/" title="大数据手册(Spark)--PySpark Core"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-core.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-03</div><div class="title">大数据手册(Spark)--PySpark Core</div></div></a></div><div><a href="/posts/75974533/" title="大数据手册(Spark)--PySpark MLlib"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-mllib.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-09</div><div class="title">大数据手册(Spark)--PySpark MLlib</div></div></a></div><div><a href="/posts/f67b0909/" title="大数据手册(Spark)--Pandas API on Spark"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-pandas.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-07</div><div class="title">大数据手册(Spark)--Pandas API on Spark</div></div></a></div><div><a href="/posts/34eba6aa/" title="大数据手册(Spark)--PySpark Streaming"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-streaming.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-13</div><div class="title">大数据手册(Spark)--PySpark Streaming</div></div></a></div><div><a href="/posts/bb755aa3/" title="大数据手册(Spark)--PySpark SQL"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-sql.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-03</div><div class="title">大数据手册(Spark)--PySpark SQL</div></div></a></div><div><a href="/posts/61a3ce5f/" title="特征工程(IV)--特征选择"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/FeatureEngine.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-16</div><div class="title">特征工程(IV)--特征选择</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Gitalk</span><span class="switch-btn"></span><span class="second-comment">Waline</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div><div><div id="waline-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Tiny Lei</div><div class="author-info__description">每天进步一点点...</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">161</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">105</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">43</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://gitee.com/wilenwu" rel="external nofollow noreferrer" target="_blank" title="Gitee"><i class="iconfont icon-gitee"></i></a><a class="social-icon" href="https://github.com/wilenwu" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="iconfont icon-github"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_41518277" rel="external nofollow noreferrer" target="_blank" title="CSDN"><i class="iconfont icon-csdn"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="iconfont icon-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">感谢访问本站，若喜欢请收藏^_^</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text"> 数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97"><span class="toc-number">1.1.</span> <span class="toc-text"> 数据清洗</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A6%BB%E6%95%A3%E7%89%B9%E5%BE%81%E7%BC%96%E7%A0%81"><span class="toc-number">1.2.</span> <span class="toc-text"> 离散特征编码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9E%E7%BB%AD%E7%89%B9%E5%BE%81%E5%88%86%E7%AE%B1"><span class="toc-number">1.3.</span> <span class="toc-text"> 连续特征分箱</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%82%E5%B8%B8%E5%80%BC%E6%A3%80%E6%B5%8B"><span class="toc-number">1.4.</span> <span class="toc-text"> 异常值检测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86"><span class="toc-number">1.5.</span> <span class="toc-text"> 缺失值处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%87%E5%87%86%E5%8C%96%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">1.6.</span> <span class="toc-text"> 标准化&#x2F;归一化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E6%80%81%E5%8F%98%E6%8D%A2"><span class="toc-number">1.7.</span> <span class="toc-text"> 正态变换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#baseline"><span class="toc-number">1.8.</span> <span class="toc-text"> Baseline</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/61a3ce5f/" title="特征工程(IV)--特征选择"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/FeatureEngine.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="特征工程(IV)--特征选择"/></a><div class="content"><a class="title" href="/posts/61a3ce5f/" title="特征工程(IV)--特征选择">特征工程(IV)--特征选择</a><time datetime="2024-03-16T15:40:52.000Z" title="发表于 2024-03-16 23:40:52">2024-03-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/c8fe03db/" title="特征工程(V)--时序特征工程"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/FeatureEngine.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="特征工程(V)--时序特征工程"/></a><div class="content"><a class="title" href="/posts/c8fe03db/" title="特征工程(V)--时序特征工程">特征工程(V)--时序特征工程</a><time datetime="2024-03-16T15:40:52.000Z" title="发表于 2024-03-16 23:40:52">2024-03-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/ce19bbb1/" title="特征工程(II)--数据预处理"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/FeatureEngine.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="特征工程(II)--数据预处理"/></a><div class="content"><a class="title" href="/posts/ce19bbb1/" title="特征工程(II)--数据预处理">特征工程(II)--数据预处理</a><time datetime="2024-03-16T15:40:52.000Z" title="发表于 2024-03-16 23:40:52">2024-03-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/ca507391/" title="特征工程(III)--特征构造"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/FeatureEngine.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="特征工程(III)--特征构造"/></a><div class="content"><a class="title" href="/posts/ca507391/" title="特征工程(III)--特征构造">特征工程(III)--特征构造</a><time datetime="2024-03-16T15:40:52.000Z" title="发表于 2024-03-16 23:40:52">2024-03-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/29bf27e3/" title="特征工程(I)--探索性数据分析"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/FeatureEngine.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="特征工程(I)--探索性数据分析"/></a><div class="content"><a class="title" href="/posts/29bf27e3/" title="特征工程(I)--探索性数据分析">特征工程(I)--探索性数据分析</a><time datetime="2024-03-16T15:40:52.000Z" title="发表于 2024-03-16 23:40:52">2024-03-16</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Tiny Lei</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'forest'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addModeChange('mermaid', runMermaid)

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '7c65134b48b13f306114',
      clientSecret: 'f049f68368a11925fdb69e57c64839eac94e13c1'',
      repo: 'gitalk-comments',
      owner: 'WilenWu',
      admin: ['WilenWu'],
      id: '341bbc9813a6723a6619136abe0aa5d2',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.textContent= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script><script>function loadWaline () {
  function initWaline () {
    const waline = Waline.init(Object.assign({
      el: '#waline-wrap',
      serverURL: 'https://waline-comments-9etq63pcv-wilenwu.vercel.app',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      path: window.location.pathname,
      comment: false,
    }, {"requiredMeta":["monsterid"]}))
  }

  if (typeof Waline === 'object') initWaline()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css').then(() => {
      getScript('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.js').then(initWaline)
    })
  }
}

if ('Gitalk' === 'Waline' || !true) {
  if (true) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
  else setTimeout(loadWaline, 0)
} else {
  function loadOtherComment () {
    loadWaline()
  }
}</script></div><script src="/js/custom.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>