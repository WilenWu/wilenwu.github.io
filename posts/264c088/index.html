<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大数据手册(Spark)--Spark Core and RDDs | 雷小小</title><meta name="author" content="Tiny Lei"><meta name="copyright" content="Tiny Lei"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Spark的核心RDD">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据手册(Spark)--Spark Core and RDDs">
<meta property="og:url" content="https://wilenwu.gitee.io/posts/264c088/index.html">
<meta property="og:site_name" content="雷小小">
<meta property="og:description" content="Spark的核心RDD">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wilenwu.gitee.io/img/apache-spark-core.png">
<meta property="article:published_time" content="2020-01-03T08:20:25.000Z">
<meta property="article:modified_time" content="2024-06-06T14:39:02.753Z">
<meta property="article:author" content="Tiny Lei">
<meta property="article:tag" content="python">
<meta property="article:tag" content="大数据">
<meta property="article:tag" content="spark">
<meta property="article:tag" content="rdd">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wilenwu.gitee.io/img/apache-spark-core.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://wilenwu.gitee.io/posts/264c088/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="baidu-site-verification" content="code-7rymn5Bitx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?654e7415ab55bed7c9c2bc6d665f03c5";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大数据手册(Spark)--Spark Core and RDDs',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-06 22:39:02'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_2849223_xh1ftc8qym.css"><link rel="stylesheet" href="/css/link-card.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="雷小小" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">179</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">109</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">47</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/user-guide/"><i class="fa-fw fa fa-compass"></i><span> 用户指南</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa fa-book"></i><span> 文档</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-images"></i><span> 图库</span></a></div><div class="menus_item"><a class="site-page" href="/ebook/"><i class="fa-fw fa fa-book-reader"></i><span> 电子书</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-star"></i><span> 收藏夹</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa-solid fa-circle-chevron-down"></i><span> 更多</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fa fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/analytics/"><i class="fa-fw fa fa-line-chart"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/log/"><i class="fa-fw fa fa-history"></i><span> 更新日志</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/apache-spark-top-img.svg')"><nav id="nav"><span id="blog-info"><a href="/" title="雷小小"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/favicon.png"/><span class="site-name">雷小小</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/user-guide/"><i class="fa-fw fa fa-compass"></i><span> 用户指南</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa fa-book"></i><span> 文档</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-images"></i><span> 图库</span></a></div><div class="menus_item"><a class="site-page" href="/ebook/"><i class="fa-fw fa fa-book-reader"></i><span> 电子书</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-star"></i><span> 收藏夹</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa-solid fa-circle-chevron-down"></i><span> 更多</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fa fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/analytics/"><i class="fa-fw fa fa-line-chart"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/log/"><i class="fa-fw fa fa-history"></i><span> 更新日志</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大数据手册(Spark)--Spark Core and RDDs<a class="post-edit-link" href="https://gitee.com/WilenWu/myblog/edit/master/source/_posts/bigdata/Spark-Core-and-RDDs.md" rel="external nofollow noreferrer" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-01-03T08:20:25.000Z" title="发表于 2020-01-03 16:20:25">2020-01-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-06T14:39:02.753Z" title="更新于 2024-06-06 22:39:02">2024-06-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/big-data/">Big Data</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/big-data/spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="大数据手册(Spark)--Spark Core and RDDs"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="启动-spark"><a class="markdownIt-Anchor" href="#启动-spark"></a> 启动 Spark</h1>
<blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://sparkbyexamples.com/pyspark-tutorial/#google_vignette">PySpark 3.5 Tutorial For Beginners with Examples</a></p>
</blockquote>
<h2 id="sparkcontext"><a class="markdownIt-Anchor" href="#sparkcontext"></a> SparkContext</h2>
<p>Spark程序必须做的第一件事是创建一个SparkContext对象，该对象告诉Spark如何访问集群。要创建SparkContext，需要先构建一个包含应用程序信息的SparkConf对象。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setMaster(<span class="string">&#x27;yarn&#x27;</span>).setAppName(<span class="string">&#x27;myApp&#x27;</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.SparkConf</th>
<th style="text-align:left">SparkConf 配置</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>SparkConf.setAppName(value)</code></td>
<td style="text-align:left">应用程序在集群UI上显示的名称</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkConf.setMaster(value)</code></td>
<td style="text-align:left">配置spark资源管理器</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkConf.contains(key)</code></td>
<td style="text-align:left">Conf中是否包含一个指定配置</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkConf.get(key, defaultValue=None)</code></td>
<td style="text-align:left">获取配置的某些键值</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkConf.getAll()</code></td>
<td style="text-align:left">得到所有的键值对</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkConf.set(key, value)</code></td>
<td style="text-align:left">设置配置属性</td>
</tr>
</tbody>
</table>
<p>参数master是Spark、Mesos或YARN集群URL，或者使用字符串 “local”启用本地模式。</p>
<ul>
<li>local：采用单线程运行spark，常用于本地开发测</li>
<li>local[n]：使用n线程运行spark</li>
<li>local[*]：使用逻辑CPU个数的线程运行</li>
<li>standalone：利用Spark自带的资源管理与调度器运行Spark集群，采用Master/Slave结构</li>
<li>yarn : 集群运行在Yarn资源管理器上，资源管理交给Yarn，Spark只负责进行任务调度和计算</li>
</ul>
<p>一旦SparkConf对象被传递给SparkContext，它就不能被修改。</p>
<table>
<thead>
<tr>
<th style="text-align:left">pyspark.SparkContext</th>
<th style="text-align:left">获取 SparkContext 信息</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>SparkContext.version</code></td>
<td style="text-align:left">获取 Spark 版本</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkContext.pythonVer</code></td>
<td style="text-align:left">获取 Python 版本</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkContext.master</code></td>
<td style="text-align:left">要连接的 Master URL</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkContext.sparkHome</code></td>
<td style="text-align:left">Spark 在工作节点的安装路径</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkContext.sparkUser()</code></td>
<td style="text-align:left">获取 SparkContext 的 Spark 用户名</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkContext.appName</code></td>
<td style="text-align:left">返回应用名称</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkContext.applicationId</code></td>
<td style="text-align:left">获取应用程序ID</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkContext.defaultParallelism</code></td>
<td style="text-align:left">返回默认并行级别</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkContext.defaultMinPartitions</code></td>
<td style="text-align:left">RDD默认最小分区数</td>
</tr>
<tr>
<td style="text-align:left"><code>sparkContext.setLogLevel(logLevel)</code></td>
<td style="text-align:left">控制日志级别。有效级别：ALL、DEBUG、ERROR、FATAL、INFO、OFF、TRACE、WARN</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkContext.range()</code></td>
<td style="text-align:left">创建一个序列</td>
</tr>
<tr>
<td style="text-align:left"><code>SparkContext.stop()</code></td>
<td style="text-align:left">终止SparkContext</td>
</tr>
</tbody>
</table>
<p>每个JVM只能激活一个SparkContext。在创建新SparkContext之前，必须终止活动的SparkContext。</p>
<h2 id="pyspark-shell"><a class="markdownIt-Anchor" href="#pyspark-shell"></a> PySpark Shell</h2>
<p>若要在 Python 解释器中以交互方式运行 Spark，请使用<code>bin/pyspark</code>。例如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pyspark  --master <span class="built_in">local</span>[2]</span><br></pre></td></tr></table></figure>
<p>或者，还要将<code>myTools.py</code> 添加到搜索路径中（以便以后能够<code>import myTools</code>），请使用：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bin/pyspark --master <span class="built_in">local</span>[2] --py-files myTools.py</span><br></pre></td></tr></table></figure>
<p>有关选项的完整列表，请运行<code>pyspark --help</code>。</p>
<p>PySpark会选择PATH中默认的python版本启动，可以通过指定<code>PYSPARK_PYTHON</code>选择Python版本，例如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">PYSPARK_PYTHON=python3.8 bin/pyspark</span><br></pre></td></tr></table></figure>
<p>在运行<code>bin/pyspark</code>时，还可以通过设置<code>PYSPARK_DRIVER_PYTHON</code>变量来选择驱动程序。</p>
<p>若要使用IPython：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">PYSPARK_DRIVER_PYTHON=ipython ./bin/pyspark</span><br></pre></td></tr></table></figure>
<p>若要使用 Jupyter Notebook：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=notebook ./bin/pyspark</span><br></pre></td></tr></table></figure>
<p>可以通过设置<code>PYSPARK_DRIVER_PYTHON_OPTS</code>来自定义<code>ipython</code>或<code>jupyter</code>命令。</p>
<p>在使用 <code>bin/pyspark</code>命令打开Spark交互式环境后，默认情况下，Spark 已经创建了名为 sc 的  SparkContext 变量，因此创建新的环境变量将不起作用。</p>
<p>但是，在提交的独立spark 应用程序中或者常规的python环境，需要自行创建SparkContext 对象连接集群。</p>
<h2 id="独立应用程序"><a class="markdownIt-Anchor" href="#独立应用程序"></a> 独立应用程序</h2>
<p>用户编写完Spark应用程序之后，需要使用 spark-submit 命令将应用程序提交到集群中运行：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">spark-submit --master <span class="string">&quot;local[2]&quot;</span> examples/src/main/python/pi.py</span><br></pre></td></tr></table></figure>
<p>但是，提交的Python独立程序内必须自行创建SparkContext 对象连接集群，例如</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setMaster(<span class="string">&#x27;local&#x27;</span>).setAppName(<span class="string">&#x27;myApp&#x27;</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br></pre></td></tr></table></figure>
<h2 id="spark运行基本流程"><a class="markdownIt-Anchor" href="#spark运行基本流程"></a> Spark运行基本流程</h2>
<p>Spark应用程序的执行流程如下：</p>
<ol>
<li>创建SparkContext对象，然后SparkContext会向Clutser Manager（例如Yarn、Standalone、Mesos等）申请资源</li>
<li>资源管理器在worker node上创建executor并分配资源（CPU、内存等)</li>
<li>SparkContext启动DAGScheduler，将提交的作业（Job）转换成若干Stage，各Stage构成DAG（Directed Acyclic Graph有向无环图），各个Stage包含若干相关 task，这些task的集合被称为TaskSet</li>
<li>TaskSet发送给TaskSet Scheduler，TaskSet Scheduler将Task发送给对应的Executor，同时SparkContext将应用程序代码发送到Executor，从而启动任务的执行</li>
<li>Executor执行Task，完成后释放相应的资源。<br />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/spark-submit.png" alt="" /></li>
</ol>
<ul>
<li>Job：一个Job包含多个RDD及作用于相应RDD上的各种操作构成的DAG图。</li>
<li>Stages：是Job的基本调度单位(DAGScheduler)，一个Job会分解为多组Stage，每组Stage包含多组任务(Task)，称为TaskSet，代表一组关联的，相互之间没有Shuffle依赖关系(最耗费资源)的任务组成的任务集。</li>
<li>Tasks：负责Stage的任务分发(TaskScheduler)，Task分发遵循基本原则：计算向数据靠拢，避免不必要的磁盘I/O开销。</li>
</ul>
<h1 id="rdd"><a class="markdownIt-Anchor" href="#rdd"></a> RDD</h1>
<p>弹性分布式数据集(RDD, Resilient Distributed Dataset)是Spark框架中的核心概念，它们是在多个节点上运行和操作以在集群上进行并行处理的元素。</p>
<p>请注意，在 Spark 2.0 之前，Spark 的主要编程接口是弹性分布式数据集 （RDD）。在 Spark 2.0 之后，RDD 被 Dataset 取代，后者具有与 RDD 类似的强类型，但在后台进行了更丰富的优化。</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">RDD Programming Guide</a></p>
<h2 id="创建rdd"><a class="markdownIt-Anchor" href="#创建rdd"></a> 创建RDD</h2>
<p>通过调用<code>SparkContext.parallelize</code>方法来创建一个可以并行操作的分布式数据集。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Parallelized Collections</span></span><br><span class="line">distData = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">pairRDD = sc.parallelize([(<span class="string">&#x27;a&#x27;</span>, <span class="number">7</span>),(<span class="string">&#x27;a&#x27;</span>, <span class="number">2</span>),(<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>)]) </span><br><span class="line">emptyRDD = sc.emptyRDD()</span><br></pre></td></tr></table></figure>
<p>创建后，分布式数据集可以并行操作。例如，我们可以调用<code>distData.reduce(lambda a, b: a + b)</code>来添加列表的元素。</p>
<p>并行集合的一个重要参数是<strong>分区</strong>，Spark将为集群的每个分区运行一个Task。通常，Spark会尝试根据您的集群自动设置分区数量，一般每个CPU 2-4个分区。但是，您也可以通过<code>parallelize</code>的第二个参数进行手动设置分区数量。</p>
<h2 id="外部数据源"><a class="markdownIt-Anchor" href="#外部数据源"></a> 外部数据源</h2>
<p>PySpark可以从Hadoop支持的任何存储源创建分布式数据集，包括本地文件系统、HDFS、Cassandra、HBase、Amazon S3等。</p>
<table>
<thead>
<tr>
<th>pyspark</th>
<th style="text-align:left"></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>SparkContext.textFile(path, minPartitions)</code></td>
<td style="text-align:left">读取文本文件</td>
</tr>
<tr>
<td><code>RDD.saveAsTextFile(path)</code></td>
<td style="text-align:left">保存为文本文件</td>
</tr>
<tr>
<td><code>SparkContext.wholeTextFiles(path, minPartitions)</code></td>
<td style="text-align:left">读取文本文件目录</td>
</tr>
<tr>
<td><code>SparkContext.sequenceFile(path, keyClass, valueClass)</code></td>
<td style="text-align:left">读取具有 key-value 的Hadoop SequenceFile</td>
</tr>
<tr>
<td><code>RDD.saveAsSequenceFile(path)</code></td>
<td style="text-align:left">Python RDD[(K,V)] 输出到Hadoop文件系统</td>
</tr>
<tr>
<td><code>SparkContext.pickleFile(path, minPartitions)</code></td>
<td style="text-align:left">加载之前使用RDD.saveAsPickleFile()方法保存的RDD</td>
</tr>
<tr>
<td><code>RDD.saveAsPickleFile(path, batchSize)</code></td>
<td style="text-align:left">保存为序列文件</td>
</tr>
<tr>
<td><code>SparkContext.hadoopFile(path, inputFormatClass, keyClass, valueClass）</code></td>
<td style="text-align:left">读取具有任意key-value类的 ‘old’ Hadoop InputFormat</td>
</tr>
<tr>
<td><code>RDD.saveAsHadoopFile(path, outputFormatClass</code>）</td>
<td style="text-align:left">使用 ‘old’ Hadoop OutputFormat API将RDD[(K, V)] 输出到Hadoop文件系统</td>
</tr>
<tr>
<td><code>SparkContext.newAPIHadoopFile(path, inputFormatClass, keyClass, valueClass)</code></td>
<td style="text-align:left">读取具有任意key-value类的 ‘new API’ Hadoop InputFormat</td>
</tr>
<tr>
<td><code>RDD.saveAsNewAPIHadoopFile(path, outputFormatClass)</code></td>
<td style="text-align:left">使用 ‘old’ Hadoop OutputFormat API将RDD[(K, V)] 输出到Hadoop文件系统</td>
</tr>
</tbody>
</table>
<p>以下是一个文本文件 I/O 示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># External Datasets</span></span><br><span class="line">rdd = sc.textFile(<span class="string">&quot;path/to/file&quot;</span>) <span class="comment"># Hadoop path</span></span><br><span class="line">rdd = sc.textFile(<span class="string">&quot;hdfs://path/to/file&quot;</span>) <span class="comment"># Hadoop path</span></span><br><span class="line">rdd = sc.textFile(<span class="string">&quot;file:///path/to/file&quot;</span>) <span class="comment"># local path</span></span><br><span class="line"></span><br><span class="line">rdd.saveAsTextFile(<span class="string">&quot;path/to/file&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>关于使用Spark读取文件的一些注意事项：</p>
<ul>
<li>如果在本地文件系统上使用路径，该文件也必须在worker node的相同路径上。要么将文件复制到所有worker，要么使用网络挂载的共享文件系统。</li>
<li>Spark的所有基于文件的读取方法，都支持使用目录、压缩文件和通配符路径。例如，您可以使用<code>SparkContext.textFile(&quot;/my/directory&quot;), SparkContext.textFile(&quot;/my/directory/*.txt&quot;)</code>和<code>SparkContext.textFile(&quot;/my/directory/*.gz&quot;)</code></li>
<li><code>textFile</code>和<code>pickleFile</code>方法还可使用第二个参数来控制文件的分区数。默认情况下，Spark为文件的每个块创建一个分区（在HDFS中默认为128MB），但您也可以通过传递更大的值来请求更多数量的分区。请注意，您的分区不能少于块。</li>
<li><code>SparkContext.wholeTextFiles</code>允许您读取包含多个小文本文件的目录，并将每个文件作为 <code>(filename, content)</code> 对返回。</li>
<li><code>RDD.saveAsPickleFile</code>和<code>SparkContext.pickleFile</code>支持以由pickled Python对象组成的简单格式保存RDD。batchSize用于pickle序列化，默认batchSize大小为10。</li>
</ul>
<h2 id="变换和行动"><a class="markdownIt-Anchor" href="#变换和行动"></a> 变换和行动</h2>
<p>在Spark里，对数据的所有操作，基本上就是围绕RDD来的。RDD支持两种类型的操作：变换（Transformation）和行动（Action）。</p>
<ul>
<li>Transformation是惰性的，因为它们不立即执行。RDD的转换操作会生成新的RDD，新的RDD的数据依赖于原来的RDD的数据，每个RDD又包含多个分区。那么一段程序实际上就构造了一个由相互依赖的多个RDD组成的有向无环图(DAG)。</li>
<li>Spark代码里面至少需要有一个Action算子。当我们的程序里面遇到一个Action算子的时候，代码才会将这个有向无环图作为一个Job提交给Spark真正执行，这种设计让Spark更加有效率地运行。</li>
</ul>
<p>Transformation 延迟执行会产生更多精细查询：DAGScheduler可以在查询中执行优化，包括能够避免shuffle数据。</p>
<p>例如，<code>map</code>是一种Transformation，它将函数传递给每个元素，并返回一个表示结果的新RDD。另一方面，<code>reduce</code>是一个Action，它使用函数聚合RDD的所有元素，并将最终结果返回。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lines = sc.textFile(<span class="string">&quot;data.txt&quot;</span>)</span><br><span class="line">lineLengths = lines.<span class="built_in">map</span>(<span class="keyword">lambda</span> s: <span class="built_in">len</span>(s))</span><br><span class="line">totalLength = lineLengths.reduce(<span class="keyword">lambda</span> a, b: a + b)</span><br></pre></td></tr></table></figure>
<p>默认情况下，每次对它运行Action时，每个变换后的RDD都可能会被重新计算。但是，可以使用<code>persist</code>或<code>cache</code>方法在内存中持久化RDD，这时，在首次计算后，Spark会将元素保留在集群上，以便下次查询时更快地访问。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lineLengths.persist()</span><br></pre></td></tr></table></figure>
<h3 id="rdd预览"><a class="markdownIt-Anchor" href="#rdd预览"></a> RDD预览</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Actions</th>
<th style="text-align:left">提取</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>RDD.collect()</code></td>
<td style="text-align:left">将RDD以列表形式返回</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.collectAsMap()</code></td>
<td style="text-align:left">将RDD以字典形式返回</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.take(n)</code></td>
<td style="text-align:left">提取前n个元素</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.takeSample(replace, n, seed)</code></td>
<td style="text-align:left">随机提取n个元素</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.takeOrdered(num, key)</code></td>
<td style="text-align:left">按指定顺序从RDD中获取num个元素</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.first()</code></td>
<td style="text-align:left">提取第1名</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.top(n)</code></td>
<td style="text-align:left">提取前n名</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.keys()</code></td>
<td style="text-align:left">返回RDD的keys</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.values()</code></td>
<td style="text-align:left">返回RDD的values</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.isEmpty()</code></td>
<td style="text-align:left">检查RDD是否为空</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.toDF(schema，sampleRatio)</code></td>
<td style="text-align:left">转化为 Spark DataFrame</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.collectAsMap()</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">2</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.keys().collect()</span><br><span class="line">[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.values().collect()</span><br><span class="line">[<span class="number">7</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<p>注意：在单机模式上，通常使用<code>RDD.foreach(println)</code>或<code>RDD.map(println)</code>打印RDD的所有元素。然而，在集群模式下，调用的stdout输出会出现在worker上，而在本地不会显示。要在本地打印所有元素，可以首先使用 <code>collect()</code>方法将RDD收集到本机，然后 <code>RDD.collect().foreach(println)</code>。然而，这可能会导致本地内存不足，因为<code>collect()</code>将整个RDD获取到一台机器上。如果您只需要打印RDD的几个元素，更安全的方法是使用<code>take()</code>。</p>
<h3 id="map-reduce"><a class="markdownIt-Anchor" href="#map-reduce"></a> Map-Reduce</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Transformations</th>
<th style="text-align:left">Map</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>RDD.map(func)</code></td>
<td style="text-align:left">将函数应用于RDD中的每个元素并返回</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.mapValues(func)</code></td>
<td style="text-align:left">不改变key，只对value执行map</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.flatMap(func)</code></td>
<td style="text-align:left">先map后扁平化返回</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.flatMapValues(func)</code></td>
<td style="text-align:left">不改变key，只对value执行flatMap</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.foreach(func)</code></td>
<td style="text-align:left">用迭代的方法将函数应用于每个元素</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.keyBy(func)</code></td>
<td style="text-align:left">执行函数于每个元素创建key-value对RDD</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.reduceByKey(func)</code></td>
<td style="text-align:left">合并每个key的value</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Actions</th>
<th>Reduce</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>RDD.reduce(func)</code></td>
<td>合并RDD的元素返回</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x + <span class="number">1</span>).collect()</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd.reduce(<span class="keyword">lambda</span> x, y : x + y)</span><br><span class="line"><span class="number">15</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd.keyBy(<span class="keyword">lambda</span> x: x % <span class="number">2</span>).collect()</span><br><span class="line">[(<span class="number">1</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">3</span>), (<span class="number">0</span>, <span class="number">4</span>), (<span class="number">1</span>, <span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD = sc.parallelize([(<span class="string">&#x27;a&#x27;</span>, <span class="number">7</span>),(<span class="string">&#x27;a&#x27;</span>, <span class="number">2</span>),(<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>)]) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.mapValues(<span class="keyword">lambda</span> x: x + <span class="number">1</span>).collect()</span><br><span class="line">[(<span class="string">&#x27;a&#x27;</span>, <span class="number">8</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">3</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.reduceByKey(<span class="keyword">lambda</span> x, y: x + y).collect()</span><br><span class="line">[(<span class="string">&#x27;a&#x27;</span>, <span class="number">9</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>)]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>names = sc.parallelize([<span class="string">&#x27;Elon Musk&#x27;</span>, <span class="string">&#x27;Bill Gates&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>names.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27; &#x27;</span>)).collect()</span><br><span class="line">[(<span class="string">&#x27;Elon&#x27;</span>, <span class="string">&#x27;Musk&#x27;</span>), (<span class="string">&#x27;Bill&#x27;</span>, <span class="string">&#x27;Gates&#x27;</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>names.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27; &#x27;</span>)).collect()</span><br><span class="line">[<span class="string">&#x27;Elon&#x27;</span>, <span class="string">&#x27;Musk&#x27;</span>, <span class="string">&#x27;Bill&#x27;</span>, <span class="string">&#x27;Gates&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h3 id="分组和聚合"><a class="markdownIt-Anchor" href="#分组和聚合"></a> 分组和聚合</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Transformations</th>
<th style="text-align:left">分组</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>RDD.groupBy(func)</code></td>
<td style="text-align:left">将RDD元素通过函数变换分组为key-iterable集</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.groupByKey()</code></td>
<td style="text-align:left">将key-value元素集分组为key-iterable集</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.aggregateByKey(zeroValue, seqOp, combOp)</code></td>
<td style="text-align:left">聚合每个键的值，使用给定的组合函数和中性“零值”。</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Actions</th>
<th>聚合</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>RDD.aggregate(zeroValue, seqOp, combOp)</code></td>
<td>聚合每个分区的元素，然后使用给定的组合函数和中性“零值”来汇总所有分区的结果。</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd.groupBy(<span class="keyword">lambda</span> x: x % <span class="number">2</span>).mapValues(<span class="built_in">list</span>).collect()</span><br><span class="line">[(<span class="number">0</span>, [<span class="number">2</span>, <span class="number">4</span>]), (<span class="number">1</span>, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>])]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD = sc.parallelize([(<span class="string">&#x27;a&#x27;</span>, <span class="number">7</span>),(<span class="string">&#x27;a&#x27;</span>, <span class="number">2</span>),(<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>)]) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.groupByKey().mapValues(<span class="built_in">list</span>).collect()</span><br><span class="line">[(<span class="string">&#x27;a&#x27;</span>, [<span class="number">7</span>, <span class="number">2</span>]), (<span class="string">&#x27;b&#x27;</span>, [<span class="number">2</span>])]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>seqOp = (<span class="keyword">lambda</span> x, y: (x[<span class="number">0</span>] + y, x[<span class="number">1</span>] + <span class="number">1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>combOp = (<span class="keyword">lambda</span> x, y: (x[<span class="number">0</span>] + y[<span class="number">0</span>], x[<span class="number">1</span>] + y[<span class="number">1</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]).aggregate((<span class="number">0</span>, <span class="number">0</span>), seqOp, combOp)</span><br><span class="line">(<span class="number">10</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">sorted</span>(rdd.aggregateByKey((<span class="number">0</span>, <span class="number">0</span>), seqFunc, combFunc).collect())</span><br><span class="line">[(<span class="string">&#x27;a&#x27;</span>, (<span class="number">3</span>, <span class="number">2</span>)), (<span class="string">&#x27;b&#x27;</span>, (<span class="number">1</span>, <span class="number">1</span>))]</span><br></pre></td></tr></table></figure>
<h3 id="统计"><a class="markdownIt-Anchor" href="#统计"></a> 统计</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Actions</th>
<th style="text-align:left">统计</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>RDD.count()</code></td>
<td style="text-align:left">返回RDD中的元素数</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.countByKey()</code></td>
<td style="text-align:left">按key计算RDD元素数量</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.countByValue()</code></td>
<td style="text-align:left">按RDD元素计算数量</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.sum()</code></td>
<td style="text-align:left">求和</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.mean()</code></td>
<td style="text-align:left">平均值</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.max()</code></td>
<td style="text-align:left">最大值</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.min()</code></td>
<td style="text-align:left">最小值</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.stdev()</code></td>
<td style="text-align:left">标准差</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.variance()</code></td>
<td style="text-align:left">方差</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.histograme()</code></td>
<td style="text-align:left">分箱（Bin）生成直方图</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.stats()</code></td>
<td style="text-align:left">综合统计（计数、平均值、标准差、最大值和最小值）</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD = sc.parallelize([(<span class="string">&#x27;a&#x27;</span>, <span class="number">7</span>),(<span class="string">&#x27;a&#x27;</span>, <span class="number">2</span>),(<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>)]) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.count()</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.countByKey()</span><br><span class="line">defaultdict(&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">int</span>&#x27;&gt;, &#123;&#x27;<span class="title">a</span>&#x27;:</span> <span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">1</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pairRDD.countByValue()</span><br><span class="line">defaultdict(&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">int</span>&#x27;&gt;, &#123;(<span class="params"><span class="string">&#x27;a&#x27;</span>, <span class="number">7</span></span>):</span> <span class="number">1</span>, (<span class="string">&#x27;a&#x27;</span>, <span class="number">2</span>): <span class="number">1</span>, (<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>): <span class="number">1</span>&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="筛选"><a class="markdownIt-Anchor" href="#筛选"></a> 筛选</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Transformations</th>
<th style="text-align:left">筛选</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>RDD.filter(func)</code></td>
<td style="text-align:left">筛选满足函数的元素(变换)</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x % <span class="number">2</span> == <span class="number">0</span>).collect()</span><br><span class="line">[<span class="number">2</span>, <span class="number">4</span>]</span><br></pre></td></tr></table></figure>
<h3 id="去重"><a class="markdownIt-Anchor" href="#去重"></a> 去重</h3>
<table>
<thead>
<tr>
<th>Transformations</th>
<th>去重</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>RDD.distinct(numPartitions)</code></td>
<td>回一个包含此RDD中不同元素的新RDD。</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">sorted</span>(sc.parallelize([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]).distinct().collect())</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<h3 id="排序"><a class="markdownIt-Anchor" href="#排序"></a> 排序</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Transformations</th>
<th style="text-align:left">排序</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>RDD.sortBy(keyfunc, ascending=True)</code></td>
<td style="text-align:left">按RDD元素变换后的值排序</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.sortByKey(ascending=True, keyfunc)</code></td>
<td style="text-align:left">按key排序</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = [(<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;1&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;d&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;2&#x27;</span>, <span class="number">5</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize(data).sortBy(<span class="keyword">lambda</span> x: x[<span class="number">0</span>]).collect()</span><br><span class="line">[(<span class="string">&#x27;1&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;2&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;d&#x27;</span>, <span class="number">4</span>)]</span><br></pre></td></tr></table></figure>
<h3 id="连接运算"><a class="markdownIt-Anchor" href="#连接运算"></a> 连接运算</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Transformations</th>
<th style="text-align:left">连接运算</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>RDD.join(other)</code></td>
<td style="text-align:left">内连接</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.cogroup(other)</code></td>
<td style="text-align:left">groupWith</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.leftOuterJoin(other)</code></td>
<td style="text-align:left">左连接</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.rightOuterJoin(other)</code></td>
<td style="text-align:left">右连接</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.fullOuterJoin(other)</code></td>
<td style="text-align:left">全连接</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1 = sc.parallelize([(<span class="string">&#x27;a&#x27;</span>, <span class="number">7</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd2 = sc.parallelize([(<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;B&#x27;</span>),(<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;C&#x27;</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.join(rdd2).collect()</span><br><span class="line">[(<span class="string">&#x27;b&#x27;</span>, (<span class="number">2</span>, <span class="string">&#x27;B&#x27;</span>))]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.fullOuterJoin(rdd2).collect()</span><br><span class="line">[(<span class="string">&#x27;b&#x27;</span>, (<span class="number">2</span>, <span class="string">&#x27;B&#x27;</span>)), (<span class="string">&#x27;c&#x27;</span>, (<span class="literal">None</span>, <span class="string">&#x27;C&#x27;</span>)), (<span class="string">&#x27;a&#x27;</span>, (<span class="number">7</span>, <span class="literal">None</span>)), (<span class="string">&#x27;a&#x27;</span>, (<span class="number">2</span>, <span class="literal">None</span>))]</span><br></pre></td></tr></table></figure>
<h3 id="集合运算"><a class="markdownIt-Anchor" href="#集合运算"></a> 集合运算</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Transformations</th>
<th style="text-align:left">集合运算</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>RDD.union(other)</code></td>
<td style="text-align:left">并集(不去重)</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.intersection(other)</code></td>
<td style="text-align:left">交集(去重)</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.subtract(other)</code></td>
<td style="text-align:left">差集(不去重)</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.subtractByKey(other)</code></td>
<td style="text-align:left">差集 by key</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.cartesian(other)</code></td>
<td style="text-align:left">笛卡尔积</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.subtractByKey(other)</code></td>
<td style="text-align:left">按key差集</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1 = sc.parallelize([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd2 = sc.parallelize([<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.union(rdd1).collect()</span><br><span class="line">[<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.intersection(rdd2).collect()</span><br><span class="line">[<span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1.subtract(rdd2).collect()</span><br><span class="line">[<span class="number">5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd2.cartesian(rdd2).collect()</span><br><span class="line">[(<span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">3</span>), (<span class="number">3</span>, <span class="number">1</span>), (<span class="number">3</span>, <span class="number">3</span>)]</span><br></pre></td></tr></table></figure>
<h3 id="抽样"><a class="markdownIt-Anchor" href="#抽样"></a> 抽样</h3>
<table>
<thead>
<tr>
<th>Transformations</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>RDD.sample(replace, frac, seed)</code></td>
<td>随机采样</td>
</tr>
<tr>
<td><code>RDD.sampleByKey(replace, frac, seed)</code></td>
<td>分层采样</td>
</tr>
<tr>
<td><code>RDD.takeSample(replace, n, seed)</code></td>
<td>随机提取n个元素</td>
</tr>
<tr>
<td><code>RDD.randomSplit(weight, seed)</code></td>
<td>随机拆分</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd.sample(<span class="literal">False</span>, <span class="number">0.8</span>, seed=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>
<h3 id="缓存"><a class="markdownIt-Anchor" href="#缓存"></a> 缓存</h3>
<p>默认情况下，每次对RDD运行Action时，每个变换都可能会被重新计算。若RDD在未来的Action中重复利用时，我们可以使用<code>persist()</code>或<code>cache()</code>方法将其标记为持久。首次计算后，它将保存在节点的内存中。这使得未来的行动要快得多（通常超过10倍）。缓存是迭代算法和快速交互使用的关键工具。Spark的缓存是容错的，如果任何分区丢失，它将使用最初创建它的转换自动重新计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Persists the data in the disk by specifying the storage level.</span></span><br><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel</span><br><span class="line">rdd.persist(StorageLevel.DISK_ONLY)</span><br></pre></td></tr></table></figure>
<p>**注意：**在Python中，存储的对象将始终使用Pickle库进行序列化，因此您是否选择序列化级别并不重要。Python中的可用存储级别包括<code>MEMORY_ONLY</code>、<code>MEMORY_ONLY_2</code>、<code>MEMORY_AND_DISK</code>、<code>MEMORY_AND_DISK_2</code>、<code>DISK_ONLY</code>、<code>DISK_ONLY_2</code>和<code>DISK_ONLY_3</code>。</p>
<table>
<thead>
<tr>
<th style="text-align:left">持久化</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>RDD.persist()</code></td>
<td style="text-align:left">标记为持久化</td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.cache()</code></td>
<td style="text-align:left">等价于<code>rdd.persist(MEMORY_ONLY)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>RDD.unpersist()</code></td>
<td style="text-align:left">释放缓存</td>
</tr>
</tbody>
</table>
<h3 id="shuffle"><a class="markdownIt-Anchor" href="#shuffle"></a> Shuffle</h3>
<p>在前面讲的Spark编程模型当中，我们对RDD中的常用transformation与action 函数进行了讲解，我们提到RDD经过transformation操作后会生成新的RDD，前一个RDD与tranformation操作后的RDD构成了lineage关系，也即后一个RDD与前一个RDD存在一定的依赖关系，根据tranformation操作后RDD与父RDD中的分区对应关系，可以将依赖分为两种：</p>
<ul>
<li><strong>窄依赖</strong>(narrow dependency)：变换操作后的RDD仅依赖于父RDD的固定分区，则它们是窄依赖的。</li>
<li><strong>宽依赖</strong>(wide dependency)：变换后的RDD的分区与父RDD所有的分区都有依赖关系（即存在shuffle过程，需要大量的节点传送数据），此时它们就是宽依赖的。</li>
</ul>
<p>如下图所示：<br />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/spark-dependency.png" alt="" /><br />
图中的实线空心矩形代表一个RDD，实线空心矩形中的带阴影的小矩形表示分区(partition)。从上图中可以看到， map, filter, union等transformation是窄依赖；而groupByKey是宽依赖；join操作存在两种情况，如果分区仅仅依赖于父RDD的某一分区，则是窄依赖的，否则就是宽依赖。</p>
<p><strong>优化</strong>：fork/join</p>
<p>宽依赖需要进行shuffle过程，需要大量的节点传送数据，无法进行优化；而所有窄依赖则不需要进行I/O传输，可以优化执行。</p>
<p>当RDD触发相应的action操作后，DAGScheduler会根据程序中的transformation类型构造相应的DAG并生成相应的stage，所有窄依赖构成一个stage，而单个宽依赖会生成相应的stage。</p>
<p>Spark通过分析各个RDD的依赖关系生成有向无环图DAG(Directed Acyclic Graph)，通过分析各个RDD中的分区之间的依赖关系来决定如何划分Stage进行任务优化。</p>
<h1 id="共享变量"><a class="markdownIt-Anchor" href="#共享变量"></a> 共享变量</h1>
<p>默认情况下，当Spark在不同节点上作为一组任务并行运行函数时，它会将函数中使用的所有变量的复制到每台机器上。有时，变量需要在任务之间共享，或在任务和驱动程序之间共享，而集群机器上变量的更新不会传播回驱动程序。Spark支持两种类型的<strong>共享变量</strong>(Shared Variables)：广播变量和累加器。</p>
<h2 id="广播变量"><a class="markdownIt-Anchor" href="#广播变量"></a> 广播变量</h2>
<p>广播变量（Broadcast Variables）在集群上广播一个只读变量。创建广播变量后，在集群上运行的任何函数中都可以使用它，而不用多次发送副本。此外，对象在广播后不应进行修改，以确保所有节点获得广播变量的值是相同的。</p>
<table>
<thead>
<tr>
<th>PySpark</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>SparkContext.broadcast(value)</code></td>
<td>创建广播变量</td>
</tr>
<tr>
<td><code>Broadcast.value</code></td>
<td>返回广播变量的值</td>
</tr>
<tr>
<td><code>Broadcast.destroy()</code></td>
<td>永久释放广播变量使用的所有资源</td>
</tr>
<tr>
<td><code>Broadcast.unpersist()</code></td>
<td>释放广播变量复制到执行器的资源</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>broadcastVar = sc.broadcast([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">&lt;pyspark.broadcast.Broadcast <span class="built_in">object</span> at <span class="number">0x102789f10</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>broadcastVar.value</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<h2 id="累加器"><a class="markdownIt-Anchor" href="#累加器"></a> 累加器</h2>
<p>Spark中的累加器（Accumulators）专门用于在集群中的工作节点之间安全更新变量。</p>
<table>
<thead>
<tr>
<th>Pyspark</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>SparkContext.accumulator(value)</code></td>
<td>创建一个初始值为value的累加器</td>
</tr>
<tr>
<td><code>Accumulator.add(term)</code></td>
<td>在此累加器的值中添加一项</td>
</tr>
<tr>
<td><code>Accumulator.value</code></td>
<td>获取累加器的值，仅在驱动程序中可用</td>
</tr>
</tbody>
</table>
<p>通过调用<code>SparkContext.accumulator(value)</code>从初始值<code>value</code>创建累加器。然后在集群上运行的任务可以使用<code>add</code>方法或<code>+=</code>运算符添加到集群中。然而，他们无法读取它的值。只有驱动程序程序可以使用其<code>value</code>方法读取累加器的值。</p>
<p>下面的代码显示了用于添加数组元素的累加器：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>accum = sc.accumulator(<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>accum</span><br><span class="line">Accumulator&lt;<span class="built_in">id</span>=<span class="number">0</span>, value=<span class="number">0</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]).foreach(<span class="keyword">lambda</span> x: accum.add(x))</span><br><span class="line">...</span><br><span class="line"><span class="number">10</span>/09/<span class="number">29</span> <span class="number">18</span>:<span class="number">41</span>:08 INFO SparkContext: Tasks finished <span class="keyword">in</span> <span class="number">0.317106</span> s</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>accum.value</span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>内置累加器只支持数字类型，但我们可以新建<a target="_blank" rel="noopener external nofollow noreferrer" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.AccumulatorParam.html#pyspark.AccumulatorParam">AccumulatorParam</a>子类来自定义数据类型。AccumulatorParam接口有两种方法：<code>zero</code>用于为您的数据类型提供“零值”，以及用于将两个值一起添加的<code>addInPlace</code>。例如，假设我们有一个表示数学向量的<code>Vector</code>类，我们可以写：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VectorAccumulatorParam</span>(<span class="params">AccumulatorParam</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">zero</span>(<span class="params">self, initialValue</span>):</span></span><br><span class="line">        <span class="keyword">return</span> Vector.zeros(initialValue.size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addInPlace</span>(<span class="params">self, v1, v2</span>):</span></span><br><span class="line">        v1 += v2</span><br><span class="line">        <span class="keyword">return</span> v1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Then, create an Accumulator of this type:</span></span><br><span class="line">vecAccum = sc.accumulator(Vector(...), VectorAccumulatorParam())</span><br></pre></td></tr></table></figure>
<p>对于仅在Actions中执行的累加器更新，Spark保证每个任务对累加器的更新将只应用一次，即重新启动的任务不会更新值。在Transformation中，如果重新执行任务或作业阶段，每个任务的更新可能会被应用超过一次。</p>
<p>累加器不会改变Spark的惰性，如果它们在RDD的Actions中更新，则仅在RDD作为Actions的一部分计算后，其值才会更新。因此，当在<code>map()</code>等惰性转换时，不能保证执行累加器更新。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">accum = sc.accumulator(<span class="number">0</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span>(<span class="params">x</span>):</span></span><br><span class="line">    accum.add(x)</span><br><span class="line">    <span class="keyword">return</span> f(x)</span><br><span class="line">data.<span class="built_in">map</span>(g)</span><br><span class="line"><span class="comment"># Here, accum is still 0 because no actions have caused the `map` to be computed.</span></span><br></pre></td></tr></table></figure>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><a class="post-meta__tags" href="/tags/spark/">spark</a><a class="post-meta__tags" href="/tags/rdd/">rdd</a></div><div class="post_share"><div class="social-share" data-image="/img/apache-spark-core.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/morty3.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/morty3.jpg" alt="Give me money!"/></a><div class="post-qr-code-desc">Give me money!</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/bb755aa3/" title="大数据手册(Spark)--Spark SQL and DataFrames"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-sql.png" onerror="onerror=null;src='/img/404_moon.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">大数据手册(Spark)--Spark SQL and DataFrames</div></div></a></div><div class="next-post pull-right"><a href="/posts/34eba6aa/" title="大数据手册(Spark)--PySpark Streaming"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-streaming.png" onerror="onerror=null;src='/img/404_moon.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">大数据手册(Spark)--PySpark Streaming</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/75974533/" title="大数据手册(Spark)--PySpark MLlib"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-mllib.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-01</div><div class="title">大数据手册(Spark)--PySpark MLlib</div></div></a></div><div><a href="/posts/34eba6aa/" title="大数据手册(Spark)--PySpark Streaming"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-streaming.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-13</div><div class="title">大数据手册(Spark)--PySpark Streaming</div></div></a></div><div><a href="/posts/bb755aa3/" title="大数据手册(Spark)--Spark SQL and DataFrames"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-sql.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-03</div><div class="title">大数据手册(Spark)--Spark SQL and DataFrames</div></div></a></div><div><a href="/posts/d099726d/" title="PySpark 特征工程(III)--特征选择"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/spark-install.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-03</div><div class="title">PySpark 特征工程(III)--特征选择</div></div></a></div><div><a href="/posts/a1358f89/" title="PySpark 特征工程(II)--特征构造"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/spark-install.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-03</div><div class="title">PySpark 特征工程(II)--特征构造</div></div></a></div><div><a href="/posts/59da38ae/" title="PySpark 特征工程(I)--数据预处理"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/spark-install.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-28</div><div class="title">PySpark 特征工程(I)--数据预处理</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Tiny Lei</div><div class="author-info__description">每天进步一点点...</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">179</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">109</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">47</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://gitee.com/wilenwu" rel="external nofollow noreferrer" target="_blank" title="Gitee"><i class="iconfont icon-gitee"></i></a><a class="social-icon" href="https://github.com/wilenwu" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="iconfont icon-github"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_41518277" rel="external nofollow noreferrer" target="_blank" title="CSDN"><i class="iconfont icon-csdn"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="iconfont icon-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">感谢访问本站，若喜欢请收藏^_^</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8-spark"><span class="toc-number">1.</span> <span class="toc-text"> 启动 Spark</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#sparkcontext"><span class="toc-number">1.1.</span> <span class="toc-text"> SparkContext</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pyspark-shell"><span class="toc-number">1.2.</span> <span class="toc-text"> PySpark Shell</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8B%AC%E7%AB%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F"><span class="toc-number">1.3.</span> <span class="toc-text"> 独立应用程序</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark%E8%BF%90%E8%A1%8C%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B"><span class="toc-number">1.4.</span> <span class="toc-text"> Spark运行基本流程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#rdd"><span class="toc-number">2.</span> <span class="toc-text"> RDD</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BArdd"><span class="toc-number">2.1.</span> <span class="toc-text"> 创建RDD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="toc-number">2.2.</span> <span class="toc-text"> 外部数据源</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%98%E6%8D%A2%E5%92%8C%E8%A1%8C%E5%8A%A8"><span class="toc-number">2.3.</span> <span class="toc-text"> 变换和行动</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#rdd%E9%A2%84%E8%A7%88"><span class="toc-number">2.3.1.</span> <span class="toc-text"> RDD预览</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#map-reduce"><span class="toc-number">2.3.2.</span> <span class="toc-text"> Map-Reduce</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%BB%84%E5%92%8C%E8%81%9A%E5%90%88"><span class="toc-number">2.3.3.</span> <span class="toc-text"> 分组和聚合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1"><span class="toc-number">2.3.4.</span> <span class="toc-text"> 统计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%9B%E9%80%89"><span class="toc-number">2.3.5.</span> <span class="toc-text"> 筛选</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%BB%E9%87%8D"><span class="toc-number">2.3.6.</span> <span class="toc-text"> 去重</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%92%E5%BA%8F"><span class="toc-number">2.3.7.</span> <span class="toc-text"> 排序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9E%E6%8E%A5%E8%BF%90%E7%AE%97"><span class="toc-number">2.3.8.</span> <span class="toc-text"> 连接运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9B%86%E5%90%88%E8%BF%90%E7%AE%97"><span class="toc-number">2.3.9.</span> <span class="toc-text"> 集合运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%BD%E6%A0%B7"><span class="toc-number">2.3.10.</span> <span class="toc-text"> 抽样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%93%E5%AD%98"><span class="toc-number">2.3.11.</span> <span class="toc-text"> 缓存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#shuffle"><span class="toc-number">2.3.12.</span> <span class="toc-text"> Shuffle</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F"><span class="toc-number">3.</span> <span class="toc-text"> 共享变量</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="toc-number">3.1.</span> <span class="toc-text"> 广播变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="toc-number">3.2.</span> <span class="toc-text"> 累加器</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/619a34fc/" title="Python(Scientific Computing)--Cython"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cython-cover.jpg" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="Python(Scientific Computing)--Cython"/></a><div class="content"><a class="title" href="/posts/619a34fc/" title="Python(Scientific Computing)--Cython">Python(Scientific Computing)--Cython</a><time datetime="2025-03-04T16:32:31.000Z" title="发表于 2025-03-05 00:32:31">2025-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/e88bb280/" title="C++ 标准库"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cpp-introduction.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="C++ 标准库"/></a><div class="content"><a class="title" href="/posts/e88bb280/" title="C++ 标准库">C++ 标准库</a><time datetime="2025-03-04T04:04:01.000Z" title="发表于 2025-03-04 12:04:01">2025-03-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3f4b6fbd/" title="C++ 快速入门"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cpp-introduction.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="C++ 快速入门"/></a><div class="content"><a class="title" href="/posts/3f4b6fbd/" title="C++ 快速入门">C++ 快速入门</a><time datetime="2025-03-03T14:16:01.000Z" title="发表于 2025-03-03 22:16:01">2025-03-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/dc8936d5/" title="Python(Machine Learning)--CatBoost"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/catboost.svg" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="Python(Machine Learning)--CatBoost"/></a><div class="content"><a class="title" href="/posts/dc8936d5/" title="Python(Machine Learning)--CatBoost">Python(Machine Learning)--CatBoost</a><time datetime="2025-02-12T16:23:00.000Z" title="发表于 2025-02-13 00:23:00">2025-02-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/db6e5578/" title="Java简单使用"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/java_cover.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="Java简单使用"/></a><div class="content"><a class="title" href="/posts/db6e5578/" title="Java简单使用">Java简单使用</a><time datetime="2024-09-17T14:55:05.000Z" title="发表于 2024-09-17 22:55:05">2024-09-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Tiny Lei</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'forest'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addModeChange('mermaid', runMermaid)

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>function getGiscusTheme (theme) {
  return theme === 'dark' ? 'dark' : 'light'
}

function loadGiscus () {
  const config = Object.assign({
    src: 'https://giscus.app/client.js',
    'data-repo': 'WilenWu/giscus-comments',
    'data-repo-id': 'R_kgDONXyMwg',
    'data-category-id': 'DIC_kwDONXyMws4Ckzx5',
    'data-mapping': 'pathname',
    'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
    'data-reactions-enabled': '1',
    crossorigin: 'anonymous',
    async: true
  },null)

  let ele = document.createElement('script')
  for (let key in config) {
    ele.setAttribute(key, config[key])
  }
  document.getElementById('giscus-wrap').insertAdjacentElement('afterbegin',ele)
}

function changeGiscusTheme (theme) {
  function sendMessage(message) {
    const iframe = document.querySelector('iframe.giscus-frame')
    if (!iframe) return
    iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app')
  }

  sendMessage({
    setConfig: {
      theme: getGiscusTheme(theme)
    }
  });
}

btf.addModeChange('giscus', changeGiscusTheme)

if ('Giscus' === 'Giscus' || !false) {
  if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
  else loadGiscus()
} else {
  function loadOtherComment () {
    loadGiscus()
  }
}</script></div><script src="/js/custom.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>