<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Python(Machine Learning)--超参数优化 | 雷小小</title><meta name="author" content="Tiny Lei"><meta name="copyright" content="Tiny Lei"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="超参数优化 超参数是用于控制学习过程的不同参数值，对机器学习模型的性能有显著影响。例如，随机森林算法中的估计器数量、最大深度和分裂标准等。超参数优化是找到超参数值的正确组合，以便在合理的时间内实现数据最大性能的过程。这个过程在机器学习算法的预测准确性中起着至关重要的作用。因此，超参数优化被认为是构建机器学习模型中最棘手的部分。 目前来说sklearn支持两种类型的超参数优化：  GridSear">
<meta property="og:type" content="article">
<meta property="og:title" content="Python(Machine Learning)--超参数优化">
<meta property="og:url" content="https://wilenwu.gitee.io/posts/794d8498/index.html">
<meta property="og:site_name" content="雷小小">
<meta property="og:description" content="超参数优化 超参数是用于控制学习过程的不同参数值，对机器学习模型的性能有显著影响。例如，随机森林算法中的估计器数量、最大深度和分裂标准等。超参数优化是找到超参数值的正确组合，以便在合理的时间内实现数据最大性能的过程。这个过程在机器学习算法的预测准确性中起着至关重要的作用。因此，超参数优化被认为是构建机器学习模型中最棘手的部分。 目前来说sklearn支持两种类型的超参数优化：  GridSear">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wilenwu.gitee.io/img/Hyperparameter-Optimization.png">
<meta property="article:published_time" content="2024-02-15T03:28:52.000Z">
<meta property="article:modified_time" content="2024-04-18T13:15:45.046Z">
<meta property="article:author" content="Tiny Lei">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wilenwu.gitee.io/img/Hyperparameter-Optimization.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://wilenwu.gitee.io/posts/794d8498/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="baidu-site-verification" content="code-7rymn5Bitx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?654e7415ab55bed7c9c2bc6d665f03c5";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Python(Machine Learning)--超参数优化',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-18 21:15:45'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_2849223_xh1ftc8qym.css"><link rel="stylesheet" href="/css/link-card.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="雷小小" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">175</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">108</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">43</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/user-guide/"><i class="fa-fw fa fa-compass"></i><span> 用户指南</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa fa-book"></i><span> 文档</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-images"></i><span> 图库</span></a></div><div class="menus_item"><a class="site-page" href="/ebook/"><i class="fa-fw fa fa-book-reader"></i><span> 电子书</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-star"></i><span> 收藏夹</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa-solid fa-circle-chevron-down"></i><span> 更多</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fa fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/analytics/"><i class="fa-fw fa fa-line-chart"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/log/"><i class="fa-fw fa fa-history"></i><span> 更新日志</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/optuna-logo.png')"><nav id="nav"><span id="blog-info"><a href="/" title="雷小小"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/favicon.png"/><span class="site-name">雷小小</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/user-guide/"><i class="fa-fw fa fa-compass"></i><span> 用户指南</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa fa-book"></i><span> 文档</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-images"></i><span> 图库</span></a></div><div class="menus_item"><a class="site-page" href="/ebook/"><i class="fa-fw fa fa-book-reader"></i><span> 电子书</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-star"></i><span> 收藏夹</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa-solid fa-circle-chevron-down"></i><span> 更多</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fa fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/analytics/"><i class="fa-fw fa fa-line-chart"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/log/"><i class="fa-fw fa fa-history"></i><span> 更新日志</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Python(Machine Learning)--超参数优化<a class="post-edit-link" href="https://gitee.com/WilenWu/myblog/edit/master/source/_posts/python/Python(Machine-Learning)--Hyperparameter-Optimization.md" rel="external nofollow noreferrer" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-15T03:28:52.000Z" title="发表于 2024-02-15 11:28:52">2024-02-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-18T13:15:45.046Z" title="更新于 2024-04-18 21:15:45">2024-04-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/python/">Python</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/python/machine-learning/">Machine Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Python(Machine Learning)--超参数优化"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="超参数优化"><a class="markdownIt-Anchor" href="#超参数优化"></a> 超参数优化</h1>
<p>超参数是用于控制学习过程的不同参数值，对机器学习模型的性能有显著影响。例如，随机森林算法中的估计器数量、最大深度和分裂标准等。超参数优化是找到超参数值的正确组合，以便在合理的时间内实现数据最大性能的过程。这个过程在机器学习算法的预测准确性中起着至关重要的作用。因此，超参数优化被认为是构建机器学习模型中最棘手的部分。</p>
<p>目前来说sklearn支持两种类型的超参数优化：</p>
<ul>
<li>GridSearchCV 网格搜索是一种广泛使用的传统方法，详尽地考虑了所有参数组合</li>
<li>RandomizedSearchCV 随机搜索可以从具有指定分布的参数空间中抽样给定数量的候选者</li>
</ul>
<p>贝叶斯优化方法 (Bayesian Optimization)是当前超参数优化领域的SOTA手段（State of the Art），可以被认为是当前最为先进的优化框架。</p>
<p>贝叶斯优化的工作原理是：首先对目标函数的全局行为建立先验知识（通常用高斯过程来表示），然后通过观察目标函数在不同输入点的输出，更新这个先验知识，形成后验分布。基于后验分布，选择下一个采样点，这个选择既要考虑到之前观察到的最优值（即利用），又要考虑到全局尚未探索的区域（即探索）。这个选择的策略通常由所谓的采集函数（Acquisition Function）来定义，比如最常用的期望提升（Expected Improvement），这样，贝叶斯优化不仅可以有效地搜索超参数空间，还能根据已有的知识来引导搜索，避免了大量的无用尝试。</p>
<p>具体的算法细节可以参考：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/643095927?utm_id=0">https://zhuanlan.zhihu.com/p/643095927?utm_id=0</a></p>
<p>本文介绍一些实用的超参数优化技术：</p>
<ol>
<li>Hyperopt</li>
<li>Scikit Optimize</li>
<li>Optuna</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># read the dataset</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X, y = load_boston(return_X_y=<span class="literal">True</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>
<h1 id="hyperopt"><a class="markdownIt-Anchor" href="#hyperopt"></a> Hyperopt</h1>
<p>Hyperopt优化器是目前最通用的贝叶斯优化器之一，它集成了包括随机搜索、模拟退火和TPE（Tree-structured Parzen Estimator Approach）等多种优化算法。</p>
<p>官方文档：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://hyperopt.github.io/hyperopt/">https://hyperopt.github.io/hyperopt/</a></p>
<p>安装库</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pip install hyperopt</span><br></pre></td></tr></table></figure>
<p>Hyperopt 优化过程主要分为4步：</p>
<p>Step 1 定义参数空间<br />
Step 2 定义目标函数<br />
Step 3 执行优化<br />
Step 4 评估输出</p>
<h2 id="step-1-定义参数空间"><a class="markdownIt-Anchor" href="#step-1-定义参数空间"></a> Step 1 定义参数空间</h2>
<p>我们使用<code>dict()</code>来定义超参数空间，其中key可以任意设置，value则需用hyperopt的hp函数：</p>
<table>
<thead>
<tr>
<th style="text-align:left">hyperopt.hp</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">hp.choice(label, options)</td>
<td style="text-align:left">用于分类参数，返回options 中的元素</td>
</tr>
<tr>
<td style="text-align:left">hp.pchoice(label, p_list)</td>
<td style="text-align:left">返回 (probability, option) 元素对</td>
</tr>
<tr>
<td style="text-align:left">hp.randint(label, low, high)</td>
<td style="text-align:left">返回区间 [low, upper) 内的随机整数</td>
</tr>
<tr>
<td style="text-align:left">hp.uniform(label, low, high)</td>
<td style="text-align:left">均匀返回 low, high 之间的浮点数</td>
</tr>
<tr>
<td style="text-align:left">hp.quniform(label, low, high, q)</td>
<td style="text-align:left">均匀返回 low, high 之间的浮点数，适用于离散值</td>
</tr>
<tr>
<td style="text-align:left">hp.uniformint(label, low, high)</td>
<td style="text-align:left">均匀返回 low, high 之间均的整数，适用于离散值</td>
</tr>
<tr>
<td style="text-align:left">hp.loguniform(label, low, high)</td>
<td style="text-align:left">对数均匀返回 e<sup>low</sup>,e<sup>high</sup> 之间浮点数</td>
</tr>
<tr>
<td style="text-align:left">hp.qloguniform(label, low, high, q)</td>
<td style="text-align:left">对数均匀返回	e<sup>low</sup>, e<sup>high</sup> 之间浮点数，适用于离散值</td>
</tr>
<tr>
<td style="text-align:left">hp.normal(label, mu, sigma)</td>
<td style="text-align:left">正态分布返回实数</td>
</tr>
<tr>
<td style="text-align:left">hp.qnormal(label, mu, sigma, q)</td>
<td style="text-align:left">正态分布返回实数，适用于离散值</td>
</tr>
<tr>
<td style="text-align:left">hp.lognormal(label, mu, sigma)</td>
<td style="text-align:left">对数正态分布返回实数</td>
</tr>
<tr>
<td style="text-align:left">hp.qlognormal(label, mu, sigma, q)</td>
<td style="text-align:left">正态分布返回实数，适用于离散值</td>
</tr>
</tbody>
</table>
<blockquote>
<p>每个hp函数都有一个label作为第一个参数，这些label用于在优化过程中将参数传递给调用方。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define a search space</span></span><br><span class="line"><span class="keyword">from</span> hyperopt <span class="keyword">import</span> hp</span><br><span class="line">space = &#123;</span><br><span class="line">    <span class="string">&#x27;random_state&#x27;</span>: <span class="number">42</span>, </span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: hp.uniformint(<span class="string">&#x27;max_depth&#x27;</span>, <span class="number">2</span>, <span class="number">10</span>),</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: hp.loguniform(<span class="string">&#x27;learning_rate&#x27;</span>, <span class="number">0.001</span>, <span class="number">1.0</span>),</span><br><span class="line">    <span class="string">&#x27;n_estimators&#x27;</span>: hp.choice(<span class="string">&#x27;n_estimators&#x27;</span>, [<span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>, <span class="number">400</span>]),</span><br><span class="line">    <span class="string">&#x27;subsample&#x27;</span>: hp.quniform(<span class="string">&#x27;subsample&#x27;</span>, <span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.1</span>),</span><br><span class="line">    <span class="string">&#x27;max_features&#x27;</span>: hp.choice(<span class="string">&#x27;max_features&#x27;</span>, [<span class="string">&#x27;sqrt&#x27;</span>, <span class="string">&#x27;log2&#x27;</span>])</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="step-2-定义目标函数"><a class="markdownIt-Anchor" href="#step-2-定义目标函数"></a> Step 2 定义目标函数</h2>
<p>Hyperopt 目前只支持目标函数的最小化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define an objective function</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">objective</span>(<span class="params">params</span>):</span>    </span><br><span class="line">    reg = GradientBoostingRegressor(**params)</span><br><span class="line">    mse = cross_val_score(reg, X_train, y_train, scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>, cv=<span class="number">5</span>).mean()</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span>*mse</span><br></pre></td></tr></table></figure>
<h2 id="step-3-执行优化"><a class="markdownIt-Anchor" href="#step-3-执行优化"></a> Step 3 执行优化</h2>
<p>hyperopt 使用 fmin 函数进行优化。</p>
<p>fmin接收两种搜索算法：</p>
<ul>
<li>tpe.suggest 指代TPE (Tree Parzen Estimators) 方法</li>
<li>rand.suggest 指代随机网格搜索方法</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># minimize the objective over the space</span></span><br><span class="line"><span class="keyword">from</span> hyperopt <span class="keyword">import</span> fmin, tpe, space_eval, Trials </span><br><span class="line">trials = Trials() <span class="comment"># Initialize trials object</span></span><br><span class="line"></span><br><span class="line">best = fmin(</span><br><span class="line">    fn=objective, <span class="comment"># Objective Function to optimize</span></span><br><span class="line">    space=space, <span class="comment"># Hyperparameter&#x27;s Search Space</span></span><br><span class="line">    algo=tpe.suggest, <span class="comment"># Optimization algorithm</span></span><br><span class="line">    max_evals=<span class="number">100</span>, <span class="comment"># Number of optimization attempts</span></span><br><span class="line">    trials = trials,</span><br><span class="line">  	verbose=<span class="literal">True</span>,</span><br><span class="line">  	early_stop_fn=no_progress_loss(<span class="number">5</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">100%|██████| 1000/1000 [02:35&lt;00:00,  6.44trial/s, best loss: 8.932729710763638]</span><br></pre></td></tr></table></figure>
<p>其中 Trials 对象用于保存所有的超参数、损失和其他信息。</p>
<h2 id="step-4-评估输出"><a class="markdownIt-Anchor" href="#step-4-评估输出"></a> Step 4 评估输出</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(space_eval(space, best))</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;&#x27;learning_rate&#x27;: 0.2, &#x27;max_depth&#x27;: 5, &#x27;max_features&#x27;: &#x27;sqrt&#x27;, &#x27;n_estimators&#x27;: 54, &#x27;subsample&#x27;: 0.9&#125;</span><br></pre></td></tr></table></figure>
<h2 id="分布式优化"><a class="markdownIt-Anchor" href="#分布式优化"></a> 分布式优化</h2>
<p>超参数调优通常涉及训练数百或数千个模型，Hyperopt 允许分布式调优。通过 trials 参数将 SparkTrials 传递给 fmin 函数，在Spark集群上并行运行这些任务。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># We can run Hyperopt locally (only on the driver machine)</span></span><br><span class="line"><span class="comment"># by calling `fmin` without an explicit `trials` argument.</span></span><br><span class="line">best_hyperparameters = fmin(</span><br><span class="line">  fn=train,</span><br><span class="line">  space=search_space,</span><br><span class="line">  algo=algo,</span><br><span class="line">  max_evals=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We can distribute tuning across our Spark cluster</span></span><br><span class="line"><span class="comment"># by calling `fmin` with a `SparkTrials` instance.</span></span><br><span class="line"><span class="keyword">from</span> hyperopt <span class="keyword">import</span> SparkTrials</span><br><span class="line">spark_trials = SparkTrials()</span><br><span class="line">best_hyperparameters = fmin(</span><br><span class="line">  fn=train,</span><br><span class="line">  space=search_space,</span><br><span class="line">  algo=algo,</span><br><span class="line">  trials=spark_trials,</span><br><span class="line">  max_evals=<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<p>SparkTrials可以通过3个参数进行配置，所有这些参数都是可选的：</p>
<ul>
<li>parallelism 最大并行数，默认为 SparkContext.defaultParallelism。</li>
<li>timeout 允许的最大时间（以秒为单位），默认为None。</li>
<li>spark_session 如果没有给出，SparkTrials将寻找现有的SparkSession。</li>
</ul>
<h1 id="scikit-optimize"><a class="markdownIt-Anchor" href="#scikit-optimize"></a> Scikit-optimize</h1>
<p>Scikit-optimize 建立在 Scipy、Numpy 和 Scikit-Learn之上。非常易于使用，它提供了用于贝叶斯优化的通用工具包，可用于超参数调优。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/python/image-20240220234034290.png" style="zoom:10%;" />
<p>官方文档：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://scikit-optimize.github.io/stable/">https://scikit-optimize.github.io/stable/</a></p>
<p>安装库</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pip install scikit-optimize</span><br></pre></td></tr></table></figure>
<p>Scikit-optimize 优化过程主要分为4步：</p>
<p>Step 1 定义参数空间<br />
Step 2 定义目标函数<br />
Step 3 执行优化<br />
Step 4 评估输出</p>
<h2 id="step-1-定义参数空间-2"><a class="markdownIt-Anchor" href="#step-1-定义参数空间-2"></a> Step 1 定义参数空间</h2>
<p>使用 Scikit-optimize 提供的方法定义参数空间：</p>
<table>
<thead>
<tr>
<th style="text-align:left">skopt.space</th>
<th style="text-align:left">comment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">space.Real(low, high,  prior, name)</td>
<td style="text-align:left">用于浮点数参数</td>
</tr>
<tr>
<td style="text-align:left">space.Integer(low, high,  prior, name)</td>
<td style="text-align:left">用于整数参数</td>
</tr>
<tr>
<td style="text-align:left">space.Categorical(categories, prior, name)</td>
<td style="text-align:left">用于分类参数</td>
</tr>
</tbody>
</table>
<p>通过可选的prior参数可以对整型或浮点型取对数操作，或给类别型先验概率</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># define a search space</span></span><br><span class="line"><span class="keyword">import</span> skopt </span><br><span class="line">search_space= [</span><br><span class="line">    skopt.space.Integer(<span class="number">2</span>, <span class="number">10</span>, name=<span class="string">&#x27;max_depth&#x27;</span>),</span><br><span class="line">    skopt.space.Real(<span class="number">0.001</span>, <span class="number">1.0</span>, prior=<span class="string">&#x27;log-uniform&#x27;</span>, name=<span class="string">&#x27;learning_rate&#x27;</span>),</span><br><span class="line">    skopt.space.Integer(<span class="number">10</span>, <span class="number">100</span>, name=<span class="string">&#x27;n_estimators&#x27;</span>),</span><br><span class="line">    skopt.space.Real(<span class="number">0.2</span>, <span class="number">0.9</span>, name=<span class="string">&#x27;subsample&#x27;</span>),</span><br><span class="line">    skopt.space.Categorical([<span class="string">&#x27;sqrt&#x27;</span>, <span class="string">&#x27;log2&#x27;</span>], name=<span class="string">&#x27;max_features&#x27;</span>)</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h2 id="step-2-定义目标函数-2"><a class="markdownIt-Anchor" href="#step-2-定义目标函数-2"></a> Step 2 定义目标函数</h2>
<p>Scikit-optimize 支持目标函数最小化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> skopt.utils <span class="keyword">import</span> use_named_args</span><br><span class="line"></span><br><span class="line"><span class="comment"># define the function used to evaluate a given configuration</span></span><br><span class="line"><span class="meta">@use_named_args(<span class="params">space</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">objective</span>(<span class="params">**params</span>):</span></span><br><span class="line">    <span class="comment"># configure the model with specific hyperparameters</span></span><br><span class="line">    clf = GradientBoostingRegressor( random_state=<span class="number">42</span>, **params)</span><br><span class="line">    mse = cross_val_score(reg, X_train, y_train, scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>, cv=<span class="number">5</span>).mean()</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span>*mse</span><br></pre></td></tr></table></figure>
<p>一般使用交叉验证来避免过拟合。used_named_args装饰器允许目标函数将参数作为关键字参数接收。</p>
<h2 id="step-3-执行优化-2"><a class="markdownIt-Anchor" href="#step-3-执行优化-2"></a> Step 3 执行优化</h2>
<p>有四种优化算法可供选择：</p>
<table>
<thead>
<tr>
<th style="text-align:left">skopt.optimizer</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">dummy_minimize</td>
<td style="text-align:left">随机搜索</td>
</tr>
<tr>
<td style="text-align:left">forest_minimize</td>
<td style="text-align:left">使用决策树的贝叶斯优化</td>
</tr>
<tr>
<td style="text-align:left">gbrt_minimize</td>
<td style="text-align:left">使用GBRT的贝叶斯优化</td>
</tr>
<tr>
<td style="text-align:left">gp_minimize</td>
<td style="text-align:left">使用高斯过程的贝叶斯优化</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> skopt <span class="keyword">import</span> gp_minimize</span><br><span class="line"></span><br><span class="line"><span class="comment"># perform optimization</span></span><br><span class="line">result = gp_minimize(</span><br><span class="line">    func=objective,</span><br><span class="line">    dimensions=search_space,</span><br><span class="line">    n_calls=<span class="number">100</span>,</span><br><span class="line">    random_state=<span class="number">42</span>,</span><br><span class="line">    verbose=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="step-4-评估输出-2"><a class="markdownIt-Anchor" href="#step-4-评估输出-2"></a> Step 4 评估输出</h2>
<p>打印最佳得分和最佳参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># summarizing finding:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Best score: <span class="subst">&#123;result.fun&#125;</span>&#x27;</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Best parameters: <span class="subst">&#123;result.x&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>打印优化过程中的目标函数值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(result.func_vals)</span><br></pre></td></tr></table></figure>
<p>绘制收敛轨迹</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># plot convergence traces</span></span><br><span class="line"><span class="keyword">from</span> skopt.plots <span class="keyword">import</span> plot_convergence</span><br><span class="line">plot_convergence(result) </span><br></pre></td></tr></table></figure>
<h2 id="scikit-learn-api"><a class="markdownIt-Anchor" href="#scikit-learn-api"></a> Scikit-Learn API</h2>
<p>Scikit-optimize 提供了一个类似于 GridSearchCV 和 RandomizedSearchCV 的接口 BayesSearchCV，实现了 fit 和 score 方法，以及 predict, predict_proba, decision_function, transform and inverse_transform  等常用方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> skopt.searchcv <span class="keyword">import</span> BayesSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># define search space </span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&quot;max_depth&quot;</span>: (<span class="number">2</span>, <span class="number">10</span>), <span class="comment"># integer valued parameter</span></span><br><span class="line">    <span class="string">&quot;learning_rate&quot;</span>: (<span class="number">0.001</span>, <span class="number">1.0</span>, <span class="string">&#x27;log-uniform&#x27;</span>)</span><br><span class="line">    <span class="string">&quot;n_estimators&quot;</span>: [<span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>, <span class="number">400</span>],</span><br><span class="line">    <span class="string">&quot;subsample&quot;</span>: (<span class="number">0.2</span>, <span class="number">0.9</span>, <span class="string">&#x27;uniform&#x27;</span>)</span><br><span class="line">    <span class="string">&quot;max_features&quot;</span>: [<span class="string">&quot;sqrt&quot;</span>, <span class="string">&quot;log2&quot;</span>],  <span class="comment"># categorical parameter</span></span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="comment"># define the search</span></span><br><span class="line">optimizer = BayesSearchCV(</span><br><span class="line">    estimator=GradientBoostingRegressor(),</span><br><span class="line">    search_spaces=params,</span><br><span class="line">    cv=<span class="number">5</span>,</span><br><span class="line">    n_iter=<span class="number">100</span>,</span><br><span class="line">    scoring=<span class="string">&quot;accuracy&quot;</span>,</span><br><span class="line">    verbose=<span class="number">4</span>,</span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># executes bayesian optimization</span></span><br><span class="line">optimizer.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># report the best result</span></span><br><span class="line"><span class="built_in">print</span>(optimizer.best_score_)</span><br><span class="line"><span class="built_in">print</span>(optimizer.best_params_)</span><br></pre></td></tr></table></figure>
<h1 id="optuna"><a class="markdownIt-Anchor" href="#optuna"></a> Optuna</h1>
<p>Optuna是目前为止最成熟、拓展性最强的超参数优化框架，它是专门为机器学习和深度学习所设计。为了满足机器学习开发者的需求，Optuna拥有强大且固定的API，因此Optuna代码简单，编写高度模块化，</p>
<p>Optuna可以无缝衔接到PyTorch、Tensorflow等深度学习框架上，也可以与sklearn的优化库scikit-optimize结合使用，因此Optuna可以被用于各种各样的优化场景。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/python/optuna-logo.png" style="zoom: 33%;" />
<p>官方文档：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://optuna.org/">https://optuna.org/</a></p>
<p>安装库</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pip install optuna</span><br></pre></td></tr></table></figure>
<p>Optuna 优化过程主要分为3步：</p>
<p>Step 1 构建目标函数及参数空间<br />
Step 2 执行优化<br />
Step 3 评估输出</p>
<h2 id="step-1-构建目标函数及参数空间"><a class="markdownIt-Anchor" href="#step-1-构建目标函数及参数空间"></a> Step 1 构建目标函数及参数空间</h2>
<p>Optuna 基于 Trial 和 Study 两个组件实现优化（optimization）。在优化过程中，Optuna 反复调用目标函数，在不同的参数下对其进行求值。一个 Trial 对应着目标函数的单次执行。在每次调用目标函数的时候，它都被内部实例化一次。而 suggest API (例如 suggest_uniform()) 在目标函数内部调用，被用于获取单个 trial 的参数。</p>
<p>Optuna 允许在目标函数中定义参数空间和目标，优化器会通过trail所携带的方法来构造参数空间。</p>
<table>
<thead>
<tr>
<th>optuna.trial.Trial</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>trial.suggest_categorical(name, choices)</td>
<td>适用于分类参数</td>
</tr>
<tr>
<td>trial.suggest_int(name, low, high, step=1, log=False)</td>
<td>适用于整数参数</td>
</tr>
<tr>
<td>trial.suggest_float(name, low, high, *, step=None, log=False)</td>
<td>适用于浮点参数</td>
</tr>
<tr>
<td>trial.suggest_uniform(name, low, high)</td>
<td>均匀分布</td>
</tr>
<tr>
<td>trial.suggest_loguniform(name, low, high)</td>
<td>对数均匀分布</td>
</tr>
<tr>
<td>trial.suggest_discrete_uniform(name, low, high, q)</td>
<td>离散均匀分布</td>
</tr>
</tbody>
</table>
<p>通过可选的 step 与 log 参数，我们可以对整形或者浮点型参数进行离散化或者取对数操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># define the search space and the objecive function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">objective</span>(<span class="params">trial</span>):</span></span><br><span class="line">    <span class="comment"># Define the search space</span></span><br><span class="line">	params= &#123;</span><br><span class="line">	   <span class="string">&#x27;max_depth&#x27;</span>: trial.suggest_int(<span class="string">&#x27;max_depth&#x27;</span>,  <span class="number">2</span>, <span class="number">10</span>, <span class="number">1</span>),</span><br><span class="line"> 	   <span class="string">&#x27;learning_rate&#x27;</span>: trial.suggest_float(<span class="string">&#x27;learning_rate&#x27;</span>, <span class="number">0.001</span>, <span class="number">1.0</span>, log=<span class="literal">True</span>),</span><br><span class="line"> 	   <span class="string">&#x27;n_estimators&#x27;</span>: trial.suggest_int(<span class="string">&#x27;n_estimators&#x27;</span>, <span class="number">100</span>, <span class="number">400</span>, <span class="number">100</span>),</span><br><span class="line"> 	   <span class="string">&#x27;subsample&#x27;</span>: trial.suggest_float(<span class="string">&#x27;subsample&#x27;</span>, <span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.1</span>),</span><br><span class="line"> 	   <span class="string">&#x27;max_features&#x27;</span>: trial.suggest_categorical(<span class="string">&#x27;max_features&#x27;</span>, [<span class="string">&#x27;sqrt&#x27;</span>, <span class="string">&#x27;log2&#x27;</span>])</span><br><span class="line">	&#125;</span><br><span class="line">    reg = GradientBoostingRegressor(**params)</span><br><span class="line">    mse = cross_val_score(reg, X_train, y_train, scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>, cv=<span class="number">5</span>).mean()</span><br><span class="line">    <span class="keyword">return</span> mse</span><br></pre></td></tr></table></figure>
<p>通常使用交叉验证来避免过拟合。</p>
<h2 id="step-2-执行优化"><a class="markdownIt-Anchor" href="#step-2-执行优化"></a> Step 2 执行优化</h2>
<p>下面是几个常用术语：</p>
<ul>
<li>Trial: 目标函数的单次调用</li>
<li>Study: 一次优化过程，包含一系列的 trials.</li>
<li>Parameter: 待优化的参数</li>
</ul>
<p>在 Optuna 中，我们用 study 对象来管理优化过程。 create_study() 方法会返回一个 study 对象 ，可以填写minimize或maximize确定优化方向，然后通过 .optimize 方法执行优化过程。</p>
<p>可以通过模块optuna.sampler来定义优化算法：</p>
<ul>
<li>GridSampler：网格搜索</li>
<li>RandomSampler：随机抽样</li>
<li>TPESampler：使用TPE (Tree-structured Parzen Estimator) 算法</li>
<li>CmaEsSampler：使用 CMA-ES算法</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> optuna.samplers <span class="keyword">import</span> TPESampler</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a study object </span></span><br><span class="line">study = optuna.create_study(direction=<span class="string">&quot;maximize&quot;</span>, sampler=TPESampler())</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Invoke optimization of the objective function.</span></span><br><span class="line">study.optimize(objective, n_trials=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>获得 trial 的数目：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(study.trials)</span><br></pre></td></tr></table></figure>
<p>Out: 100</p>
<p>再次执行 optimize()，可以继续优化过程</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">study.optimize(objective, n_trials=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>获得更新后的的 trial 数量：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(study.trials)</span><br></pre></td></tr></table></figure>
<p>Out: 200</p>
<h2 id="step-3-评估输出"><a class="markdownIt-Anchor" href="#step-3-评估输出"></a> Step 3 评估输出</h2>
<p>打印最佳最佳分数和超参数值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Best score: <span class="subst">&#123;study.best_value&#125;</span>&#x27;</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Best parameters: &#x27;</span>, *[<span class="string">f&#x27;- <span class="subst">&#123;k&#125;</span> = <span class="subst">&#123;v&#125;</span>&#x27;</span> <span class="keyword">for</span> k,v <span class="keyword">in</span> study.best_params], sep=<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>Optuna 中提供了不同的方法来可视化优化结果：</p>
<table>
<thead>
<tr>
<th>函数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>plot_contour(study)</td>
<td>将参数关系绘制成等值线</td>
</tr>
<tr>
<td>plot_intermidiate_values(study)</td>
<td>绘制所有trial的学习曲线</td>
</tr>
<tr>
<td>plot_optimization_history(study)</td>
<td>绘制所有trial的优化历史记录</td>
</tr>
<tr>
<td>plot_param_importances(study)</td>
<td>绘制超参数重要性及其值</td>
</tr>
<tr>
<td>plot_edf(study)</td>
<td>绘制study目标值的edf</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optuna.visualization.plot_optimization_history(study)</span><br></pre></td></tr></table></figure>
<h2 id="参数空间进阶"><a class="markdownIt-Anchor" href="#参数空间进阶"></a> 参数空间进阶</h2>
<p>在 Optuna 中，我们使用和 Python 语法类似的方式来定义搜索空间，其中包含条件和循环语句。</p>
<p>分支：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn.ensemble</span><br><span class="line"><span class="keyword">import</span> sklearn.svm</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">objective</span>(<span class="params">trial</span>):</span></span><br><span class="line">    classifier_name = trial.suggest_categorical(<span class="string">&quot;classifier&quot;</span>, [<span class="string">&quot;SVC&quot;</span>, <span class="string">&quot;RandomForest&quot;</span>])</span><br><span class="line">    <span class="keyword">if</span> classifier_name == <span class="string">&quot;SVC&quot;</span>:</span><br><span class="line">        svc_c = trial.suggest_float(<span class="string">&quot;svc_c&quot;</span>, <span class="number">1e-10</span>, <span class="number">1e10</span>, log=<span class="literal">True</span>)</span><br><span class="line">        classifier_obj = sklearn.svm.SVC(C=svc_c)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        rf_max_depth = trial.suggest_int(<span class="string">&quot;rf_max_depth&quot;</span>, <span class="number">2</span>, <span class="number">32</span>, log=<span class="literal">True</span>)</span><br><span class="line">        classifier_obj = sklearn.ensemble.RandomForestClassifier(max_depth=rf_max_depth)</span><br></pre></td></tr></table></figure>
<p>循环：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span>(<span class="params">trial, in_size</span>):</span></span><br><span class="line">    n_layers = trial.suggest_int(<span class="string">&quot;n_layers&quot;</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_layers):</span><br><span class="line">        n_units = trial.suggest_int(<span class="string">&quot;n_units_l&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i), <span class="number">4</span>, <span class="number">128</span>, log=<span class="literal">True</span>)</span><br><span class="line">        layers.append(nn.Linear(in_size, n_units))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_size = n_units</span><br><span class="line">    layers.append(nn.Linear(in_size, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure>
<h2 id="多目标优化"><a class="markdownIt-Anchor" href="#多目标优化"></a> 多目标优化</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> make_scorer, root_mean_squared_error</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">objective</span>(<span class="params">trial</span>):</span></span><br><span class="line">    <span class="comment"># Define the search space</span></span><br><span class="line">	params= &#123;</span><br><span class="line">	   <span class="string">&#x27;max_depth&#x27;</span>: trial.suggest_int(<span class="string">&#x27;max_depth&#x27;</span>,  <span class="number">2</span>, <span class="number">10</span>, <span class="number">1</span>),</span><br><span class="line"> 	   <span class="string">&#x27;learning_rate&#x27;</span>: trial.suggest_float(<span class="string">&#x27;learning_rate&#x27;</span>, <span class="number">0.001</span>, <span class="number">1.0</span>, log=<span class="literal">True</span>),</span><br><span class="line"> 	   <span class="string">&#x27;n_estimators&#x27;</span>: trial.suggest_int(<span class="string">&#x27;n_estimators&#x27;</span>, <span class="number">100</span>, <span class="number">400</span>, <span class="number">100</span>),</span><br><span class="line"> 	   <span class="string">&#x27;subsample&#x27;</span>: trial.suggest_float(<span class="string">&#x27;subsample&#x27;</span>, <span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.1</span>),</span><br><span class="line"> 	   <span class="string">&#x27;max_features&#x27;</span>: trial.suggest_categorical(<span class="string">&#x27;max_features&#x27;</span>, [<span class="string">&#x27;sqrt&#x27;</span>, <span class="string">&#x27;log2&#x27;</span>])</span><br><span class="line">	&#125;</span><br><span class="line">    reg = GradientBoostingRegressor(**params)</span><br><span class="line">    mse = cross_val_score(reg, X_train, y_train, scoring=make_scorer(root_mean_squared_error), cv=<span class="number">5</span>).mean()</span><br><span class="line">    r2 = cross_val_score(reg, X_train, y_train, scoring=<span class="string">&#x27;r2&#x27;</span>, cv=<span class="number">5</span>).mean()</span><br><span class="line">    <span class="keyword">return</span> mse, r2</span><br><span class="line"></span><br><span class="line">study = optuna.create_study(directions=[<span class="string">&quot;minimize&quot;</span>, <span class="string">&quot;maximize&quot;</span>])</span><br><span class="line">study.optimize(objective, n_trials=<span class="number">100</span>, timeout=<span class="number">300</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of finished trials: &quot;</span>, <span class="built_in">len</span>(study.trials))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：多目标优化使用的参数 directions 和单目标参数direction不同。</p>
</blockquote>
<h2 id="常见问题"><a class="markdownIt-Anchor" href="#常见问题"></a> 常见问题</h2>
<p>官方链接：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://optuna.readthedocs.io/en/stable/faq.html">https://optuna.readthedocs.io/en/stable/faq.html</a></p>
<h3 id="如何定义带有额外参数的目标函数"><a class="markdownIt-Anchor" href="#如何定义带有额外参数的目标函数"></a> 如何定义带有额外参数的目标函数？</h3>
<p>有两种方法可以实现这类函数。</p>
<p>首先，如下例所示，可调用的 objective 类具有这个功能：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> optuna</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Objective</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, min_x, max_x</span>):</span></span><br><span class="line">        <span class="comment"># Hold this implementation specific arguments as the fields of the class.</span></span><br><span class="line">        self.min_x = min_x</span><br><span class="line">        self.max_x = max_x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, trial</span>):</span></span><br><span class="line">        <span class="comment"># Calculate an objective value by using the extra arguments.</span></span><br><span class="line">        x = trial.suggest_float(<span class="string">&quot;x&quot;</span>, self.min_x, self.max_x)</span><br><span class="line">        <span class="keyword">return</span> (x - <span class="number">2</span>) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Execute an optimization by using an `Objective` instance.</span></span><br><span class="line">study = optuna.create_study()</span><br><span class="line">study.optimize(Objective(-<span class="number">100</span>, <span class="number">100</span>), n_trials=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>其次，你可以用 <code>lambda</code> 或者 <code>functools.partial</code> 来创建带有额外参数的函数（闭包）。 下面是一个使用了 <code>lambda</code> 的例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> optuna</span><br><span class="line"></span><br><span class="line"><span class="comment"># Objective function that takes three arguments.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">objective</span>(<span class="params">trial, min_x, max_x</span>):</span></span><br><span class="line">    x = trial.suggest_float(<span class="string">&quot;x&quot;</span>, min_x, max_x)</span><br><span class="line">    <span class="keyword">return</span> (x - <span class="number">2</span>) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Extra arguments.</span></span><br><span class="line">min_x = -<span class="number">100</span></span><br><span class="line">max_x = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Execute an optimization by using the above objective function wrapped by `lambda`.</span></span><br><span class="line">study = optuna.create_study()</span><br><span class="line">study.optimize(<span class="keyword">lambda</span> trial: objective(trial, min_x, max_x), n_trials=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>其他例子参见 <a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/optuna/optuna/blob/master/examples/sklearn_additional_args.py">sklearn_addtitional_args.py</a> .</p>
<h3 id="如何在目标函数中保存训练好的机器学习模型"><a class="markdownIt-Anchor" href="#如何在目标函数中保存训练好的机器学习模型"></a> 如何在目标函数中保存训练好的机器学习模型？</h3>
<p>Optuna 会保存超参数和对应的目标函数值，但是它不会存储诸如机器学习模型或者网络权重这样的中间数据。要保存模型或者权重的话，请利用你正在使用的机器学习库提供的对应功能。</p>
<p>在保存模型的时候，我们推荐将 <code>optuna.trial.Trial.number</code>一同存储。这样易于之后确认对应的 trial.比如，你可以用以下方式在目标函数中保存训练好的 SVM 模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">objective</span>(<span class="params">trial</span>):</span></span><br><span class="line">    svc_c = trial.suggest_float(<span class="string">&quot;svc_c&quot;</span>, <span class="number">1e-10</span>, <span class="number">1e10</span>, log=<span class="literal">True</span>)</span><br><span class="line">    clf = sklearn.svm.SVC(C=svc_c)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save a trained model to a file.</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;&#123;&#125;.pickle&quot;</span>.<span class="built_in">format</span>(trial.number), <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> fout:</span><br><span class="line">        pickle.dump(clf, fout)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> - accuracy_score(y_valid, clf.predict(X_valid))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">study = optuna.create_study()</span><br><span class="line">study.optimize(objective, n_trials=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the best model.</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;&#123;&#125;.pickle&quot;</span>.<span class="built_in">format</span>(study.best_trial.number), <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> fin:</span><br><span class="line">    best_clf = pickle.load(fin)</span><br><span class="line"><span class="built_in">print</span>(accuracy_score(y_valid, best_clf.predict(X_valid)))</span><br></pre></td></tr></table></figure>
<h3 id="在优化study时如何避免内存不足oom"><a class="markdownIt-Anchor" href="#在优化study时如何避免内存不足oom"></a> 在优化study时，如何避免内存不足（OOM）?</h3>
<p>如果你运行更多trials时，导致内存增加，尝试定期运行垃圾收集器。 在回调中调用<code>optimize()</code>或调用``gc.collect()<code>时，设置</code>gc_after_trial=True`。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">objective</span>(<span class="params">trial</span>):</span></span><br><span class="line">    x = trial.suggest_float(<span class="string">&quot;x&quot;</span>, -<span class="number">1.0</span>, <span class="number">1.0</span>)</span><br><span class="line">    y = trial.suggest_int(<span class="string">&quot;y&quot;</span>, -<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">    <span class="keyword">return</span> x + y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">study = optuna.create_study()</span><br><span class="line">study.optimize(objective, n_trials=<span class="number">10</span>, gc_after_trial=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># `gc_after_trial=True` is more or less identical to the following.</span></span><br><span class="line">study.optimize(objective, n_trials=<span class="number">10</span>, callbacks=[<span class="keyword">lambda</span> study, trial: gc.collect()])</span><br></pre></td></tr></table></figure>
<h2 id="如何保存和恢复-study"><a class="markdownIt-Anchor" href="#如何保存和恢复-study"></a> 如何保存和恢复 study？</h2>
<p>有两种方法可以将 study 持久化。具体采用哪种取决于你是使用内存存储 (in-memory) 还是远程数据库存储 (RDB). 通过 <code>pickle</code> 或者 <code>joblib</code>, 采用了内存存储的 study 可以和普通的 Python 对象一样被存储和加载。比如用 <code>joblib</code> 的话：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">study = optuna.create_study()</span><br><span class="line">joblib.dump(study, <span class="string">&quot;study.pkl&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>恢复 study:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">study = joblib.load(<span class="string">&quot;study.pkl&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best trial until now:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; Value: &quot;</span>, study.best_trial.value)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; Params: &quot;</span>)</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> study.best_trial.params.items():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;    <span class="subst">&#123;key&#125;</span>: <span class="subst">&#123;value&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/python/">python</a></div><div class="post_share"><div class="social-share" data-image="/img/Hyperparameter-Optimization.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/morty3.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/morty3.jpg" alt="Give me money!"/></a><div class="post-qr-code-desc">Give me money!</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/44910830/" title="Python(Machine Learning)--LightGBM"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/LightGBM_cover.svg" onerror="onerror=null;src='/img/404_moon.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Python(Machine Learning)--LightGBM</div></div></a></div><div class="next-post pull-right"><a href="/posts/ce19bbb1/" title="特征工程(II)--数据预处理"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/FeatureEngine.png" onerror="onerror=null;src='/img/404_moon.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">特征工程(II)--数据预处理</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/90489eb7/" title="PySpark机器学习Demo"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/spark-install.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-17</div><div class="title">PySpark机器学习Demo</div></div></a></div><div><a href="/posts/75974533/" title="大数据手册(Spark)--PySpark MLlib"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-mllib.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-01</div><div class="title">大数据手册(Spark)--PySpark MLlib</div></div></a></div><div><a href="/posts/425f9947/" title="特征工程(VII)--模型集成"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/FeatureEngine.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-20</div><div class="title">特征工程(VII)--模型集成</div></div></a></div><div><a href="/posts/44910830/" title="Python(Machine Learning)--LightGBM"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/LightGBM_cover.svg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-25</div><div class="title">Python(Machine Learning)--LightGBM</div></div></a></div><div><a href="/posts/3d0ef432/" title="特征工程(VI)--机器学习"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/FeatureEngine.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-20</div><div class="title">特征工程(VI)--机器学习</div></div></a></div><div><a href="/posts/6c94e349/" title="Python(Machine Learning)--statsmodels"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/statsmodels.svg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-05-10</div><div class="title">Python(Machine Learning)--statsmodels</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Tiny Lei</div><div class="author-info__description">每天进步一点点...</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">175</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">108</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">43</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://gitee.com/wilenwu" rel="external nofollow noreferrer" target="_blank" title="Gitee"><i class="iconfont icon-gitee"></i></a><a class="social-icon" href="https://github.com/wilenwu" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="iconfont icon-github"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_41518277" rel="external nofollow noreferrer" target="_blank" title="CSDN"><i class="iconfont icon-csdn"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="iconfont icon-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">感谢访问本站，若喜欢请收藏^_^</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96"><span class="toc-number">1.</span> <span class="toc-text"> 超参数优化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#hyperopt"><span class="toc-number">2.</span> <span class="toc-text"> Hyperopt</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#step-1-%E5%AE%9A%E4%B9%89%E5%8F%82%E6%95%B0%E7%A9%BA%E9%97%B4"><span class="toc-number">2.1.</span> <span class="toc-text"> Step 1 定义参数空间</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#step-2-%E5%AE%9A%E4%B9%89%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text"> Step 2 定义目标函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#step-3-%E6%89%A7%E8%A1%8C%E4%BC%98%E5%8C%96"><span class="toc-number">2.3.</span> <span class="toc-text"> Step 3 执行优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#step-4-%E8%AF%84%E4%BC%B0%E8%BE%93%E5%87%BA"><span class="toc-number">2.4.</span> <span class="toc-text"> Step 4 评估输出</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E4%BC%98%E5%8C%96"><span class="toc-number">2.5.</span> <span class="toc-text"> 分布式优化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#scikit-optimize"><span class="toc-number">3.</span> <span class="toc-text"> Scikit-optimize</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#step-1-%E5%AE%9A%E4%B9%89%E5%8F%82%E6%95%B0%E7%A9%BA%E9%97%B4-2"><span class="toc-number">3.1.</span> <span class="toc-text"> Step 1 定义参数空间</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#step-2-%E5%AE%9A%E4%B9%89%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0-2"><span class="toc-number">3.2.</span> <span class="toc-text"> Step 2 定义目标函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#step-3-%E6%89%A7%E8%A1%8C%E4%BC%98%E5%8C%96-2"><span class="toc-number">3.3.</span> <span class="toc-text"> Step 3 执行优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#step-4-%E8%AF%84%E4%BC%B0%E8%BE%93%E5%87%BA-2"><span class="toc-number">3.4.</span> <span class="toc-text"> Step 4 评估输出</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#scikit-learn-api"><span class="toc-number">3.5.</span> <span class="toc-text"> Scikit-Learn API</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#optuna"><span class="toc-number">4.</span> <span class="toc-text"> Optuna</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#step-1-%E6%9E%84%E5%BB%BA%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E5%8F%8A%E5%8F%82%E6%95%B0%E7%A9%BA%E9%97%B4"><span class="toc-number">4.1.</span> <span class="toc-text"> Step 1 构建目标函数及参数空间</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#step-2-%E6%89%A7%E8%A1%8C%E4%BC%98%E5%8C%96"><span class="toc-number">4.2.</span> <span class="toc-text"> Step 2 执行优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#step-3-%E8%AF%84%E4%BC%B0%E8%BE%93%E5%87%BA"><span class="toc-number">4.3.</span> <span class="toc-text"> Step 3 评估输出</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E7%A9%BA%E9%97%B4%E8%BF%9B%E9%98%B6"><span class="toc-number">4.4.</span> <span class="toc-text"> 参数空间进阶</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96"><span class="toc-number">4.5.</span> <span class="toc-text"> 多目标优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="toc-number">4.6.</span> <span class="toc-text"> 常见问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%AE%9A%E4%B9%89%E5%B8%A6%E6%9C%89%E9%A2%9D%E5%A4%96%E5%8F%82%E6%95%B0%E7%9A%84%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="toc-number">4.6.1.</span> <span class="toc-text"> 如何定义带有额外参数的目标函数？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%9C%A8%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%B8%AD%E4%BF%9D%E5%AD%98%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.6.2.</span> <span class="toc-text"> 如何在目标函数中保存训练好的机器学习模型？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8%E4%BC%98%E5%8C%96study%E6%97%B6%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E5%86%85%E5%AD%98%E4%B8%8D%E8%B6%B3oom"><span class="toc-number">4.6.3.</span> <span class="toc-text"> 在优化study时，如何避免内存不足（OOM）?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BF%9D%E5%AD%98%E5%92%8C%E6%81%A2%E5%A4%8D-study"><span class="toc-number">4.7.</span> <span class="toc-text"> 如何保存和恢复 study？</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/db6e5578/" title="Java速成入门"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/java_cover.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="Java速成入门"/></a><div class="content"><a class="title" href="/posts/db6e5578/" title="Java速成入门">Java速成入门</a><time datetime="2024-09-17T14:55:05.000Z" title="发表于 2024-09-17 22:55:05">2024-09-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/a14a3278/" title="机器学习(VII)--强化学习(七)Actor-Critic"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/reinforcement_learning_cover.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="机器学习(VII)--强化学习(七)Actor-Critic"/></a><div class="content"><a class="title" href="/posts/a14a3278/" title="机器学习(VII)--强化学习(七)Actor-Critic">机器学习(VII)--强化学习(七)Actor-Critic</a><time datetime="2024-08-29T09:24:00.000Z" title="发表于 2024-08-29 17:24:00">2024-08-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3644c10c/" title="机器学习(VII)--强化学习(六)策略梯度方法"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/reinforcement_learning_cover.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="机器学习(VII)--强化学习(六)策略梯度方法"/></a><div class="content"><a class="title" href="/posts/3644c10c/" title="机器学习(VII)--强化学习(六)策略梯度方法">机器学习(VII)--强化学习(六)策略梯度方法</a><time datetime="2024-08-29T09:23:00.000Z" title="发表于 2024-08-29 17:23:00">2024-08-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/2bd55622/" title="机器学习(VII)--强化学习(五)值函数近似"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/reinforcement_learning_cover.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="机器学习(VII)--强化学习(五)值函数近似"/></a><div class="content"><a class="title" href="/posts/2bd55622/" title="机器学习(VII)--强化学习(五)值函数近似">机器学习(VII)--强化学习(五)值函数近似</a><time datetime="2024-08-29T09:22:00.000Z" title="发表于 2024-08-29 17:22:00">2024-08-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/363ac257/" title="机器学习(VII)--强化学习(四)时序差分方法"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/reinforcement_learning_cover.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="机器学习(VII)--强化学习(四)时序差分方法"/></a><div class="content"><a class="title" href="/posts/363ac257/" title="机器学习(VII)--强化学习(四)时序差分方法">机器学习(VII)--强化学习(四)时序差分方法</a><time datetime="2024-08-29T09:21:00.000Z" title="发表于 2024-08-29 17:21:00">2024-08-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Tiny Lei</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'forest'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addModeChange('mermaid', runMermaid)

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>function getGiscusTheme (theme) {
  return theme === 'dark' ? 'dark' : 'light'
}

function loadGiscus () {
  const config = Object.assign({
    src: 'https://giscus.app/client.js',
    'data-repo': 'WilenWu/giscus-comments',
    'data-repo-id': 'R_kgDONXyMwg',
    'data-category-id': 'DIC_kwDONXyMws4Ckzx5',
    'data-mapping': 'pathname',
    'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
    'data-reactions-enabled': '1',
    crossorigin: 'anonymous',
    async: true
  },null)

  let ele = document.createElement('script')
  for (let key in config) {
    ele.setAttribute(key, config[key])
  }
  document.getElementById('giscus-wrap').insertAdjacentElement('afterbegin',ele)
}

function changeGiscusTheme (theme) {
  function sendMessage(message) {
    const iframe = document.querySelector('iframe.giscus-frame')
    if (!iframe) return
    iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app')
  }

  sendMessage({
    setConfig: {
      theme: getGiscusTheme(theme)
    }
  });
}

btf.addModeChange('giscus', changeGiscusTheme)

if ('Giscus' === 'Giscus' || !false) {
  if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
  else loadGiscus()
} else {
  function loadOtherComment () {
    loadGiscus()
  }
}</script></div><script src="/js/custom.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>