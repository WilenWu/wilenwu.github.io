<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大数据手册(Spark)--PySpark Streaming | 雷小小</title><meta name="author" content="Tiny Lei"><meta name="copyright" content="Tiny Lei"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="mini-batch 数据流处理">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据手册(Spark)--PySpark Streaming">
<meta property="og:url" content="https://www.tinylei.tech/posts/34eba6aa/index.html">
<meta property="og:site_name" content="雷小小">
<meta property="og:description" content="mini-batch 数据流处理">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.tinylei.tech/img/apache-spark-streaming.png">
<meta property="article:published_time" content="2020-01-13T09:48:05.000Z">
<meta property="article:modified_time" content="2023-12-27T13:35:38.545Z">
<meta property="article:author" content="Tiny Lei">
<meta property="article:tag" content="大数据">
<meta property="article:tag" content="python">
<meta property="article:tag" content="spark">
<meta property="article:tag" content="数据流">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.tinylei.tech/img/apache-spark-streaming.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.tinylei.tech/posts/34eba6aa/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="baidu-site-verification" content="code-7rymn5Bitx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?654e7415ab55bed7c9c2bc6d665f03c5";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大数据手册(Spark)--PySpark Streaming',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-12-27 21:35:38'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_2849223_xh1ftc8qym.css"><link rel="stylesheet" href="/css/link-card.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="雷小小" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">166</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">107</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">43</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/user-guide/"><i class="fa-fw fa fa-compass"></i><span> 用户指南</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa fa-book"></i><span> 文档</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-images"></i><span> 图库</span></a></div><div class="menus_item"><a class="site-page" href="/ebook/"><i class="fa-fw fa fa-book-reader"></i><span> 电子书</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-star"></i><span> 收藏夹</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa-solid fa-circle-chevron-down"></i><span> 更多</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fa fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/analytics/"><i class="fa-fw fa fa-line-chart"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/log/"><i class="fa-fw fa fa-history"></i><span> 更新日志</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/apache-spark-top-img.svg')"><nav id="nav"><span id="blog-info"><a href="/" title="雷小小"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/favicon.png"/><span class="site-name">雷小小</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/user-guide/"><i class="fa-fw fa fa-compass"></i><span> 用户指南</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa fa-book"></i><span> 文档</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-images"></i><span> 图库</span></a></div><div class="menus_item"><a class="site-page" href="/ebook/"><i class="fa-fw fa fa-book-reader"></i><span> 电子书</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-star"></i><span> 收藏夹</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa-solid fa-circle-chevron-down"></i><span> 更多</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/about/"><i class="fa-fw fa fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/analytics/"><i class="fa-fw fa fa-line-chart"></i><span> 文章统计</span></a></li><li><a class="site-page child" href="/log/"><i class="fa-fw fa fa-history"></i><span> 更新日志</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大数据手册(Spark)--PySpark Streaming<a class="post-edit-link" href="https://gitee.com/WilenWu/myblog/edit/master/source/_posts/bigdata/Spark-Streaming.md" rel="external nofollow noreferrer" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-01-13T09:48:05.000Z" title="发表于 2020-01-13 17:48:05">2020-01-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-27T13:35:38.545Z" title="更新于 2023-12-27 21:35:38">2023-12-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/big-data/">Big Data</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/big-data/spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>14分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="大数据手册(Spark)--PySpark Streaming"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Spark Streaming 是利用 Spark Core 的快速计划功能执行流式分析的实时解决方案。它会提取迷你批处理中的数据，使用为批处理分析编写的相同应用程序代码实现对此类数据的分析。这样一来，开发人员的效率得到改善，因为他们可以将相同代码用于批处理和实时流式处理应用程序。Spark Streaming 支持来自 Twitter、Kafka、Flume、HDFS 和 ZeroMQ 的数据，以及 Spark 程序包生态系统中的其他众多数据。</p>
<h1 id="spark-streaming"><a class="markdownIt-Anchor" href="#spark-streaming"></a> Spark Streaming</h1>
<h2 id="概述"><a class="markdownIt-Anchor" href="#概述"></a> 概述</h2>
<p>Hadoop的MapReduce及Spark SQL等只能进行离线计算，无法满足实时性要求较高的业务需求，例如实时推荐、实时网站性能分析等，流式计算可以解决这些问题。目前有三种比较常用的流式计算框架，它们分别是Twitter Storm，Spark Streaming和Samza。</p>
<p>Spark Streaming用于进行实时流数据的处理，它具有高扩展、高吞吐率及容错机制。</p>
<p>如下图所示，Spark Streaming 把流式计算当做一系列连续的小规模批处理(batch)来对待。Spark Streaming 接收输入数据流，并在内部将数据流按均匀的时间间隔分为多个较小的batch。然后再将这部分数据交由Spark引擎进行处理，处理完成后将结果输出到外部文件。<br />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/Spark-Streaming.png" style="zoom:80%;" /><br />
Spark Streaming的主要抽象是离散流（DStream)，它代表了前面提到的构成数据流的那些batch。DStream可以看作是多个有序的RDD组成，因此它也只通过map, reduce, join and window等操作便可完成实时数据处理。，另外一个非常重要的点便是，Spark Streaming可以与Spark MLlib、Graphx等结合起来使用，功能十分强大，似乎无所不能。</p>
<p>目前，围绕Spark Streaming有四种广泛的场景：</p>
<ul>
<li>streaming ETL：将数据推入下游系统之前对其进行持续的清洗和聚合。这么做通常可以减少最终数据存储中的数据量。</li>
<li>Triggers(触发器)：实时检测行为或异常事件，及时触发下游动作。例如当一个设备接近了检测器或者基地，就会触发警报。</li>
<li>数据浓缩：将实时数据与其他数据集连接，可以进行更丰富的分析。例如将实时天气信息与航班信息结合，以建立更好的旅行警报。</li>
<li>复杂会话和持续学习：与实时流相关联的多组事件被持续分析，以更新机器学习模型。例如与在线游戏相关联的用户活动流，允许我们更好地做用户分类。</li>
</ul>
<p>下图提供了Spark driver、workers、streaming源与目标间的数据流：<br />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/DStream.PNG" alt="" /><br />
Spark Streaming内置了一系列receiver，可以接收很多来源的数据，最常见的是Apache Kafka、Flume、HDFS/S3、Kinesis和Twitter。</p>
<h2 id="应用案例及数据源"><a class="markdownIt-Anchor" href="#应用案例及数据源"></a> 应用案例及数据源</h2>
<p>Spark Streaming可整合多种输入数据源，如Kafka、Flume、HDFS，甚至是普通的TCP套接字。经处理后的数据可存储至文件系统、数据库，或显示在仪表盘里。<br />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/DStream-demo.PNG" height="80%" width="80%" ></p>
<p>编写Spark Streaming程序的基本步骤是：</p>
<ol>
<li>通过创建输入DStream来定义输入源</li>
<li>通过对DStream应用转换操作和输出操作来定义流计算</li>
<li>用streamingContext.start()来开始接收数据和处理流程</li>
<li>通过streamingContext.awaitTermination()方法来等待结束（例如&lt;ctrl+C&gt;），或通过streamingContext.stop()来手动结束流计算进程</li>
</ol>
<p>下面我们使用Python的Spark Streaming来创建一个简单的单词计数例子。<br />
这个字数计数示例将使用Linux/Unix nc命令——它是一种读写跨网络连接数据的简单实用程序。我们将使用两个不同的bash终端，一个使用nc命令将多个单词发送到我们计算机的本地端口（9999），另一个终端将运行Spark Streaming来接收这些字，并对它们进行计数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a local SparkContext and Streaming Contexts</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create sc with two working threads</span></span><br><span class="line">sc = SparkContext(<span class="string">&#x27;local[2]&#x27;</span>,<span class="string">&#x27;NetworkWordCount&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark Streaming入口点(每隔一秒钟运行一次微批次)</span></span><br><span class="line">ssc = StreamingContext(sc, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建DStream输入源：套接字流</span></span><br><span class="line">lines = ssc.socketTextStream(sys.argv[<span class="number">1</span>], sys.argv[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split lines into words and count</span></span><br><span class="line">wordCounts = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>)) \</span><br><span class="line">    .<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>)) \</span><br><span class="line">    .reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</span><br><span class="line">    </span><br><span class="line">wordCounts.pprint()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动Spark Streaming，并等待终止命令</span></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>
<p>如前所述，现在有了脚本，打开两个终端窗口：一个用于您的nc命令，另一个用于Spark Streaming程序。<br />
要从其中一个终端启动nc命令，请键入：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ nc -lk 9999</span><br></pre></td></tr></table></figure>
<p>从这个点开始，你在这个终端所输入的一切都将被传送到9999端口。本例中，敲入<code>green</code>这个词三次，<code>blue</code>五次。<br />
从另一个终端屏幕，我们来运行刚创建的Python流脚本（<a target="_blank" rel="noopener external nofollow noreferrer" href="http://NetworkWordCount.py">NetworkWordCount.py</a>）。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ spark-submit NetworkWordCount.py localhost 9999</span><br></pre></td></tr></table></figure>
<p>该命令将运行脚本，读取本地计算机（即localhost）端口9999以接收发送到该套接字的任何内容。由于你已经在第一个终端将信息发送端口，因此在启动脚本后不久，Spark Streaming程序会读取发送到端口9999的任何单词，并按照以下屏幕截图中所示的样子执行单词计数：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ nc -lk 9999</span><br><span class="line">green green green blue blue blue blue blue</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 2018-12-24 11:30:26</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 2018-12-24 11:30:27</span><br><span class="line">-------------------------------------------</span><br><span class="line">(<span class="string">&#x27;blue&#x27;</span>, 5)</span><br><span class="line">(<span class="string">&#x27;green&#x27;</span>, 3)</span><br></pre></td></tr></table></figure>
<p><strong>文件流</strong>：包括文本格式和任意HDFS的输入格式。创建DStream输入源示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lines = ssc.textFileStream(<span class="string">&#x27;wordfile&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>套接字流 (socket)</strong>：从一个本地或远程主机的某个端口服务上读取数据。它无法提供端到端的容错保障，Socket源一般仅用于测试或学习用途。<br />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/socket.PNG" alt="" /><br />
创建DStream输入源示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lines = ssc.socketTextStream(<span class="string">&quot;local&quot;</span>, <span class="number">9999</span>)</span><br></pre></td></tr></table></figure>
<p><strong>RDD序列流</strong>：在调试Spark Streaming应用程序的时候，我们可以使用streamingContext.queueStream(queueOfRDD)创建基于RDD队列的DStream</p>
<p><strong>kafka</strong>：Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者在网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。Kafka的目的是通过<a target="_blank" rel="noopener external nofollow noreferrer" href="https://baike.baidu.com/item/Hadoop">Hadoop</a>的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息。<br />
在公司的大数据生态系统中，可以把Kafka作为数据交换枢纽，不同类型的分布式系统（关系数据库、NoSQL数据库、流处理系统、批处理系统等），可以统一接入到Kafka，实现和Hadoop各个组件之间的不同类型数据的实时高效交换。</p>
<p>下图为kafka组成<br />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/kafka.PNG" alt="" /></p>
<ul>
<li>Broker：Kafka集群包含一个或多个服务器，这种服务器被称为broker</li>
<li>Topic ：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）</li>
<li>Partition：是物理上的概念，每个Topic包含一个或多个Partition.</li>
<li>Producer：负责发布消息到Kafka broker</li>
<li>Consumer：消息消费者，向Kafka broker读取消息的客户端。</li>
<li>Consumer Group：每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）<br />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/kafka-broker.PNG" height="80%" width="80%"></li>
</ul>
<p>我们可以创建基于Kafka的DStream</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.streaming.kafka <span class="keyword">import</span> KafkaUtils</span><br><span class="line">kvs = KafkaUtils.createStream(...)</span><br></pre></td></tr></table></figure>
<h2 id="转化操作"><a class="markdownIt-Anchor" href="#转化操作"></a> 转化操作</h2>
<p><strong>无状态转化操作</strong>：把简单的RDDtransformation分别应用到每个批次上，每个批次的处理不依赖于之前的批次的数据。<br />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/RDDtransformation.PNG" alt="" /><br />
<strong>有状态转化操作</strong>：需要使用之前批次的数据或者中间结果来计算当前批次的数据。包括基于滑动窗口的转化操作，和追踪状态变化的转化操作(updateStateByKey)<br />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/updateStateByKey.PNG" alt="" /></p>
<table>
<thead>
<tr>
<th style="text-align:left">无状态转化操作</th>
<th style="text-align:left">说明（同RDD转化类似）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>map(func)</code></td>
<td style="text-align:left">映射变换</td>
</tr>
<tr>
<td style="text-align:left"><code>flatMap(func)</code></td>
<td style="text-align:left">同RDD</td>
</tr>
<tr>
<td style="text-align:left"><code>filter(func)</code></td>
<td style="text-align:left">返回过滤后新的DStream</td>
</tr>
<tr>
<td style="text-align:left"><code>reduce(func)</code></td>
<td style="text-align:left">聚合</td>
</tr>
<tr>
<td style="text-align:left"><code>count()</code></td>
<td style="text-align:left">计数</td>
</tr>
<tr>
<td style="text-align:left"><code>union(otherStream)</code></td>
<td style="text-align:left">合并</td>
</tr>
<tr>
<td style="text-align:left"><code>countByValue()</code></td>
<td style="text-align:left">值计数</td>
</tr>
<tr>
<td style="text-align:left"><code>reduceByKey(func, [numTasks])</code></td>
<td style="text-align:left">对于相同key的数据聚合</td>
</tr>
<tr>
<td style="text-align:left"><code>join(otherStream, [numTasks])</code></td>
<td style="text-align:left">交集</td>
</tr>
<tr>
<td style="text-align:left"><code>cogroup(otherStream, [numTasks])</code></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"><code>transform(func)</code></td>
<td style="text-align:left">任意变换</td>
</tr>
<tr>
<td style="text-align:left"><code>repartition(numPartitions)</code></td>
<td style="text-align:left">重分区</td>
</tr>
</tbody>
</table>
<p><strong>滑动窗口转化操作</strong><br />
<code>window(windowLength, slideInterval)</code> 基于源DStream产生的窗口化的批数据，计算得到一个新的Dstream<br />
<code>countByWindow(windowLength, slideInterval)</code> 返回流中元素的一个滑动窗口数<br />
<code>reduceByWindow(func, windowLength, slideInterval)</code> 返回一个单元素流。利用函数func聚集滑动时间间隔的流的元素创建这个单元素流。函数func必须满足结合律，从而可以支持并行计算<br />
<code>countByValueAndWindow(windowLength, slideInterval, [numTasks])</code> 当应用到一个(K,V)键值对组成的DStream上，返回一个由(K,V)键值对组成的新的DStream。每个key的值都是它们在滑动窗口中出现的频率</p>
<p><strong>reduceByKeyAndWindow方法</strong><br />
<code>reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</code> 应用到一个(K,V)键值对组成的DStream上时，会返回一个由(K,V)键值对组成的新的DStream。每一个key的值均由给定的reduce函数(func函数)进行聚合计算。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。可以通过numTasks参数的设置来指定不同的任务数。<br />
<code>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])</code> <strong>更加高效</strong>的reduceByKeyAndWindow，每个窗口的reduce值，是基于先前窗口的reduce值进行增量计算得到的；它会对进入滑动窗口的新数据进行reduce操作，并对离开窗口的老数据进行“逆向reduce”操作。但是，只能用于“可逆reduce函数”，即那些reduce函数都有一个对应的“逆向reduce函数”（以InvFunc参数传入）。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/reduceByKeyAndWindow.PNG" alt="" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lines = ssc.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">counts = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>))\</span><br><span class="line">    .<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>))\</span><br><span class="line">    .reduceByKeyAndWindow(<span class="keyword">lambda</span> x,y:x+y, <span class="keyword">lambda</span> x,y:x-y, <span class="number">30</span>, <span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">counts.pprint()</span><br></pre></td></tr></table></figure>
<p><strong>UpdateStateByKey转化方法</strong>：需要在跨批次之间维护状态时，需要UpdateStateByKey方法。通俗点说，假如我们想知道一个用户最近访问的10个页面是什么，可以把键设置为用户ID，然后UpdateStateByKey就可以跟踪每个用户最近访问的10个页面，这个列表就是“状态”对象。</p>
<p>回到本章初的应用案例（无状态转化），1秒在nc端键入3个green和5个blue，2秒再键入1个gohawks，4秒再键入2个green。<br />
下图展示了lines DStream及其微批量数据：<br />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/lines-DStream.jpg" height="60%" width="60%" ><br />
下图表示我们计算的是有状态的全局聚合：<br />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/lines-DStream2.jpg" height="60%" width="60%" ></p>
<p>代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a local SparkContext and Streaming Contexts</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create sc with two working threads</span></span><br><span class="line">sc = SparkContext(<span class="string">&#x27;local[2]&#x27;</span>,<span class="string">&#x27;NetworkWordCount&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark Streaming入口点(每隔一秒钟运行一次微批次)</span></span><br><span class="line">ssc = StreamingContext(sc, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了确保持续运行可以容错，配置一个检查点</span></span><br><span class="line">ssc.checkpoint(<span class="string">&quot;checkpoint&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建DStream输入源：套接字流</span></span><br><span class="line">lines = ssc.socketTextStream(sys.argv[<span class="number">1</span>], sys.argv[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义更新函数：sum of the (key, value) pairs</span></span><br><span class="line"><span class="function"><span class="keyword">def</span>  <span class="title">updateFunc</span>(<span class="params">new_values, last_sum</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(new_values) + (last_sum <span class="keyword">or</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">lines = ssc.socketTextStream(sys.argv[<span class="number">1</span>], <span class="built_in">int</span>(sys.argv[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># RDD with initial state (key, value) pairs</span></span><br><span class="line">initialStateRDD = sc.parallelize([])</span><br><span class="line"></span><br><span class="line">running_counts = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>))\</span><br><span class="line">    .<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>))\</span><br><span class="line">    .updateStateByKey(updateFunc, initialRDD=initialStateRDD)</span><br><span class="line">    </span><br><span class="line">running_counts.pprint()</span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>
<p>两者的主要区别在于使用了updateStateByKey方法，该方法将执行前面提到的执行加和的updateFunc。updateStateByKey是Spark Streaming的方法，用于对数据流执行计算，并以有利于性能的方式更新每个key的状态。通常在Spark 1.5及更早版本中使用updateStateByKey，这些有状态的全局聚合的性能与状态的大小成比例，从Spark 1.6起，应该使用mapWithState。</p>
<h1 id="structured-streaming"><a class="markdownIt-Anchor" href="#structured-streaming"></a> Structured Streaming</h1>
<h2 id="概述-2"><a class="markdownIt-Anchor" href="#概述-2"></a> 概述</h2>
<p>对于Spark 2.0，Apache Spark社区致力于通过引入结构化流（structured streaming）的概念来简化流，结构化流将Streaming概念与Dataset/DataFrame相结合。结构化流式引入的是增量，当处理一系列数据块时，结构化流不断地将执行计划应用在所接收的每个新数据块集合上。通过这种运行方式，引擎可以充分利用Spark DataFrame/Dataset所包含的优化功能，并将其应用于传入的数据流。<br />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/data-stream.PNG" alt="" /></p>
<ol>
<li>微批处理：Structured Streaming默认使用微批处理执行模型，这意味着Spark流计算引擎会定期检查流数据源，并对自上一批次结束后到达的新数据执行批量查询。（数据到达和得到处理并输出结果之间的延时超过100毫秒）</li>
<li>持续处理：Spark从2.3.0版本开始引入了持续处理的试验性功能，可以实现流计算的毫秒级延迟。<br />
在持续处理模式下，Spark不再根据触发器来周期性启动任务，而是启动一系列的连续读取、处理和写入结果的长时间运行的任务。</li>
</ol>
<h2 id="应用案例及数据源-2"><a class="markdownIt-Anchor" href="#应用案例及数据源-2"></a> 应用案例及数据源</h2>
<p>编写Structured Streaming程序的基本步骤包括：</p>
<ol>
<li>创建输入数据源</li>
<li>定义流计算过程</li>
<li>启动流计算并输出结果</li>
</ol>
<p>我们来看一下使用updateStateByKey的有状态流的文字计数脚本，并将其改成一个Structured Streaming的文字计数脚本：<br />
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://warehouse-1310574346.cos.ap-shanghai.myqcloud.com/images/spark/Structured-Streaming-demo.PNG" alt="" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Import necessary classes and create a local SparkSession</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> split</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> explode</span><br><span class="line"></span><br><span class="line">spark = SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(<span class="string">&quot;StructuredNetworkWordCount&quot;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line">    </span><br><span class="line"><span class="comment"># from connection to localhost: 9999</span></span><br><span class="line">lines = spark \</span><br><span class="line">    .readStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&quot;socket&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;host&quot;</span>, <span class="string">&quot;localhost&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;port&quot;</span>, <span class="number">9999</span>) \</span><br><span class="line">    .load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># split lines into words</span></span><br><span class="line">words = lines.select(</span><br><span class="line">  explode(</span><br><span class="line">         split(lines.value, <span class="string">&quot; &quot;</span>)</span><br><span class="line">  ).alias(<span class="string">&quot;word&quot;</span>)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate runing word count</span></span><br><span class="line">wordCounts = words.groupBy(<span class="string">&quot;word&quot;</span>).count()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出至控制台</span></span><br><span class="line">query = wordCounts \</span><br><span class="line">    .writeStream \</span><br><span class="line">    .outputMode(<span class="string">&quot;complete&quot;</span>) \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&quot;console&quot;</span>) \</span><br><span class="line">    .trigger(processingTime=<span class="string">&quot;8 seconds&quot;</span>) \</span><br><span class="line">    .start()</span><br><span class="line">    </span><br><span class="line">query.awaitTermination()</span><br></pre></td></tr></table></figure>
<p>取而代之的，流那部分的代码是通过调用readStream来初始化的，我们可以使用熟悉的DataFrame groupBy语句和count来生成运行的文字计数。<br />
由于程序中需要用到拆分字符串和展开数组内的所有单词的功能，所以引用了来自pyspark.sql.functions里面的split和explode函数。<br />
让我们回到第一个终端运行我们的nc作业：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ nc -lk <span class="number">9999</span></span><br></pre></td></tr></table></figure>
<p>检查以下输出。如你所见，你既能得到有状态流的优势，还能使用更为熟悉的DataFrame API：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Batch: 0</span><br><span class="line">-------------------------------------------</span><br><span class="line">+----+-----+</span><br><span class="line">|word|count|</span><br><span class="line">+----+-----+</span><br><span class="line">| cat|    1|</span><br><span class="line">| dog|    3|</span><br><span class="line">+----+-----+</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Batch: 1</span><br><span class="line">-------------------------------------------</span><br><span class="line">+----+-----+</span><br><span class="line">|word|count|</span><br><span class="line">+----+-----+</span><br><span class="line">| cat|    2|</span><br><span class="line">| dog|    3|</span><br><span class="line">| owl|    1|</span><br><span class="line">+----+-----+</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Batch: 2</span><br><span class="line">-------------------------------------------</span><br><span class="line">+----+-----+</span><br><span class="line">|word|count|</span><br><span class="line">+----+-----+</span><br><span class="line">| cat|    2|</span><br><span class="line">| dog|    4|</span><br><span class="line">| owl|    2|</span><br><span class="line">+----+-----+</span><br></pre></td></tr></table></figure>
<p><strong>数据流</strong>：通过调用readStream来初始化。支持的格式包括文件流（csv、json、orc、parquet、text）、Kafka、套接字流（socket）、Rate源等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lines = spark \</span><br><span class="line">    .readStream \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&quot;kafka&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;localhost:9092&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;subscribe&quot;</span>, <span class="string">&#x27;wordcount-topic&#x27;</span>) \</span><br><span class="line">    .load() </span><br></pre></td></tr></table></figure>
<p><strong>输出</strong>：DataFrame/Dataset的.writeStream()方法将会返回DataStreamWriter接口，接口通过.start()真正启动流计算，并将DataFrame/Dataset写入到外部的输出接收器，DataStreamWriter接口有以下几个主要函数：</p>
<ol>
<li>format：接收器类型。</li>
<li>outputMode：输出模式，指定写入接收器的内容，可以是Append模式、Complete模式或Update模式。</li>
<li>queryName：查询的名称，可选，用于标识查询的唯一名称。</li>
<li>trigger：触发间隔，可选，设定触发间隔，如果未指定，则系统将在上一次处理完成后立即检查新数据的可用性。如果由于先前的处理尚未完成导致超过触发间隔，则系统将在处理完成后立即触发新的查询。</li>
</ol>
<p>输出模式用于指定写入接收器的内容，主要有以下几种：</p>
<ul>
<li>Append模式：只有结果表中自上次触发间隔后增加的新行，才会被写入外部存储器。这种模式一般适用于“不希望更改结果表中现有行的内容”的使用场景。</li>
<li>Complete模式：已更新的完整的结果表可被写入外部存储器。</li>
<li>Update模式：只有自上次触发间隔后结果表中发生更新的行，才会被写入外部存储器。这种模式与Complete模式相比，输出较少，如果结果表的部分行没有更新，则不会输出任何内容。当查询不包括聚合时，这个模式等同于Append模式。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">query = windowedCounts \</span><br><span class="line">    .writeStream \</span><br><span class="line">    .outputMode(<span class="string">&quot;complete&quot;</span>) \</span><br><span class="line">    .<span class="built_in">format</span>(<span class="string">&quot;console&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&#x27;truncate&#x27;</span>, <span class="string">&#x27;false&#x27;</span>) \</span><br><span class="line">    .trigger(processingTime=<span class="string">&quot;10 seconds&quot;</span>) \</span><br><span class="line">    .start()</span><br><span class="line">query.awaitTermination()</span><br></pre></td></tr></table></figure>
<p>参考链接：</p>
<ul>
<li>Spark 编程基础 - 厦门大学 | 林子雨</li>
<li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://book.douban.com/subject/27602352/">Learning PySpark - Tomasz Drabas</a></li>
</ul>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/spark/">spark</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E6%8D%AE%E6%B5%81/">数据流</a></div><div class="post_share"><div class="social-share" data-image="/img/apache-spark-streaming.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/morty3.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/morty3.jpg" alt="Give me money!"/></a><div class="post-qr-code-desc">Give me money!</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/264c088/" title="大数据手册(Spark)--Spark Core and RDDs"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-core.png" onerror="onerror=null;src='/img/404_moon.png'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">大数据手册(Spark)--Spark Core and RDDs</div></div></a></div><div class="next-post pull-right"><a href="/posts/644684db/" title="偏微分方程(一)"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/pde.png" onerror="onerror=null;src='/img/404_moon.png'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">偏微分方程(一)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/264c088/" title="大数据手册(Spark)--Spark Core and RDDs"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-core.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-03</div><div class="title">大数据手册(Spark)--Spark Core and RDDs</div></div></a></div><div><a href="/posts/75974533/" title="大数据手册(Spark)--PySpark MLlib"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-mllib.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-01</div><div class="title">大数据手册(Spark)--PySpark MLlib</div></div></a></div><div><a href="/posts/bb755aa3/" title="大数据手册(Spark)--Spark SQL and DataFrames"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-sql.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-03</div><div class="title">大数据手册(Spark)--Spark SQL and DataFrames</div></div></a></div><div><a href="/posts/d02a6da3/" title="大数据手册(Spark)--Spark安装配置"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/spark-install.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-15</div><div class="title">大数据手册(Spark)--Spark安装配置</div></div></a></div><div><a href="/posts/32722c50/" title="大数据手册(Spark)--Spark 简介"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/apache-spark-overview.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-03</div><div class="title">大数据手册(Spark)--Spark 简介</div></div></a></div><div><a href="/posts/a1358f89/" title="PySpark 特征工程(II)--特征构造"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/spark-install.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-03</div><div class="title">PySpark 特征工程(II)--特征构造</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Gitalk</span><span class="switch-btn"></span><span class="second-comment">Waline</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div><div><div id="waline-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Tiny Lei</div><div class="author-info__description">每天进步一点点...</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">166</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">107</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">43</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://gitee.com/wilenwu" rel="external nofollow noreferrer" target="_blank" title="Gitee"><i class="iconfont icon-gitee"></i></a><a class="social-icon" href="https://github.com/wilenwu" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="iconfont icon-github"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_41518277" rel="external nofollow noreferrer" target="_blank" title="CSDN"><i class="iconfont icon-csdn"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="iconfont icon-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">感谢访问本站，若喜欢请收藏^_^</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#spark-streaming"><span class="toc-number">1.</span> <span class="toc-text"> Spark Streaming</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text"> 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B%E5%8F%8A%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="toc-number">1.2.</span> <span class="toc-text"> 应用案例及数据源</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%AC%E5%8C%96%E6%93%8D%E4%BD%9C"><span class="toc-number">1.3.</span> <span class="toc-text"> 转化操作</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#structured-streaming"><span class="toc-number">2.</span> <span class="toc-text"> Structured Streaming</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-2"><span class="toc-number">2.1.</span> <span class="toc-text"> 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B%E5%8F%8A%E6%95%B0%E6%8D%AE%E6%BA%90-2"><span class="toc-number">2.2.</span> <span class="toc-text"> 应用案例及数据源</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/4f81b9fa/" title="机器学习(V)--无监督学习(三)EM算法"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/ML-unsupervised-learning.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="机器学习(V)--无监督学习(三)EM算法"/></a><div class="content"><a class="title" href="/posts/4f81b9fa/" title="机器学习(V)--无监督学习(三)EM算法">机器学习(V)--无监督学习(三)EM算法</a><time datetime="2024-07-07T13:47:30.000Z" title="发表于 2024-07-07 21:47:30">2024-07-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/d5456183/" title="机器学习(IV)--监督学习(二)线性和二次判别分析"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/ML-supervised-learning.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="机器学习(IV)--监督学习(二)线性和二次判别分析"/></a><div class="content"><a class="title" href="/posts/d5456183/" title="机器学习(IV)--监督学习(二)线性和二次判别分析">机器学习(IV)--监督学习(二)线性和二次判别分析</a><time datetime="2024-06-24T11:20:00.000Z" title="发表于 2024-06-24 19:20:00">2024-06-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/26cd5aa6/" title="机器学习(V)--无监督学习(二)流形学习"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/ML-unsupervised-learning.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="机器学习(V)--无监督学习(二)流形学习"/></a><div class="content"><a class="title" href="/posts/26cd5aa6/" title="机器学习(V)--无监督学习(二)流形学习">机器学习(V)--无监督学习(二)流形学习</a><time datetime="2024-06-21T14:05:00.000Z" title="发表于 2024-06-21 22:05:00">2024-06-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/c929642b/" title="机器学习(V)--无监督学习(二)主成分分析"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/ML-unsupervised-learning.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="机器学习(V)--无监督学习(二)主成分分析"/></a><div class="content"><a class="title" href="/posts/c929642b/" title="机器学习(V)--无监督学习(二)主成分分析">机器学习(V)--无监督学习(二)主成分分析</a><time datetime="2024-06-15T05:36:00.000Z" title="发表于 2024-06-15 13:36:00">2024-06-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/8c3d002c/" title="机器学习(V)--无监督学习(一)聚类"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/ML-unsupervised-learning.png" onerror="this.onerror=null;this.src='/img/404_moon.png'" alt="机器学习(V)--无监督学习(一)聚类"/></a><div class="content"><a class="title" href="/posts/8c3d002c/" title="机器学习(V)--无监督学习(一)聚类">机器学习(V)--无监督学习(一)聚类</a><time datetime="2024-06-12T05:38:00.000Z" title="发表于 2024-06-12 13:38:00">2024-06-12</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Tiny Lei</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'forest'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addModeChange('mermaid', runMermaid)

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '7c65134b48b13f306114',
      clientSecret: 'f049f68368a11925fdb69e57c64839eac94e13c1'',
      repo: 'gitalk-comments',
      owner: 'WilenWu',
      admin: ['WilenWu'],
      id: '36d531be6dcbf49b68524731940b021e',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.textContent= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script><script>function loadWaline () {
  function initWaline () {
    const waline = Waline.init(Object.assign({
      el: '#waline-wrap',
      serverURL: 'https://waline-comments-9etq63pcv-wilenwu.vercel.app',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      path: window.location.pathname,
      comment: false,
    }, {"requiredMeta":["monsterid"]}))
  }

  if (typeof Waline === 'object') initWaline()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css').then(() => {
      getScript('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.js').then(initWaline)
    })
  }
}

if ('Gitalk' === 'Waline' || !true) {
  if (true) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
  else setTimeout(loadWaline, 0)
} else {
  function loadOtherComment () {
    loadWaline()
  }
}</script></div><script src="/js/custom.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>